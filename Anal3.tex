\documentclass[a4paper,11pt,titlepage]{book}
\pdfpagewidth
\paperwidth
\pdfpageheight
\paperheight
\usepackage[italian]{babel} 
\usepackage{epsfig}
\usepackage{fancyhdr} 
\usepackage{amsmath,amssymb}
\usepackage{undertilde}
\usepackage{tikz}
\usepackage{amscd} 
\author{A cura di Manuel Trezzi}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc} 
\frenchspacing 
\usepackage{geometry}
\usepackage{rotating}
\usepackage{caption}
\captionsetup{labelformat=empty, textfont=sl}
\geometry{a4paper,tmargin=3cm,bmargin=3cm, lmargin=3cm,rmargin=2cm} \usepackage{multirow}
\usepackage{picture}
\setlength\parindent{0pt}
\title{Analisi 3}

\begin{document}

\maketitle

\tableofcontents

\chapter{Successioni e serie di funzioni}

\section{Successioni di funzioni}

\subsection{Convergenza puntuale}

\textbf{Definizione 1.1.1:} Sia $X$ un insieme qualunque e sia $\{f_{n}\}$ una successione di funzioni $f_{n}:X\rightarrow\mathbb{R}$, diciamo che $\{f_{n}\}$ converge puntualmente (o semplicemente) in $X$ alla funzione $f:X\rightarrow\mathbb{R}$ se e solo se $\forall{x_{0}}\in{X}$ si ha che: 
$$\lim_{n \to \infty}{f_{n}(x_{0})}=f(x_{0})$$
Analogamente possiamo dire che:

 $$\forall{x_{0}}\in{X}\mbox{ e }\forall{\epsilon{>0}}\mbox{ }\exists{N}=N(x_{0},\epsilon)\mbox{: se }n\geq{N} \Rightarrow|f_{n}(x_{0})-f(x_{0})|<\epsilon$$ 
 
 \textbf{Osservazione 1.1.2:} \begin{itemize}
\item L'eventuale struttura di X non ha ruolo
\item Essendo $x_0$ fissato $\{f_{n}(x_0)\}$ e $f(x_0)$ sono numeri, la relazione $\lim\limits_{n\to\infty}f_n(x_0)=f(x_0)$ è quindi quella di successioni in $\mathbb{R}$
\item $\mathbb{R}$ è uno spazio metrico quindi rispetta la proprietà di Hausdorff, questo vuol dire che il limite se esiste è unico. La funzione $f$ è univocamente determinata  $$f(x_{0}):=\lim_{n \to \infty}{f_{n}}(x_{0})$$
\item Nella definizione si può sostituire $\mathbb{R}$ con $\mathbb{R}^n$ o un qualsiasi altro spazio metrico
\end{itemize}

La convergenza puntuale non preserva proprietà importanti quali:  limitatezza, continuità, derivabilità e integrabilità \\

\textbf{Esempio 1.1.3:} $f_{n}(x)\to f(x)$ con $f_n$ limitata $ \forall n$ non implica che $f$ sia limitata.\\

$f_{n}(x)=\min(|x|,n)$, ogni $f_{n}$ è limitata ma la successione converge puntualmente a $f(x)=|x|$ che non è limitata\\

\begin{center}
\begin{tikzpicture}
      \draw[->] (-3.2,0) -- (3.2,0) node[right] {$x$};
      \draw[->] (0,-0.20) -- (0,3.2) node[above] {$y$};
      \draw[very thin,color=gray] (-3,0) grid (3,3);
      \draw[scale=1,domain= -3.2:-2,smooth,variable=\x,blue,dashed] plot ({\x},{-\x});
      \draw[scale=1,domain=2:3.2,smooth,variable=\x,blue,dashed] plot ({\x},{\x})node[above] {$f(x)$};
      \draw[scale=1,domain=1:3.2,smooth,variable=\x,red] plot ({\x},{1}) node[above] {$f_1(x)$};
      \draw[scale=1,domain=-3.2:-1,smooth,variable=\x,red] plot ({\x},{1});
      \draw[scale=1,domain=2:3.2,smooth,variable=\x,red] plot ({\x},{2})node[above] {$f_2(x)$};
      \draw[scale=1,domain=-3.2:-2,smooth,variable=\x,red] plot ({\x},{2});
      \draw[scale=1,domain=-2:0,smooth,variable=\x,red] plot ({\x},{-\x});
      \draw[scale=1,domain=0:2,smooth,variable=\x,red] plot ({\x},{\x});
      \coordinate [label=below right:\textcolor{black}
{$\scriptstyle n$}] (c) at (1,0);
 \coordinate [label=below right:\textcolor{black}
{$\scriptstyle 2n$}] (c) at (2,0);
\coordinate [label=below left:\textcolor{black}
{$\scriptstyle -n$}] (c) at (-1,0);
 \coordinate [label=below left:\textcolor{black}
{$\scriptstyle -2n$}] (c) at (-2,0);

      
      \end{tikzpicture}
      \end{center}
      
      
    

\textbf{Esempio 1.1.4:} $f_{n}(x)\to f(x)$ con $f_n$ continua $ \forall n$ non implica che $f$ sia continua.\\

 $f_{n}(x):[0,1]\rightarrow\mathbb{R}$ $f_{n}(x)=x^n$, le $f_{n}$ sono continue ma la successione converge puntualmente a $$f(x)=
\begin{cases}
 1 &  x=1 \\ 
0 & x\in [0,1)
\end{cases}$$
\begin{center}
\begin{tikzpicture}[scale=4]
      \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x$};
      \draw[->] (0,-0.2) -- (0,1.2) node[above] {$y$};
      \draw[scale=1,domain=0:1,smooth,variable=\x,red] plot ({\x},{\x}) node[above] {$f_1(x)$};
      \draw[scale=1,domain=0:1,smooth,variable=\x,blue] plot ({\x},{\x^3});
      \draw[scale=1,domain=0:1,smooth,variable=\x,green] plot ({\x},{\x^2}) node[right] {$f_2(x)$};
      \draw[very thin,color=gray,dashed] (-0.2,-0.20) grid (1.2,1.2);
      \coordinate [label=below right:\textcolor{black}
{$\scriptstyle 1$}] (c) at (1,0);
     \end{tikzpicture}
     \end{center}

$f$ non è continua \\

\textbf{Esempio 1.1.5:} $f_{n}(x)\to f(x)$ con $f_n$ integrabile $ \forall n$ non implica che $f$ sia integrabile.\\ 

$f_{n}(x):[0,1]\rightarrow\mathbb{R}$ con $$f_{n}(x)=
\begin{cases}
 1 &  x=\frac{a}{2^n}, a\in \mathbb{N}  \\ 
0 & altrimenti
\end{cases}$$
le $f_n$ sono Riemann integrabili ma la successione converge puntualmente a 
$$f(x)=
\begin{cases}
 1 &  x=\frac{a}{2^b}, a,b\in \mathbb{N}  \\ 
0 & altrimenti
\end{cases}$$
siccome i due insiemi di discontinuità sono densi l'uno nell'altro la funzione non è integrabile.\\

\textbf{Esempio 1.1.6:} $f_{n}(x)\to f(x)$ con $f_n$ derivabile $ \forall n$ non implica che $f$ sia derivabile.\\

$f_{n}(x):\Omega\subseteq\mathbb{R}\rightarrow\mathbb{R}$ con $\Omega$ aperto $f_{n}(x)=\sqrt{(x^2+\frac{1}{n})}$, tutte le $f_n$ sono derivabili ma la successione converge puntualmente a $f(x)=|x|$ che non è derivabile.

\begin{center}
\begin{tikzpicture}
      \draw[->] (-3.2,0) -- (3.2,0) node[right] {$x$};
      \draw[->] (0,-0.20) -- (0,3.2) node[above] {$y$};
      \draw[very thin,color=gray] (-3,0) grid (3,3);
      \draw[scale=1,domain= -3.2:0,smooth,variable=\x,blue,dashed] plot ({\x},{-\x});
      \draw[scale=1,domain=0:3.2,smooth,variable=\x,blue,dashed] plot ({\x},{\x})node[above] {$f(x)$};
       \draw[scale=1,domain=-3.2:+3.2,smooth,variable=\x,red] plot (\x,{(\x*\x+1)^0.5});
       \draw[scale=1,domain=-3.2:+3.2,smooth,variable=\x,red] plot (\x,{(\x*\x+0.5)^0.5});
       \draw[scale=1,domain=-3.2:+3.2,smooth,variable=\x,red] plot (\x,{(\x*\x+1/3)^0.5});
      \end{tikzpicture}
      \end{center}

\subsection{Convergenza uniforme}

\textbf{Definizione 1.1.7:} Sia $X$ un insieme qualunque e sia $\{f_{n}\}$ una successione di funzioni $f_{n}:X\rightarrow\mathbb{R}$, diciamo che $\{f_{n}\}$ converge uniformemente in $X$ alla funzione $f:X\rightarrow\mathbb{R}$ se:

\begin{center}
$\forall{\epsilon{>0}}$ $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $|f_{n}(x_{0})-f(x_{0})|<\epsilon$  $\forall{x_{0}}\in{X}$ \\
\end{center} 

A differenza della definizione di convergenza puntuale la convergenza uniforme richiede che lo stesso $\epsilon$ valga per ogni $x_0$. \\

\textbf{Definizione 1.1.8:} Data $g:X\rightarrow\mathbb{R}$ definiamo $||g||_{\infty,X}:=\sup|g(x)|$ $x\in{X}$, il perchè lo indichiamo con il simbolo di norma lo vedremo in seguito\\ 

Possiamo riscrivere la definizione di convergenza uniforme attraverso questo nuovo concetto

\begin{center}
$\forall{\epsilon{>0}}$ $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $||f_{n}-f||_{\infty,X}<\epsilon$\\
\end{center}

Ovvero che:
$$n\leq N \Rightarrow \sup|f_n(x)-f(x)|<\epsilon$$

Ovvero che:
$$\lim_{n\to\infty}\sup|f_n(x)-f(x)|=0$$

Ovvero che:
$$\lim_{n\to\infty}||f_{n}-f||_{\infty,X}=0$$

\begin{center}
\begin{tikzpicture}[scale=4]
      \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x$};
      \draw[->] (0,-0.2) -- (0,1.2) node[above] {$y$};
      \draw[scale=1,domain=0.2:+0.8,smooth,variable=\x,red] plot (\x,{10*(\x -0.3)*(\x -0.5)*(\x -0.7)+0.6})node[right] {$f(x)$};
      \draw[scale=1,domain=0.2:+0.8,smooth,variable=\x,red,dashed] plot (\x,{10*(\x -0.3)*(\x -0.5)*(\x -0.7)+0.8})node[right] {$f(x)+\epsilon$};
      \draw[scale=1,domain=0.2:+0.8,smooth,variable=\x,red,dashed] plot (\x,{10*(\x -0.3)*(\x -0.5)*(\x -0.7)+0.4})node[right] {$f(x)-\epsilon$};
     
     
      \end{tikzpicture}
      \end{center}
      
Possiamo dire che se la successione converge uniformemente allora $\forall\epsilon$ esiste un $N$ per cui tutte le $f_n$ con $n\geq N$ stanno nel tubicino $f(x)-\epsilon$ e $f(x)+\epsilon$.\\

\textbf{Proposizione 1.1.9:} Data $\{f_{n}\}:X\rightarrow\mathbb{R}$ e $f:X\rightarrow\mathbb{R}$, se $\{f_{n}\}$ converge uniformemente a $f$ in $X$ allora converge puntualmente alla stessa $f$ $\forall x\in X$. La convergenza uniforme è una condizione più forte della convergenza semplice.\\

\textbf{Dimostrazione:} Fissato $x_0\in X$ e si ha che $|f_{n}(x_{0})-f(x_{0})|\leq \sup |f_{n}-f|=||f_{n}-f||_{\infty,X}$ quindi se non ci fosse convergenza puntuale non potrebbe esistere convergenza uniforme. $\Box$

\subsection{Proprietà della convergenza uniforme}

\textbf{Teorema 1.1.10:} Data $\{f_{n}\}:X\rightarrow\mathbb{R}$ e $f:X\rightarrow\mathbb{R}$, se $\{f_{n}\}$ converge uniformemente a $f$ in $X$ allora se ogni $f_{n}$ è limitata $f$ è limitata\\

\textbf{Dimostrazione:} Scelgo $\epsilon =1$ nella definizione di convergenza uniforme  quindi $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $|f_{n}(x_{0})-f(x_{0})|<1$\\

Scriviamo $|f(x)|=|f(x)-f_{n}(x)+f_{n}(x)|\leq |f(x)-f_{n}(x)|+|f_{n}(x)|\leq 1+||f_{n}||_{\infty,X}$ per la nostra scelta fatta su $\epsilon$\\

Dall'ipotesi di limitatezza si ha che $||f_{n}||_{\infty,X}<\infty$ da cui deduciamo $\sup |f(x)|\leq 1+||f_{n}||_{\infty,X}<\infty$ quindi $f$ limitata. $\Box$ \\

\textbf{Teorema 1.1.11:} Data $\{f_{n}\}:X\rightarrow\mathbb{R}$ e $f:X\rightarrow\mathbb{R}$ (con$X\subseteq \mathbb{R}$) con $\{f_{n}\}\rightarrow f$ uniformemente in $X$ allora se ogni $f_{n}$ è continua in $x_0 \in X$ anche $f$ è continua in $x_0$\\

\textbf{Dimostrazione:} Se $x_0 \in X$ è un punto isolato la funzione è sicuramente continua quindi scelgo $x_0$ d'accumulazione, fisso $\epsilon >0$ e dalla definizione di convergenza uniforme segue che $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $|f_{n}(x_{0})-f(x_{0})|<\epsilon$. Scrivo che:


$$|f(x)-f(x_0)|=|f(x)-f_{n}(x)+f_{n}(x)-f_{n}(x_{0})+f_{n}(x_{0})-f(x_0)|$$ $$\leq
|f(x)-f_{n}(x)|+|f_{n}(x)-f_{n}(x_{0})|+|f_{n}(x_{0})-f(x_0)|$$

 Il primo e il terzo addendo sono ciascuno $<\epsilon$ per l'ipotesi di convergenza uniforme perciò: 
 $$|f(x)-f(x_0)|\leq 2\epsilon+|f_{n}(x)-f_{n}(x_{0})|$$
 
Dall'ipotesi che le $\{f_{n}\}$ sono continue so che $\exists{\delta}=\delta(\epsilon)$: se $|x-x_0|\leq{\delta}$ $\Rightarrow$ $|f_{n}(x_{0})-f(x_{0})|<\epsilon$ quindi $|f(x)-f(x_0)|\leq 3\epsilon$. $\Box$ \\

\textbf{Definizione 1.1.12:} Definiamo $B(X,\mathbb{R}):=\{f:X\rightarrow\mathbb{R},\sup |f(x)|<\infty\}$ come l'insieme delle funzioni limitate da $X$ in $\mathbb{R}$ \\

\textbf{Osservazione 1.1.13:} $B(X,\mathbb{R})$ è uno spazio vettoriale infatti $\sup |(f+g)(x)|=\sup |f(x)+g(x)|\leq ||f||_{\infty,X}+||g||_{\infty,X}<\infty$ quindi la somma di due funzioni limitate è limitata e se moltiplico per uno scalare $|(\lambda f(x)|=|\lambda |\cdot |f(x)|\leq \sup |f(x)|=|\lambda | \cdot ||f||_{\infty,X}$ ottengo sempre una funzione limitata\\

\textbf{Osservazione 1.1.14:} $||f||_{\infty,X}$ è una norma su $B(X,\mathbb{R})$
\begin{itemize}
\item $||f||_{\infty,X}\geq 0$ e $||f||_{\infty,X}=0\Leftrightarrow f\equiv 0$
\item Rispetta la disuguaglianza triangolare (si veda sopra)
\item $||\lambda f||_{\infty,X}=|\lambda |\cdot ||f||_{\infty,X}$ \\
\end{itemize}

\textbf{Proposizione 1.1.15:} $B(X,\mathbb{R})$ dotato della norma $||f||_{\infty,X}$ è uno spazio normato completo (è uno spazio di Banach) \\

\textbf{Dimostrazione:} Vogliamo dimostrare che ogni sequenza di Cauchy è convergente. Sia $\{f_{n}\}$ una successione di Cauchy, per definizione $\forall{\epsilon}>0$ si ha che: $\exists{N}=N(\epsilon)$: se $n,m\geq{N}$ $\Rightarrow$ $||f_{n}-f_{m}||_{\infty,X}<\epsilon$\\

Fisso $x_0$ nello spazio, $\epsilon >0$ e $ N $ come sopra quindi per $m,n \geq N$:  
$$|f_{n}(x_0)-f_{m}(x_0)|\leq \sup|f_{n}(x)-f_{n}(x)|=||f_{n}-f_{m}||_{\infty,X}<\epsilon$$ 

Quindi la successione $\{f_{n}(x_0)\}$ è di Cauchy in $\mathbb{R}$ che è completo $\Rightarrow\{f_{n}(x_0)\}$  converge $\forall{x_{0}}\in{X}$ scelto arbitrariamente  $\Rightarrow \lim_{n \to \infty}f_{n}(x_0)$ esiste $\forall{x_{0}}\in{X}$. Sia $f(x):=\lim\limits_{n \to \infty}{f_{n}}(x)$ allora $\{f_{n}\}\rightarrow f$ puntualmente\\

Dalla proprietà di Cauchy segue che $\forall{\epsilon}>0$ $\exists{N}=N(\epsilon)$: se $n,m\geq{N}$ $\Rightarrow$ $|f_{n}(x)-f_{m}(x)|<\epsilon$  $\forall{x}$ per cui se fisso $x_0$ ho $|f_{n}(x_0)-f_{m}(x_0)|<\epsilon$, prendo il limite per $m$ che tende a $+\infty$ e ho $|f_{n}(x_0)-f(x_0)|<\epsilon$ e siccome $x_0$ è arbitrario e $N$ non dipende da esso la convergenza è uniforme $\forall{\epsilon{>0}}$ $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $||f_{n}-f||_{\infty,X}<\epsilon$. $\Box$\\

\textbf{Definizione 1.1.16:} Sia $\Omega \subseteq \mathbb{R}$ aperto $C(\Omega,\mathbb{R}):=\{f:\Omega\rightarrow\mathbb{R}, f$ continua$\}$, prendiamo l'intersezione $C(\Omega,\mathbb{R})\cap B(\Omega,\mathbb{R})=:CB(\Omega,\mathbb{R})$ ovvero l'insieme delle funzioni continue e limitate in $\Omega$\\

\textbf{Proposizione 1.1.17:} $CB(\Omega,\mathbb{R})$ dotato della norma $||f||_{\infty,X}$ è uno spazio di Banach\\

\textbf{Dimostrazione:} Per la proposizione precedente, poichè siamo in un sottoinsieme di uno spazio completo, è sufficiente provare la chiusura dell'insieme. Sia $\{f_{n}\}\subseteq CB(\Omega,\mathbb{R})$ che converge puntualmente a $f\in B(\Omega,\mathbb{R})$, siccome la convergenza è uniforme $f$ sta anche in $C(\Omega,\mathbb{R})$ $\Rightarrow f \in CB(\Omega,\mathbb{R})$.$\Box$\\

\textbf{Teorema 1.1.18:} Siano $\{f_{n}\},f:[a,b]\subset\mathbb{R}\rightarrow\mathbb{R}$ con $\{f_{n}\}\rightarrow f$ uniformemente e  $\{f_{n}\}\in\mathcal{R}([a,b])$ allora:
\begin{itemize}
\item $f\in \mathcal{R}([a,b])$
\item $|\int_{a}^{b}f_{n}(x)dx-\int_{a}^{b}f_(x)dx|\leq (b-a)\cdot\||f_{n}-f||_{\infty,X}$ quindi $\lim\limits_{n \to \infty}\int_{a}^{b}f_{n}(x)dx=\int_{a}^{b}f_(x)dx$ ovvero $\lim\limits_{n \to \infty}\int_{a}^{b}f_{n}(x)dx=\int_{a}^{b}\lim\limits_{n \to \infty}{f_{n}(x)}dx$ (limite e integrale con queste ipotesi si possono scambiare) \\
\end{itemize}

\textbf{Dimostrazione:} Per l'ipotesi di convergenza uniforme fisso $\epsilon >0$ per cui $\exists{N}=N(\epsilon)$: se $n\geq{N}$ $\Rightarrow$ $||f_{n}-f||_{\infty,[a,b]}<\epsilon$. Siccome $\{f_{n}\}\in\mathcal{R}([a,b])$ $\exists P$ partizione per la quale:


 $$\sum_{j=1}^n (\widetilde{M}_{j,N}-\widetilde{m}_{j,N})\cdot\ (x_{j}-x_{j-1})\leq\epsilon$$

 dove $\widetilde{M}_{j,N}:=\sup_{[x_{j-1},x_{j}]}f_{n}(x)$ e $\widetilde{m}_{j,N}:=\inf_{[x_{j-1},x_{j}]}f_{n}(x)$. \\
 
 Poniamo $f(x)=f_{n}(x)+f(x)-f_{n}(x)$ da cui: $$f(x)\leq f_{n}(x)+|f(x)-f_{n}(x)|\leq f_{n}(x)+||f_{n}-f||_{\infty,[a,b]}\leq f_{n}(x)+\epsilon$$ quindi se $x\in [x_{j-1},x_{j}] $ ho $f(x) \leq \sup f_{n}(x)+\epsilon=\widetilde{M}_{j,N}+\epsilon$ perciò $M_{j,N}:=\sup f(x)\leq\widetilde{M}_{j,N}+\epsilon$.\\

Passiamo ora a $f(x)\geq f_{n}(x)-|f(x)-f_{n}(x)|\geq f_{n}(x)-||f_{n}-f|| \geq f_{n}(x)-\epsilon$ quindi se $x\in [x_{j-1},x_{j}] $ ho $f(x) \geq \inf f_{n}(x)-\epsilon=\widetilde{m}_{j,N}+\epsilon$ perciò $m_{j,N}:=\inf f(x)\geq\widetilde{m}_{j,N}+\epsilon$. \\

Ora sappiamo che:
 $$\sum_{j=1}^n (\widetilde{M}_{j,N}-\widetilde{m}_{j,N}) \cdot (x_{j}-x_{j-1})\leq \sum_{j=1}^n (M_{j,N}-m_{j,N}+2\epsilon)\cdot (x_{j}-x_{j-1})$$  $$=\sum_{j=1}^n (M_{j,N}-m_{j,N})\cdot (x_{j}-x_{j-1})+2\epsilon\sum_{j=1}^n(x_{j}-x_{j-1})\leq \epsilon +2\epsilon(b-a)$$.

$|\int_{a}^{b}f_{n}(x)dx-\int_{a}^{b}f_(x)dx|=|\int_{a}^{b}(f_{n}(x)-f(x))dx|\leq\int_{a}^{b}||f_{n}-f||_{\infty,[a,b]}dx=||f_{n}-f||_{\infty,[a,b]}(b-a)$ $\Box$\\

\textbf{Osservazione 1.1.19:} Integrale e limite in generale non possono essere scambiati

$$f_{n}(x)=\begin{cases} n &  x\in (\frac{1}{n},\frac{2}{n}) \\ 0 & altrimenti \end{cases}$$ quindi $\{f_{n}\}\rightarrow f\equiv 0$. Si ha che $\int_{0}^{1}f_{n}(x)dx=1 \forall{n}$ perciò $ \lim_{n \to \infty}\int_{0}^{1}f_{n}(x)dx=1$ ma $\int_{0}^{1}f(x)dx=\int_{a}^{b}\lim_{n \to \infty}{f_{n}(x)}dx=0$ \\

\textbf{Osservazione 1.1.20:} Il teorema non vale per gli integrali impropri

$$f_{n}(x)=\begin{cases} \frac{1}{n} &  x\in (-n,n) \\ 0 & |x|\geq n \end{cases}$$ Allora $\{f_{n}\}\rightarrow f$ uniformemente in $\mathbb{R}$ poichè $\sup |f_{n}(x)-f(x)|=\frac{1}{n} \rightarrow 0$. Valutiamo gli integrali $\int_{ \mathbb{R} }^{}f_{n}=2$ ma $\int_{\mathbb{R}}^{}f=0$\\

\textbf{Osservazione 1.21:} La convergenza uniforme non preserva la derivabilità. Si consideri l'esempio 1.4 e si osservi che  $\{f_{n}\}\rightarrow f$ uniformemente.\\

\textbf{Teorema 1.1.22:} Sia $\{f_{n}\}:(a,b)\subseteq \mathbb{R} \rightarrow\mathbb{R}$ e supponiamo che $f_{n}(x_0)$ converga a un numero $l$ con $x_0\in (a,b)$, ipotizziamo anche che ogni $f_{n}$ è derivabile in $(a,b)$  e che $\exists g:(a,b)\rightarrow\mathbb{R}$ tale che $\{f'_{n}\}\rightarrow g$ uniformemente allora:\begin{itemize}
\item $\exists f:(a,b)\rightarrow\mathbb{R}$ tale che $\{f_{n}\}\rightarrow f$ $\forall{x}\in (a,b)$
\item la convergenza è uniforme sui compatti contenuti in $(a,b)$
\item $f$ è derivabile e $f'(x)=g(x)$ $\forall{x}\in (a,b)$ ovvero $\frac{d}{dx}(\lim\limits_{n \to \infty}f_{n})(x)=(\lim\limits_{n \to \infty}(\frac{d}{dx}f_{n}))(x)$\\
\end{itemize}

\textbf{Dimostrazione:} Per semplificarci il lavoro supponiamo inoltre che $f'_{n}$ siano continue $\forall{n}$ (il teorema è comunque valido senza questa ipotesi aggiuntiva).\\

Vale che $f_{n}(x)=f_{n}(x_0)+\int_{x_0}^{x}f'_{n}(u)du$, $g$ è il limite uniforme delle $\{f'_{n}\}$ continue quindi anche $g$ è continua. Sia 

\begin{center}
$f(x):=l+\int_{x_0}^{x}g(u)du$
\end{center}

$f$ è ben definita per quanto detto su $g$ e derivabile in $(a,b)$ con derivata $g$ continua $\Rightarrow f\in\mathcal{C}^1((a,b))$.\\

Prendiamo $f_{n}(x)-f(x)=f_{n}(x_0)-l+\int_{x_0}^{x}(f'_{n}(u)-g(u))du$. Per la disuguaglianza triangolare:

\begin{center}
$f_{n}(x)-f(x)\leq |f_{n}(x_0)-l|+|\int_{x_0}^{x}|(f'_{n}(u)-g(u))|du|$
\end{center}

Sia $K=[\alpha,\beta]$ compatto in $(a,b)$ allora tornando alla disuguaglianza si ha:\\

$$f_{n}(x)-f(x)\leq |f_{n}(x_0)-l|+|\int_{x_0}^{x}||f'_{n}-g(u)||_{\infty,K}du|$$ $$=|f_{n}(x_0)-l|+||f'_{n}-g(u)||_{\infty,(a,b)}|x-x_0|\leq|f_{n}(x_0)-l|+||f'_{n}-g(u)||_{\infty,(a,b)}|\beta - \alpha|$$\\


Abbiamo quindi stimato $||f_{n}-f||_{\infty,K}\leq|f_{n}(x_0)-l|+||f'_{n}-g(u)||_{\infty,(a,b)}|\beta - \alpha|$ ma entrambi gli addendi per ipotesi tendono a zero quindi $||f_{n}-f||_{\infty,K}\rightarrow 0$ ovvero la convergenza è uniforme su $K$ compatto; inoltre siccome $K$ è arbitrario si ha convergenza puntuale su tutto $(a,b)$.$\Box$\\

\textbf{Esercizio 1.1.23:} Sia $C^1B((a,b)):=\{f:(a,b)\rightarrow\mathbb{R},$  $f\in\mathcal{C}^1$ e limitata$\}$ verificare che è uno spazio vettoriale e data $|||f|||_1:=||f||_{\infty,(a,b)}+||f'||_{\infty,(a,b)}$ dimostrare che è ben definita ed è una norma, $C^1B$ dotato della norma $|||$ $|||_1$ è uno spazio di Banach.\\

\textbf{Esercizio 1.1.24:} Sia $x_0$ fissato allora anche $|||f|||_2:=|f(x_0)|+||f'||_{\infty,(a,b)}$ è una norma ed inoltre $\frac{1}{b-a+1}|||f|||_1\leq |||f|||_2 \leq |||f|||_1$. Segue che le due norme sono equivalenti e $C^1B$ è uno spazio di Banach anche con la norma $|||$ $|||_2$

\section{Serie di funzioni}

\subsection{Convergenza di una serie di funzioni}

Siano $\{f_{k}\}:X\rightarrow\mathbb{R}$ costruisco $\sum_{k=0}^\infty f_{k}$ come limite della successione delle somme parziali  $S_n=\sum_{k=0}^{n}f_k$; $\sum_{k=0}^\infty f_{k}$ rappresenta il limite delle $S_n$ qualora esso dovesse esistere in qualche senso.\\

\textbf{Osservazione 1.2.0:} Con questa notazione si possono vedere le successioni come caso particolare delle successioni in realtà si può scrivere ogni serie come una successione e viceversa. Si potrebbero studiare prima le serie e poi le successioni viste come caso particolare di serie.\\

\textbf{Osservazione 1.2.1:} C'è un diffuso abuso di notazione: il simbolo $\sum_{k=0}^\infty f_{k}$ all'inizio indica la successione delle somme parziali, una volta stabilità la sua convergenza passa ad indicare il limite della successione


Dalla definizione di $\sum$ come limite della sequenza delle somme parziali segue che ogni risultato per la convergenza delle successioni si trasmette alla serie.
Quindi $\sum_{k=0}^\infty f_{k}$ converge uniformemente in $X$ se  la successione $S_n=\sum_{k=0}^{n}f_k$ converge uniformemente $\Rightarrow$ $\{S_n\}$ è di Cauchy ovvero $\forall{\epsilon}>0 \exists N=N(\epsilon): m,n\geq N \Rightarrow ||\sum_{k=n}^{m} f_{k}||_{\infty,X)}=||S_m-S_n||_{\infty,X}<\epsilon$\\

Supponiamo che $\sum_{k=0}^{\infty} f_{k}$ sia di Cauchy in norma $\sup$, sia X $\Rightarrow$ la sequenza $S_n$ è di Cauchy con norma $\sup$ su X $\Rightarrow S_n$ è limitata (perchè di Cauchy) $S_n\in B(X,\mathbb{R})$ e siccome $B$ è completo $\exists$ limite uniforme delle $S_n$ ovvero che: $$f:=\sum\limits_{k=0}^{\infty}f_k=\lim\limits_{k\to\infty}f_k$$

\textbf{Definizione 1.2.2:} Data $\{f_{k}\}:X\rightarrow\mathbb{R}$ la $\sum_{k=0}^\infty f_{k}$ converge uniformemente in X se e solo se è rispettata la condizione di Cauchy ovvero: $$\forall\epsilon >0 \exists N=N(\epsilon): m\geq n\geq N \Rightarrow ||\sum\limits_{k=n}^{m} f_{k}||_{\infty,X}<\epsilon$$

\textbf{Corollario 1.2.3:} Data $\{f_{k}\}:X\rightarrow\mathbb{R}$ se $\sum_{k=0}^\infty f_{k}$ converge uniformemente allora $||f_k||_{\infty,X)}\to 0$\\

\textbf{Osservazione 1.2.4:} Il criterio è solo necessario.\\

\textbf{Dimostrazione:} Basta prendere $m=n$ nella definizione. \\

\textbf{Corollario (Criterio di Weierstrass) 1.2.5:} Se $\sum_{k=0}^{\infty}||f_k||_{\infty,X}$ converge allora $\sum_{k=0}^{\infty}f_k$ converge uniformemente. \\

\textbf{Dimostrazione:} Fisso $\epsilon >0$, siccome $\sum_{k=0}^{\infty}||f_k||_{\infty,X}$ converge $\exists N=N(\epsilon): m\geq n\geq N \Rightarrow \sum_{k=n}^{m} ||f_{k}||_{\infty,X}\leq\epsilon$ ma allora $$||\sum_{k=n}^{m}f_{k}||_{\infty,X}\leq\sum_{k=n}^{m} ||f_{k}||_{\infty,X}<\leq\epsilon$$ quindi $\sum_{k=0}^\infty f_{k}$ soddisfa la condizione equivalente alla convergenza uniforme. $\Box$\\

\textbf{Osservazione 1.2.6:} Il valore di $\sum_{k=0}^{\infty}||f_k||_{\infty,X}$ non ha alcun ruolo, per applicare Weiestrass basta stabilire la sua esistenza come numero in $\mathbb{R}$. Quindi si può non calcolare il valore di $||f_k||_{\infty,X}$, basta avere delle  stime.\\

\textbf{Esercizio 1.2.7:} Stabilire se $\sum_{k=1}^{\infty}\frac{\sin{kx}}{k^2+e^kx}$ converge uniformemente.
Sia $f_k=\frac{\sin{kx}}{k^2+e^kx}$ allora $$|f_k(x)|\leq\frac{1}{k^2+e^{kx}}\leq\frac{1}{k^2}\Rightarrow ||f_k||_{\infty,\mathbb{R}}\leq\frac{1}{k^2}\Rightarrow\sum_{k=1}^{\infty}||f_k||_{\infty,\mathbb{R}}\leq\sum_{k=1}^{\infty}\frac{1}{k^2}\leq\infty$$ quindi converge per Weierstrass.\\

\textbf{Definizione 1.2.8:} Se $\sum_{k=0}^{\infty}||f_k||_{\infty,X}\leq\infty$ allora la serie converge totalmente.\\

\textbf{Osservazione 1.2.9:} $||f_k||_{\infty,X}=||$ $f_k$ $||_{\infty,X}$ quindi la convergenza totale implica quella assoluta.\\

\textbf{Osservazione 1.2.10:} La convergenza totale implica quella assoluta e uniforme ma non vale il viceversa.\\

\textbf{Esempio 1.2.11:} $f_k(x):=\frac{1}{x}\chi_{[k,k+1)}(x)$ allora la serie $\sum_{k=0}^{\infty} f_k(x)$ converge uniformemente e assolutamente a $\frac{1}{x}\chi_{[1,\infty)}(x)$ tuttavia $||f_k||_{\infty,X}=\frac{1}{x}$ e $\sum_{k=0}^{\infty}\frac{1}{x}$ non converge.\\

\textbf{Esercizio 1.2.12:} Provare che $\sum_{k=0}^{\infty}\frac{(-1)^k}{k+x}$ converge uniformemente in $[0,+\infty)$ ma non totalmente. \\

\textbf{Proposizione 1.2.13:}  $\{f_{k}\}:\Omega\subseteq\mathbb{R}\to\mathbb{R}$ ($\Omega$ aperto) con le $f_k$ continue in $x_0$ e supponiamo che $\sum_{k=0}^\infty f_{k}$ converga uniformemente allora $F(x)=\sum_{k=0}^\infty f_{k}$ è continua in $x_0$\\

\textbf{Proposizione 1.2.14:}  $\{f_{k}\}:[a,b]\subseteq\mathbb{R}\to\mathbb{R}$ $f_k\in\mathcal{R}([a,b])$, supponiamo che $\sum_{k=0}^\infty f_{k}$ converga uniformemente in [a,b] allora $F(x)=\sum_{k=0}^\infty f_{k}\in\mathcal{R}([a,b])$ e inoltre: $$\int_a^b\sum_{k=0}^\infty f_{k}dx=\int_a^bF(x)dx=\int_a^b\lim\limits_{n\to\infty}S(x)dx=$$ $$\lim\limits_{n\to\infty}\int_a^b S(x)dx=\lim\limits_{n\to\infty}\sum_{k=0}^\infty\int_a^b f_k(x)dx=\sum_{k=0}^\infty\int_a^b f_k(x)dx$$ 

\textbf{Proposizione 1.2.15:} $\{f_{k}\}:(a,b)\subseteq\mathbb{R}\to\mathbb{R}$ con ogni $f_k$  è derivabile in $(a,b)$ ed esiste $x_0\in (a,b)$ tale che $\sum_{k=0}^\infty f_{k}(x_0)$ converge, supponiamo che $\sum_{k=0}^\infty f'_{k}$ converge uniformemente in $(a,b)$ allora la $F(x)=\sum_{k=0}^\infty f_{k}$ converge semplicemente in $(a,b)$, la convergenza è uniforme su ogni compatto contenuto in $(a,b)$ e la $F(x)$ è derivabile con $F'(x)=\sum_{k=0}^\infty f'_{k}$ ovvero: 

$$\left(\frac{d}{dx}\sum f_k\right)(x)=\sum\left(\frac{d}{dx}\right)(x)$$

\subsection{Serie di potenze}

\textbf{Definizione 1.2.16:} Chiamiamo serie di potenze un arnese siffatto: $$\sum_{k=0}^{\infty}a_k(x-x_0)^k$$ con $\{a_k\}$ serie numerica e $x_0\in\mathbb{R}$, $x_0$ è detto centro. \\

\textbf{Osservazione 1.2.17:} Se pongo $x-x_0=w$ ottengo una serie di potenze di centro 0, questo procedimento si chiama riduzione in forma standard. D'ora in avanti cercheremo di studiare serie centrate in 0. \\

Data una serie di potenze cerchiamo di studiare il dominio di convergenza $D:=\{x\in\mathbb{R}:\sum_{k=0}^{\infty}a_k x^k$ converge $\}$. \\

\textbf{Osservazione 1.2.18:} Il dominio non è mai vuoto infatti $0\in D$.\\

\textbf{Lemma 1.2.19:} Se la serie converge in $w$ allora converge puntualmente in $(-|w|,|w|)$ e la convergenza è totale nei compatti K contenuti in $(-|w|,|w|)$.\\

\textbf{Dimostrazione:} Se $w=0$ è banale, supponiamo $x\ne 0$ allora $\sum_{k=0}^{\infty}a_k w^k$ è una serie numerica convergente percio $\{a_k w^k\}$ tende a 0 $\Rightarrow$ limitata ovvero $\exists c:|a_k w^k|<c$ perciò $$|a_k x^k|=|a_k x^k\cdot\left (\frac{x}{w} \right )^k|\leq c \cdot \left(\frac{|x|}{|w|} \right)^k$$ Siccome siamo in un compatto $K$ allora $\exists\alpha$ con $0<\alpha <|w|:K\subseteq[-\alpha , \alpha]$ allora $|a_k x^k|\leq c \cdot \left(\frac{\alpha}{|w|} \right)^k$ $\Rightarrow ||a_k x^k||\leq c \cdot \left(\frac{\alpha}{|w|} \right)^k$ e tende a 0 quindi $\sum ||a_k x^k|| \leq \sum c \cdot \left(\frac{\alpha}{|w|} \right)^k<\infty$ ovvero la convergenza è totale.$\Box$ \\

\textbf{Definizione 1.2.20:} Chiamiamo raggio di convergenza $\rho := \sup D$. Si ha che $(-\rho,\rho)\subseteq D \subseteq [-\rho,\rho]$, se $\rho=\infty$ intendiamo che $D=\mathbb{R}$.\\

\textbf{Criterio di Cauchy - Hadamard 1.2.21:} Posto $l:=\limsup\limits_{k\to\infty}\sqrt[k]{|a_k|}$ allora si ha che $\rho=\frac{1}{l}$ (se $l=\infty$ allora $\rho=0$ e viceversa).\\

\textbf{Criterio di d'Alembert 1.2.22:} Se $a_k \ne 0 \forall k$ allora se esiste $l:=\lim\limits_{k\to\infty} \left|\frac{a_{k+1}}{a_k}\right|$ in $\overline{\mathbb{R}}$ si ha che $\rho=\frac{1}{l}$\\

\textbf{Dimostrazioni:} Sono conseguenze immediate del criterio della radice e del confronto per le serie numeriche\\

\textbf{Osservazione 1.2.23:} Il criterio di Chauchy - Hadamard è sempre applicabile in quanto il $\limsup$ esiste sempre. Al contrario il criterio di d'Alembert potrebbe "fallire" e quindi è preferibile applicare il criterio precedente\\

\textbf{Esempi 1.2.24:} \begin{itemize}
\item $\sum{k=1}^{\infty}\frac{x^k}{k^2}$ allora $\left|\frac{a_{k+1}}{a_k}\right|=\frac{k^2}{1+k^2}\to 1$ quindi $l=\rho=1$ e converge sia in $x=1$ e $x=-1$ $\Rightarrow D=[-1,1]$
\item $\sum{k=0}^{\infty} k^2x^k$ e dal criterio del rapporto $l=\rho=1$ ma converge solo in $(-1,1)$
\item $\sum{k=1}^{\infty}\frac{x^k}{k}$ converge in $[-1,1)$
\item $\sum{k=0}^{\infty}\frac{x^k}{k!}$  e $\left | \frac{a_{k+1}}{a_k}\right|=\frac{1}{k+1}\to 0\Rightarrow\rho = \infty$ (con $k!$ a numeratore avremmo ottenuto $\rho=0$)\\
\end{itemize}

\textbf{Teorema di Abel 1.2.25:} Se la serie $\sum_{k=0}^{\infty}a_k x^k$ converge in $w$ allora converge uniformemente in $[0,w]$ o $[w,0]$ (a seconda che $w$ sia positivo o negativo).\\

\textbf{Dimostrazione: }Possiamo supporre $w=1$ (basta cambiare $a_k$ con $a_kw^k$) e $\sum_{k=0}^\infty a_k=0$ (basta prendere $a_0\to a_0-\sum_{k=0}^\infty a_k$).\\

Sia $A_k:=\sum_{l\leq k} a_l$ (con $A_{-1}=0$) allora:

$$\sum_{k=0}^na_kx^k=\sum_{k=0}^n(A_k-A_{k-1}) x^k=\sum_{k=0}^nA_kx^k-\sum_{k=0}^nA_{k-1}x^k=\sum_{k=0}^nA_kx^k-\sum_{k=0}^{n-1}A_{k}x^k$$

(questo perchè abbiamo posto $A_{-1}=0$)

$$=A_nx^n-\sum_{k=0}^{n-1}A_{k}(x^{k+1}-x^k)$$

e quindi se $m>n$:

$$\sum_{k=n+1}^ma_kx^k=\sum_{k=0}^ma_kx^k-\sum_{k=0}^n=A_mx^m-A_nx^n-\sum_{k=n}^{m-1}A_k(x^{k+1}-x^k)$$

Ma per ipotesi $\sum_{k=0}^\infty a_k=0$, ovvero $A_k\to 0$ per $k\to\infty$ quindi se fisso $\epsilon >0$ $\exists N$ tale che $\forall k\geq N$ si ha che $|A_k|\leq\epsilon$. Assumo $m\geq n\geq N$ così:

$$\left|\sum_{k=n}^ma_kx^k\right|\leq \epsilon|x|^m+\epsilon|x|^n+\epsilon\sum_{k=n}^{m-1}|x^{k+1}-x^k|\quad (*)$$

Se $x\in[0,1]$ allora $|x|\leq 1$ e $|x^{k+1}-x^k|=x^{k+1}-x^k$ da cui:

$$\sum_{k=n}^{m-1}|x^{k+1}-x^k|=\sum_{k=n}^{m-1}(x^{k+1}-x^k)=x^n-x^{m-1}\leq x^n \leq 1$$

e si ha $|\sum_{k=n}^m a_k x^k|\leq 3\epsilon\quad\forall x\in[0,1]$, ovvero:

$$\left|\left|\sum_{k=n}^m a_k x^k\right|\right|_{\infty,[0,1]}\leq 3\epsilon$$

quindi il criterio per la convergenza uniforme è soddisfatto. $\Box$\\

\textbf{Corollario 1.2.26:} La convergenza della serie è uniforme in D.\\

\textbf{Teorema 1.2.27:} Sia $\sum_{k=0}^\infty a_kx^k$ con $\rho >0$. Allora $f:(-\rho,\rho)\to\mathbb{R}$ con $f(x)=\sum_{k=0}^\infty a_kx^k\in\mathcal{C}^\infty((\-\rho,\rho))$\begin{itemize}
\item $f^{(l)}=\sum_{k=0}^\infty a_k\cdot\underbrace{k \cdot (k-1) \cdots (k-l+1)}_{\mbox{l}}\cdot x^{k-l}=\sum_{k=0}^\infty a_{k+l}\cdot\underbrace{ (k+l) \cdots (k-1)}_{\mbox{l fattori}}\cdot x^{k}$
\item Il raggio di convergenza di $f^{(l)}$ è sempre $\rho$
\item $f^{(l)}(0)=a_l\cdot l!$ ovvero $a_l=\frac{f^{(l)}(0)}{l!}$\\
\end{itemize}

\textbf{Dimostrazione:} Osservo che $f$ è una serie di polinomi $a_kx^k$ che quindi sono certamente funzioni $\mathbb{C}^1$. Infatti la serie delle derivate: $$\sum_{k=0}^\infty a_k k x^{k-1}=a_1+2a_2x+3a_3x^2+\cdots =\sum_{k=0}^\infty a_{k+1} (k+1) x^{k}$$ è ancora una serie di potenze. Sia $\rho'$ il suo raggio di convergenza allora $\rho'=\frac{1}{l'}$ dove $l'=\limsup\limits_{k\to\infty}[(k+1)|a_{k+1}]^{\frac{1}{k}}$. Osservo che $$[(k+1)|a_{k+1}]^{\frac{1}{k}}=e^{\frac{1}{k}\log{k+1}+\frac{1}{k}\log{a_{k+1}}}$$ (questo vale anche se $a_{k+1}=0$ intendendo $e^{-\infty}=0$). Inoltre la funzione esponenziale è monotona crescente quindi $\limsup\limits_{k\to\infty} e^{b_k}=e^{\limsup\limits_{k\to\infty} b_k}\Rightarrow$ $$l'=\limsup\limits_{k\to\infty}e^{b_k}=e^{[\limsup\limits_{k\to\infty}[\frac{1}{k}\log(k+1)+\frac{1}{k}\log|a_{k+1}|]]}$$ ma $\frac{1}{k}\log(k+1)\to 0$ quindi: $$l'=e^{[\limsup\limits_{k\to\infty}[0+\frac{1}{k}\log|a_{k+1}|]]}=e^{[\limsup\limits_{k\to\infty}[\frac{k+1}{k}\cdot\frac{1}{k+1}\log|a_{k+1}|]]}$$ ma ma $\frac{k+1}{k}\to 1$ quindi: $$l'=e^{[\limsup\limits_{k\to\infty}[\frac{1}{k+1}\log|a_{k+1}|]]}=\limsup\limits_{k\to\infty}|a_{k+1}|^{\frac{1}{k+1}}=l$$ Da cui $\rho'=\rho$. Ne segue che la serie delle derivate converge in $(-\rho,\rho)$, totalmente nei compatti. Ma allora su ciascun aperto $(a,b)$ con $-\rho<a<0<b<\rho$ valgono le ipotesi del teorema di derivazione (con $0$ come $x_0$), da cui segue che $f$ è $\mathcal{C}^1$ con $f'=\sum_{k=0}^\infty a_kkx^{k-1}$ in $(-\rho,\rho)$.  Iterando il processo (cosa possibile perchè la derivata è una serie di potenze) si ha la tesi. $\Box$\\

\textbf{Osservazione 1.2.28:} La convergenza è totale nei compatti contenuti in $(-\rho,\rho)$ perciò: $$\int_0^x(\sum_{k=0}^\infty a_ku^k)du=\sum_{k=0}^\infty a_k\int_0^xu^kdu=\sum_{k=0}^\infty \frac{a_kx^{k+1}}{k+1}$$ Quindi la funzione primitiva di una serie di potenze è ancora una serie di potenze. Questa identità vale $\forall x \in D$\\

\textbf{Osservazione 1.2.29:} Da Abel segue che se $\sum_{k=0}^\infty a_k\rho^k$ converge allora l'uguaglianza $\int_0^x(\sum_{k=0}^\infty a_ku^k)du=\sum_{k=0}^\infty \frac{a_kx^{k+1}}{k+1}$ vale anche per $x=\rho$.\\ 

\textbf{Osservazione 1.2.30:} Date due serie di potenze: $\sum_1=\sum_{k=0}^\infty a_k x^k$ e  $\sum_2=\sum_{k=0}^\infty b_k x^k$ di raggio rispettivamente $\rho_1$ e $\rho_2$ definiamo la serie somma e la serie prodotto come: \begin{itemize}
\item $\sum_1+\sum_2=\sum_{k=0}^\infty (a_k+b_k)x^k$
\item $\sum_1\cdot\sum_2=\sum_{k=0}^\infty(\sum\limits_{u,v\geq 0}(a_u b_v)x^k$
\end{itemize} Allora si hanno le seguenti proprietà (verificare per esercizio)\begin{enumerate}
\item $\rho_{(\sum_1+\sum_2)}$ e $\rho_{(\sum_1\cdot\sum_2)}$ sono entrambi $\leq\min\{\rho_1,\rho_2\}$
\item $(\sum_1+\sum_2)=\sum_1(x)+\sum_2(x)$ se $|x|<\min\{\rho_1,\rho_2\}$ (analogo per il prodotto)
\item Se $\rho_1\neq\rho_2$ allora $\rho_{(\sum_1+\sum_2)}=\min\{\rho_1,\rho_2\}$ (non vale per il prodotto)\\
\end{enumerate}

\textbf{Osservazione 1.2.31:} L'insieme $\{\sum_{k=0}^\infty a_k x^k, \rho\leq 1\}$ gode di buone proprietà analitiche (le $\sum$ sono di classe $\mathcal{C}^\infty$ e si conserva per derivazione e integrazione $i.e$ è un anello differenziale) e algebriche (è un anello commutativo con unità di cui l'insieme dei polinomi è un suo sottoanello) \\

Se una funzione $f$ è esprimibile come serie di potenze allora è di classe  $\mathcal{C}^\infty$. Non vale il viceversa, neanche localmente.\\

\textbf{Esempio 1.2.32:} Se $f$ è una serie di potenze $f(x)=\sum_{k=0}^\infty a_k x^k$ allora $f^{(l)}(0)=a_l l!\Rightarrow$ se è una serie di potenze è $\sum_{k=0}^\infty \frac{f^{(l)}(u)}{l!}x^l$ (unicità della rappresentazione). Sia ora: $$f(x)=
\begin{cases}
 e^{-\frac{1}{x^2}} &  x\ne 0 \\ 
0 & x=0
\end{cases}$$  f è $\mathcal{C}^\infty$ e $f^{(l)}(0)=0$ $\forall l$ ma allora $f$ è rappresentabile dalla serie nulla che rappresenta la funzione $g\equiv 0$ che non è $f$\\

\textbf{Proposizione 1.2.33:} Sia $A\subseteq\mathbb{R}$ aperto indichiamo con $\Omega(A)$ l'insieme $\{f:A\to\mathbb{R}, f\in\mathcal{C}^\infty$ e tali per cui in ogni punto $x_0\in A$ la serie di potenze $\sum_{k=0}^\infty \frac{f^{(l)}(x_0)}{l!}(x-x_0)^k$ converge ad f in un intorno di $x_0\}$. Le funzioni di $\Omega(A)$ sono dette analitiche e l'esempio mostra che $\Omega(A)\subset\mathcal{C}^\infty$ (strettamente)

\textbf{Proposizione 1.2.34:} Data $\sum_{k=0}^\infty a_k x^k$ con $\rho>0$ allora $f:(-\rho,\rho)\to\mathbb{R}$ $f(x)=\sum_{k=0}^\infty a_k x^k$ $|x|<\rho$ è analitica in $(-\rho,\rho)$\\

Se una funzione $f$ è rappresentabile come serie di potenze di centro $x_0$ in un intorno del punto allora la serie che rappresenta il punto è il suo sviluppo di Taylor centrato in $x_0$\\

\textbf{Proposizione 1.2.35:} Sia $f:(-r,r)\subseteq\mathbb{R}\to\mathbb{R}$ di classe $\mathcal{C}^\infty$ in $(-r,r)$, supponiamo che $||f^{(k)}||_{\infty,(-r,r)}\ll\frac{k!}{r^k}$ per $k\to\infty$ allora $f$ è rappresentata dalla sua serie di Taylor.\\

\textbf{Dimostrazione:} Fissato $x\in (-r,r)$ valutiamo la differenza $f(x)-\sum_{k=0}^{N-1} \frac{f^{(k)}(0)}{k!}(x)^k$ e verifichiamo che tende a 0. Il secondo termine della differenza il polinomio di Taylor arrestato all'ordine N-1, da Lagrange sappiamo che $\exists\xi\in(0,x)$ tale che la differenza considerata sopra è uguale a $\frac{f^{(N)}(\xi)}{N!}x^N$. Siccome $|\xi|<r$ si ha che: $$|f(x)-\sum_{k=0}^{N-1} \frac{f^{(k)}(0)}{k!}(x)^k|=|\frac{f^{(N)}(\xi)}{N!}x^N|\leq|f^{(N)}(\xi)|\frac{|x|^N}{N!}\leq||f^{(N)}||\cdot\frac{|x|^N}{N!}\ll\frac{N!}{r^N}\cdot\frac{|x|^N}{N!}=\left(\frac{|x|}{r}\right)^N\to 0$$ poichè $|x|<r$ . $\Box$\\

\textbf{Teorema 1.2.36:} $e^x=\sum_{k=0}^\infty \frac{x^k}{k!}$\\

\textbf{Dimostrazione:} Fisso $r>0$, si ha che $f^{(k)}(x)=e^x$ $\forall k in \mathbb{N}$ allora $|f^{(k)}(x)|=|e^x|\leq e^r \Rightarrow ||f^{(k)}||_{\infty,(-r,r)}=e^r\ll\frac{k!}{r^k}$ ed è vero poichè il rapporto $\frac{e^r r^k}{k!}$ tende a 0 ($e^r$ è limitato in un compatto K)  \fbox{\phantom{3}}\\

\textbf{Teorema 1.2.37:} $\sin(x)=\sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)!}x^{2k+1}$ e $\cos(x)=\sum_{k=0}^\infty \frac{(-1)^k}{(2k)!}x^{2k}$\\

\textbf{Esercizio 1.2.38:} Verificare tramite serie di potenze:\begin{enumerate}
\item $e^x\cdot e^y=e^{x+y}$
\item $\sin(x+y)=\sin(x)\cos(y)-\cos(x)\sin(y)$
\item $\exp^{ix}=\cos(x)+i\sin(x)$
\end{enumerate}

\chapter{Funzioni implicite}

\section{Premessa}

L'obiettivo è quello di descrivere il luogo degli zeri di opportune funzioni $f(\underline{x})=0$ per opportune classi di funzioni $f$.\\

\textbf{Osservazione 2.1:} \begin{enumerate}
\item Ogni grafico è il luogo degli zeri di qualcosa infatti sia $y=g(x)$ la funzione $f(x,y)=y-g(x)$ ha per luogo degli zeri il grafico di $g$.
\item L'unione (anche molteplice) di zeri è luogo di zeri: $Z_f=\{(x,y):f(x,y)=0\}$ e $Z_g=\{(x,y):g(x,y)=0\}$ allora $Z_f \cup Z_g = Z_{f\cdot g}$ e  $Z_f \cap Z_g = Z_{f^2+g^2}$.
\item In generale i luoghi di zeri non sono grafici di funzioni ad esempio $x^2+y^2-1=0$.
\end{enumerate}

L'obiettivo dei prossimi teoremi è dimostrare che se $f$ è abbastanza "buona" allora $Z_f$ è localmente il grafico di una funzione. Per esempio:\\

\begin{center}
\begin{tikzpicture}
      \draw[->] (-1.2,0) -- (1.2,0) node[right] {$x$};
      \draw[->] (0,-1.20) -- (0,1.2) node[above] {$y$};
\draw (0,0) circle (1);
\filldraw[black] ({(2^0.5)/2},{(2^0.5)/2}) circle (2pt) node[anchor=west] {Localmente il grafico di $y=\sqrt{1-x^2}$};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}
      \draw[->] (-1.2,0) node[left] {$x$} -- (1.2,0);
      \draw[->] (0,-1.20) -- (0,1.2) node[above] {$y$};
\draw (0,0) circle (1);
\filldraw[black] (1,0) circle (2pt) node[anchor=west] {$\mbox{  }$Localmente il grafico di $x=\sqrt{1-y^2}$};
\end{tikzpicture}
\end{center}

\textbf{Proposizione 2.2:} Data $f:[a,b]\times[c,d]\to\mathbb{R}$ supponiamo:\begin{enumerate}
\item $\forall x \in [a,b]$, $f(x,\bullet):[c,d]\to\mathbb{R}$ è continua e $f(x,c)\cdot f(x,d)<0$
\item $\forall x \in [a,b]$, $f(x,\bullet)$ è strettamente monotona
\end{enumerate}
Allora il grafico degli zeri di $f$ in $[a,b]$x$[c,d]$ coincide con il grafico di una funzione $\phi: [a,b]\to[c,d]$.\\

\textbf{Dimostrazione:} Fisso $x_0\in[a,b]$, da 1 (teorema degli zeri) esiste almeno un punto in cui la funzione $f$ si annulla, da 2 segue che è unico. Poichè vale $\forall x$ si ha la tesi. $\Box$ \\

\section{Teorema di Dini}

Una volta stabilita l'esistenza della funzione $\phi$ ci chiediamo come è possibile studiarne la regolarità. \\

\textbf{Teorema di Dini (monodimensionale) 2.3:} Sia $f:\Omega\subseteq\mathbb{R}^2\to\mathbb{R}$, con $\Omega$ aperto e $f\in\mathcal{C}^1(\Omega)$:
\begin{enumerate}
\item Sia $(x_0,y_0)\in\Omega$ tale che $f(x_0,y_0)=0$
\item $\partial y f(x_0,y_0)\ne 0$
\end{enumerate}

Allora $\exists$ un intorno aperto $(a,b)\times(c,d)$ con $(x_0,y_0)\in(a,b)\times (c,d)$ e $[a,b]\times[c,d]\subseteq\Omega$ ed esiste una funzione $\phi:(a,b)\times(c,d)$ tale che il luogo degli zeri di $f$ in $(a,b)\times(c,d)$ coincide con il grafico di $\phi$. Inoltre $\phi\in\mathcal{C}((a,b))$ e: $$\frac{d\phi}{dx}(x)= - \frac{\partial xf}{\partial yf} (x,\phi(x)),\quad \forall x \in (a,b)$$

\textbf{Dimostrazione:} Assumiamo $\partial_y f(x_0,y_0)>0$ (altrimenti si cambia $f$ con $-f$), siccome $\partial y f$ è continua  $\exists [a',b']\times[c,d]\subseteq\Omega$ contenente il punto $(x_0,y_0)$ in cui $\partial y f>0$.\\
\begin{center}
\begin{tikzpicture}
\draw[->] (-0.2,0) node[left] {$x$} -- (4.2,0);
      \draw[->] (0,-0.20) -- (0,4.2) node[above] {$y$};
\draw [black, dashed] plot [smooth cycle] coordinates {(1,3.5) (2,2.5) (3,4) (4,2) (3.5,1) (2,-1) (0.5,1)} node[anchor=east] {$\mbox{ }\mbox{ }\mbox{ }\Omega$};
\draw[-] (1.5,2) -- (2.5,2) ;
\draw[-] (1.5,0.3) -- (2.5,0.3);
\draw[-,dashed] (1.5,0)node[anchor=north] {$a'$} -- (1.5,2) ;
\draw[-,dashed] (2.5,0)node[anchor=north] {$b'$} -- (2.5,2) ;
\draw[-,dashed] (0,2) node[anchor=east] {$d$} -- (1.5,2);
\draw[-,dashed] (0,0.3)node[anchor=east] {$c$} -- (1.5,0.3);
\filldraw[black] (1.8,1.5) circle (1pt) node[anchor=west] {$(x_0,y_0)$};
    \end{tikzpicture}
    \end{center}


Considero $f(x_0,\bullet):[c,d]\to\mathbb{R}$ essa vale 0 in $y_0$ ed è strettamente crescente quindi $f(x_0,c)<0$ e $f(x_0,d)>0$. Ma $f$ è continua perciò $\exists\mathcal{U}((x_0,c))$ dove $f<0$ e $\exists\mathcal{U}((x_0,d))$ dove $f>0$. Allora $\exists [a,b]$ con $a'\leq a \leq x_0 \leq b \leq b'$ in cui $f(x,c)<0$ e $f(x,d)>0$ $f(x,c)<0$ $\forall x\in[a,b]$. Ristretta ad $[a,b]\times[c,d]$ la funzione $f$ soddisfa le ipotesi della proposizione 2.2 perciò $\exists\phi:(a,b)\to(c,d)$ tale che il luogo degli zeri di $f$ coincide con il grafico di $\phi$.\\

\begin{center}
\begin{tikzpicture} [scale=2]
\draw[-,dashed] (0,0)node[anchor=east] {$c$}--(0,2) node[anchor=east] {$d$};
\draw [-,dashed] (1,0)--(1,2);
\draw (0,0) node[anchor=north] {$a$}--(1,0)node[anchor=north] {$b$};
\draw (0,2) --(1,2);
\filldraw[black] (0.5,2) circle (0pt) node[anchor=south] {$+$};
\filldraw[black] (0.5,0) circle (0pt) node[anchor=north] {$-$};
\draw [black, ] plot [smooth ] coordinates {(0.1,0.2) (0.3,0.5) (0.5,0.9) (0.8,1.8)};
\end{tikzpicture}
    \end{center}

Dimostriamo ora la regolarità della funzione $\phi$. Sia $x\in(a,b)$ fissato e $h$ tale che $x+h\in(a,b)$. $f$ si annulla in $(x,\phi(x))$ e $(x+h,\phi(x+h))$ (per come è stata costruita $\phi$, inoltre è $\mathcal{C}^1(\Omega)$ allora: 

$$0=f(x+h,\phi(x+h))-f(x,\phi(x))=\langle \nabla f(\alpha,\beta),(h,\phi(x+h)-\phi(x)\rangle$$ Dove $(\alpha,\beta)$ sta nel segmento congiungente $(x,\phi(x))$ e $(x+h,\phi(x+h))$ perciò: 

$$(\phi(x+h)-\phi(x))\cdot\partial y f(\alpha,\beta)+h\partial x f(\alpha,\beta)=0$$ 
ovvero $\phi(x+h)-\phi(x)=-\frac{\partial x f}{\partial y f}(\alpha,\beta)\cdot h (*)$ (ciò è possibile perchè $\partial y f(\alpha,\beta)\ne 0$) ma $(\alpha,\beta)\in[a',b']\times[c,d]$ e $\frac{\partial x f}{\partial y f}$ è continua (quoziente di funzioni continue) quindi ha massimo $M:=||\frac{\partial x f}{\partial y f}||_{\infty,[a',b']\times[c,d]}$, da $(*)$ segue che: $$\phi(x+h)-\phi(x)\leq M\cdot |h|$$
quindi $\phi$ è lipschitziana in $x$ $\Rightarrow$ continua. Questo mostra che se $h\to 0$ allora $(\alpha,\beta)\to(x,\phi(x))$ perciò $\lim\limits_{h\to 0}\frac{\partial x f}{\partial y f}(\alpha,\beta)$ esiste e vale $\frac{\partial x f}{\partial y f}(x,\phi(x))$ e così: 

$$\frac{\phi(x+h)-\phi(x)}{h}=-\frac{\partial x f}{\partial y f}(\alpha,\beta)\to -\frac{\partial x f}{\partial y f}(x,\phi(x))$$
 Allora $\phi$ è derivabile in $x$ con $\phi '=-\frac{\partial x f}{\partial y f}(x,\phi(x))$ e $\phi '$ è continua perchè composizione di continue. $\Box$\\

\textbf{Corollario 2.4:} Se poi $f\in\mathcal{C}^k(\Omega)$ allora $\phi$ è di classe $\mathcal{C}^k$ dove esiste ($k\geq 1$).\\

\textbf{Dimostrazione:} Uso $\phi '$ in modo iterato. $\Box$ \\

\textbf{Osservazione 2.5:} Se $\partial x f(x_0,y_0)\ne 0$ si può procedere come prima ma col ruolo di $x$ e $y$ invertiti ovvero $\exists\psi:(c,d)\to(a,b)$ tale che il grafico di $\psi$ coincide localmente col luogo degli zeri di $f$ cioè: $f(\psi(y),y)=0\quad\forall y\in (c,d)$. Se $\nabla f(x_0,y_0)\ne 0$ il luogo degli zeri è localmente il grafico di qualcosa, ciò porta a definire critici quei punti in cui il gradiente si annulla e non è possibile applicare Dini.\\

\textbf{Esempi 2.6:}\begin{itemize}
\item $f(x,y)=x^2-y^2$ $f(0,0)=0$ ma $\nabla f(0,0)=(0,0)$ e il luogo degli zeri di f sono le rette $y=x$ e $y=-x$ che in un intorno di $(0,,0)$ non è il grafico di una funzione.
\begin{center}
\begin{tikzpicture}[scale= 0.5]
      \draw[->] (-3.2,0) -- (3.2,0) node[right] {$x$};
      \draw[->] (0,-3.20) -- (0,3.2) node[above] {$y$};
      
      \draw[scale=1,domain= -3.2:3.2,smooth,variable=\x,blue] plot ({\x},{-\x});
      \draw[scale=1,domain=-3.2:3.2,smooth,variable=\x,blue] plot ({\x},{\x});
      \end{tikzpicture}
\end{center}
\item $f(x,y)=x^2+y^2$ $f(0,0)=0$ ma $\nabla f(0,0)=(0,0)$ e il luogo degli zeri è il solo punto $(0,0)$ che non è un grafico.
\item $f(x,y)=y^2-x^2(x+1)$ $f(0,0)=0$ (parabola campaniformis cum ovali) ma $\nabla f(0,0)=(0,0)$ e il luogo degli zeri non è il grafico di nessuna funzione in $(0,0)$

\begin{center}
\begin{tikzpicture}[scale= 1]
      \draw[->] (-1.2,0) -- (1.2,0) node[right] {$x$};
      \draw[->] (0,-1.20) -- (0,1.2) node[above] {$y$};
      \draw[scale=1,domain= -1.0:1.0,smooth,variable=\x,blue] plot ({\x},{-((\x*\x*\x+\x*\x)^0.5)});
      \draw[scale=1,domain=-1.0:1.0,smooth,variable=\x,blue] plot ({\x},{(\x*\x*\x+\x*\x)^0.5});
      \end{tikzpicture}
\end{center}
\end{itemize}

\section{Contrazioni e Teorema del punto fisso}

Ora ci prendiamo una pausa per enunciare un importante risultato negli spazi metrici completi:\\

\textbf{Definizione 2.7:} Sia $(X,d)$ uno spazio metrico completo e $T$ una mappa $T:X\to X$, diciamo che $T$ è una contrazione se $\exists\lambda<1$ tale che $d(Tx,Ty)<\lambda\cdot d(x,y)$ $\forall x,y$ (T è lipschitziana con costante di lipschitz $<1$).\\

\textbf{Teorema di Banach - Cacciopoli 2.8:} Sia $(X,d)$ uno spazio metrico completo e $T$ una contrazione allora l'equazione $Tx=x$ ha una e una sola soluzione.\\

\textbf{Dimostrazione:} Sia $x_0\in X$ fissato a caso e sia $\{x_n\}$ la successione: $x_{n+1}=Tx_n$ (quindi $x_n=\underbrace{ T\circ\ldots\circ T}_{n}x_0$). Dalla disuguaglianza triangolare segue che $d(x,y)\leq d(x,Tx)+d(Tx,Ty)+d(Ty,y)\leq d(x,Tx)+\lambda d(x,y)+d(Ty,y)$, da cui si ricava: $$d(x,y)\leq \frac{1}{1-\lambda}(d(x,Tx)+d(y,Ty))\quad(*)$$ Osservo che $d(x_{n+1},x_n)=d(Tx_n,Tx_{n-1})\leq \lambda d(x_n,x_{n-1})\leq \ldots \leq \lambda^n d(x_1,x_0)\quad(**)$. Applico $(*)$ e $(**)$ ai punti $x_n$ e $x_m$ ed ottengo $$d(x_n,x_m)\leq \frac{1}{1-\lambda}(d(Tx_n,x_n)+d(Tx_m,x_m))\leq \frac{\lambda^n + \lambda^m}{1-\lambda}d(x_1,x_0)$$ Fisso $\epsilon >0$, sia $N: \lambda^N<\epsilon$ allora se $n,m\geq N$ segue $d(x_n,x_m)\leq\epsilon\cdot\frac{2d(x_1,x_0)}{1-\lambda}$ perciò $\{x_n\}$ è una successione di Cauchy quindi converge perchè siamo in uno spazio metrico completo, sia $x_\infty := \lim\limits_{n\to\infty} x_n, \quad Tx_\infty=T(\lim\limits_{n\to\infty} x_n) =$ (per la continuità) $\lim\limits_{n\to\infty} Tx_n=x_\infty$.\\
Verifico ora l'unicità, siano $x$ e $y$ due punti fissi allora $d(x,y)\leq\frac{1}{1-\lambda}(d(Tx,x)+d(Ty,y))=0$ (in quanto $x=Tx$ e $y=Ty$) $\Rightarrow d(x,y)=0\Rightarrow x=y.\quad \Box$\\

\textbf{Osservazione 2.9:} La dimostrazione contiene un metodo costruttivo per trovare $x_\infty$. Inoltre passando al limite si ha $d(x_n,x_\infty)=\lim\limits_{m\to\infty}d(x_n,x_m)\leq\frac{ \lambda ^n}{1-\lambda}d(x_1,x_0)$ che dà la distanza di $x_n$ dal limite in funzione dei dati.\\

\textbf{Corollario 2.10:} Il teorema puó essere generalizzato: sia $(X,d)$ uno spazio metrico completo e $T:X\to X$. Supponiamo che $\exists K$ tale che $T^{(k)}=T\circ\ldots\circ T$ ($k$ volte) sia una contrazione allora $T$ ha un unico punto fisso.\\

\textbf{Dimostrazione:} Sia $x_\infty$ punto fisso per $T^{(k)}$ (che esiste per via del teorema) allora $T^{(k)}(Tx_\infty)=T^{(k+1)}x_\infty=T(T^{(k)} x_\infty)=Tx_\infty$ perció $Tx_\infty$ è fisso per $T^{(k)}$ ma $T^{(k)}$ ha $x_\infty$ come punto unico fisso $\Rightarrow Tx_\infty=x_\infty$ ovvero $x_\infty$ è punto fisso per $T$. $Box$\\

Devo dimostrare che è unico ma ogni punto fissato da $T$ è fissato da $T{(k)}$ quindi $T$ ha un solo punto fisso (altrimenti $T{(k)}$ non ne avrebbe uno solo).\\

\textbf{Osservazione 2.11:} Se $T$ è contrazione allora anche $T^{2}$ (mappa iterata) è contrazione e $d(T^2(x),T^2(y))=d(T(Tx),T(Ty))\leq\lambda d(Tx,Ty)\leq \lambda^2 d(x,y)$ e $\lambda^2<\lambda$ (vale anche per le altre potenze). Se peró ogni $T$ con interata contrattiva fosse contrattiva il corollario precedente sarebbe inutile ma non é questo il caso: \\

$T:\mathbb{R}\to\mathbb{R}$ con $T(x)=\cos (x)$ non é una contrazione ma $T^2(x)\cos(\cos(x))$ lo é. \\

\textbf{Esempio 2.12:} (Metodo di Newton) Il metodo di Newton serve per calcolare gli zeri di una funzione $f$. Idea: sia $x_0$ un punto scelto a caso e si trova la tangente al grafico nel punto $f(x_0)$. L'intersezione della tangente con l'asse delle ascisse darà un nuovo punto $x_1$, si prende poi la tangente nel punto $f(x_1)$ e si itera il procedimento. La relazione di ricorrenza è: $$x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$$ 

\begin{center}
\begin{tikzpicture}[scale=1.5]
      \draw[->] (-0.2,0) -- (3.2,0) node[right] {$x$};
      \draw[->] (0,-0.60) -- (0,3.2) node[above] {$y$};
      \draw[-,dashed] (2,0) -- (2,0.875) ;
      \draw[scale=1,domain= -0.0:3.0,smooth,variable=\x,blue] plot ({\x},{0.5*\x^2-0.5*\x-0.5*0.25});
      \draw[scale=1,domain= (1.25):3.0,smooth,variable=\x,red,dashed] plot ({\x},{1.5*\x-2.15});
      \filldraw[black] (2,0) circle (1pt) node[anchor=north] {$x_n$};
      \filldraw[black] (1.433333,0) circle (1pt) node[anchor=north] {$x_{n+1}$};
      \end{tikzpicture}
\end{center}

\textbf{Esercizio 2.13:} Voglio trovare una soluzione di $f(x)=0$ con $f(x)=x^3+2x-1$ utilizzando il metodo di Newton. $T(x)=x-\frac{f(x)}{f'(x)}=x-\frac{x^3+2x-1}{3x^2+2}=\frac{2x^3+1}{3x^2+2}$, se considero $\mathbb{R}$ intero é difficile dimostrare che $T$ é una contrazione perció osservo che $T:[0,+\infty)\to[0,+\infty)$ e che in questo sottointervallo $T'(x)=\frac{6x^4+12x^2-6x}{(3x^2+2)^2}\leq\frac{6x^4+12x^2}{(3x^2+2)^2}=\frac{3}{4}-\frac{3x^4-12x^2-6x}{(3x^2+2)^2}\leq\frac{3}{4}$ e $T'(x)\geq\frac{-6x}{(3x^2+2)^2}\geq\frac{-3}{4}$ quindi $|T'(x)|\leq\frac{3}{4}$. \\

Da Lagrange so che $|T(x)-T(y)|=|T'(\xi)|\cdot|x-y|\leq\frac{3}{4}|x-y|$ perció $T$ é una contrazione e quindi $\exists !$ punto fisso di $T$, ovvero soluzione di $Tx=x$ ovvero di $f(x)$. Non solo ma se $x_0$ é scelto a caso e $x_n=Tx_{n-1}$ allora: $|x_n-x_\infty|\leq\frac{\left(\frac{3}{4}\right)^n}{1-\frac{3}{4}}|x_1-x_0|=4\cdot\left(\frac{3}{4}\right)^n|\frac{2x_0^3+1}{3x_0^2+2}-x_0|$. Scelgo $x_0=0$ e ho:\begin{itemize}
	\item $x_0=0$
	\item $x_1=Tx_0=0,454545\ldots$
	\item $x_2=Tx_1=0,453398\ldots$
	\item $x_3=Tx_2=0,453397\ldots$
	\item $x_4=Tx_3=0,453397\ldots$\\
\end{itemize}

La successione sembra convergere più velocemente di quanto stimato. Ció non é colpa di $\frac{3}{4}$ che é una buona stima di $\sup|T'(x)|=0,7499$; osservo che $T^2(x)=T(Tx)=\frac{8x^9+51x^8+\ldots}{18x^8+39x^6+\ldots}$ ha una derivata che in modulo é $\leq\frac{1}{10}$. Ne segue che se $S:=^2$ e $x_n=S^nx_0=T^{2n}x_0$ allora per $x_0=0$ vale: $$|x_n-x_\infty|\leq\frac{1}{10^n}\cdot\frac{10}{9}\cdot\frac{4}{5}=\frac{8}{9}\cdot\frac{1}{10^n}$$ Il fattore $\frac{1}{10}$ é decisamente migliore del $\left(\frac{3}{4}\right)^2$ che si era ottenuto prima per la stessa sequenza.\\

\textbf{Esercizio 2.14:} Cercare le soluzioni di $f(x)=x^5+4x-4$ $f(x)=0$.\\

\textbf{Esercizio 2.15:} Trovare le soluzione di $f(x)=x^a+bx-c$ $f(x)=0$ con $a<0$, $0\leq c\leq b$ e $a^2\leq b$



\section{Teorema di Invertibilità locale}

Prima di enunciare e dimostrare il teorema è bene ricorda un lemma utile ai fini della dimostrazione. \\

\textbf{Lemma 2.16:} Sia $F:X\to M(m\times n,\mathbb{R})$ chiamo $|||F|||_{2,\infty,X}:=\left(\sum^m_{i=1}\sum^n_{j=1}||F_{ij}||^2_{\infty,X}\right)^{\frac{1}{2}}$, se considero $f:\Omega\subseteq\mathbb{R}^m\to\mathbb{R}^n$, $\Omega$ aperto convesso, di classe $\mathcal{C}^1(\Omega)$ allora $||f(x)-f(w)||_{2,\mathbb{R}^n}\leq|||Jf|||_{2,\infty,X}\cdot||x-w||_{2,\mathbb{R}^m}$.\\

\textbf{Dimostrazione:}   $f$, in quanto funzione vettoriale, è un elenco $f=(f_1,f_2,\ldots,f_n)$, considero $f_j:\Omega\subseteq\mathbb{R}^n\to\mathbb{R}$ quindi è una funzione a valori scalari e per il teorema di Lagrange $f_j(x)-f_j(w)=<\nabla f_j(\alpha),(x-w)>$ dove $\alpha$ sta nel segmento che congiunge $x$ e $w$. Per Chauchy-Schwarz: $$|f_j(x)-f_j(y)|^2\leq||\nabla f_j(\alpha)||_2^2\cdot||(x-w)||_2^2$$ e quindi: $$|f_j(x)-f_j(y)|^2\leq\left(\sum_{i=1}^m \left|\left|\frac{\partial f_j}{\partial x_i}\right|\right|^2_{2,\infty}\right)\cdot ||x-w||_2$$ Questo risultato vale per ogni $j=1\ldots n$ quindi sommo tra loro le varie disuguaglianze ed arrivo alla tesi. $\Box$\\

\textbf{Teorema di invertibilitá locale 2.17:} Sia $\underline{f}:\Omega\subseteq\mathbb{R}^n\to\mathbb{R}^n$, $\Omega$ aperto, supponiamo $\underline{f}\in\mathcal{C}^1(\Omega)$ e sia $\underline{x_0}\in\Omega$ con $J\underline{f}(\underline{x_0})$ invertibile. Allora esiste un intorno $\mathcal{U}(\underline{x_0})$ e un intorno $\mathcal{V}(\underline{f}(\underline{x_0}))$ aperti tali che $f|_{\mathcal{U}(\underline{x_0})}:\mathcal{U}(\underline{x_0})\to\mathcal{V}(\underline{f}(\underline{x_0}))$ é biunivoca  con $f^{-1}\in\mathcal{C}^1(\mathcal{V}(\underline{f}(\underline{x_0})))$. Inoltre: $$ (J\underline{f}^{-1})(f(x))=[(J\underline{f})(x)]^{-1}\quad\forall x \in \mathcal{U}(\underline{x_0})$$

(Per non appesantire la notazione verranno omesse le barrette per indicare i vettori).\\

\textbf{Dimostrazione: }La dimostrazione è un'applicazione del teorema di punto fisso. Prendo la funzione:

$$g(x):=((Jf)(x_0))^{-1}\cdot(f(x_0+x)-f(x))$$
 (è ben definita in $\Omega-x_0$, traslato di $\Omega$, perchè $((Jf)(x_0))^{-1}$ esiste per ipotesi). Osservo che $g(0)=0$ e $(Jg)(0)=((Jf)(x_0))^{-1}\cdot(Jf)(x_0)=I$. D'altra parte $f(x)=(Jf)(x_0)\cdot g(x-x_0)+f(x_0)$ e questa relazione evidenzia che f è invertibile in $\mathcal{U}(x_0)$ se e solo se $g$ è invertibile in $\mathcal{U}(x_0)-x_0$. Nel procedere con la dimostrazione potremmo quindi assumere come ipotesi aggiuntive $x_0=0$, $f(x_0)=0$ e $(Jf)(x_0)=I$.\\

Sia $H(x):=x-f(x)\in\mathcal{C}^1(\Omega)$ con $H(0)=0$ e $(JH)(0)=I-I=0$. Visto che le funzioni in $JH$ sono continue esiste $\epsilon >0$ sufficientemente piccolo affinchè $|||JH|||_{2,\infty,\overline{B}_\epsilon}\leq\frac{1}{2}$ (scelto arbitrariamente) dove $\overline{B}_\epsilon=\{x:||x||\leq\epsilon\}$ (bolla chiusa). Inoltre visto che $\det Jf$ è continua e che $\det Jf(0)=\det I=1$ possiamo scegliere $\epsilon$ in modo che alla relazione precedente si abbia anche $\det Jf(x)\ne 0$. \\

Dato che $|||JH|||_{2,\infty,\overline{B}_\epsilon}\leq\frac{1}{2}$ dal lemma si ha che: $$||H(x)-H(w)||\leq\frac{1}{2}\cdot||x-w||\quad \forall x,w\in\overline{B}_\epsilon \quad (*)$$ e così: $$||f(x)-f(w)||=||x-w-(H(x)-H(w))||\geq$$ $$||x-w||-||H(x)-H(w)||\geq||x-w||-\frac{1}{2}||x-w||=\frac{1}{2}||x-w||\quad (**)$$ da qui segue che $f$ è iniettiva in  $\overline{B}_\epsilon$ perchè se fosse $f(x)=f(w)$ da $(**)$ seguirebbe $||x-w||\leq 2\cdot 0\Rightarrow x=y$.\\

Sia $y\in \overline{B}_\frac{\epsilon}{2}$, ovvero $||y||\leq\frac{\epsilon}{2}$, definisco $H_y(x):=y+H(x)$ e osservo che $$||H_y(x)||=||y+H(x)||\leq||y||+||H(x)||=||y||+||H(x)-H(0)||$$ siccome $||y||\leq\frac{\epsilon}{2}$, $H(0)=0$ e $x\in\overline{B}_\epsilon$ ho: $$||H_y(x)||\leq\frac{\epsilon}{2}=\frac{1}{2}||x-0||\leq \frac{\epsilon}{2}+\frac{\epsilon}{2}(**)=\epsilon$$ Da qui ricaviamo che $H_y:\overline{B}_\epsilon\to\overline{B}_\epsilon$. Inolte $H_y(x)-H_y(w)=H(x)-y-H(w)+y=H(x)-H(w)$ allora $||H_y(x)-H_y(w)||=||H(x)-H(w)||\leq\frac{1}{2}||x-w||$ quindi $H_y(x)$ é una contrazione ma $\mathbb{R}^n$ con la metrica euclidea é uno spazio di Banach allora $H_y$ ha un unico punto fisso. Ovvero $\exists!x$ tale che: $$H_y(x)=x\Leftrightarrow y+H(x)=x \Leftrightarrow y+x-f(x)=x\Leftrightarrow y=f(x)$$ quindi ogni $y\in\overline{B}_\frac{\epsilon}{2}$ é immagine di uno e uno solo punto $x\in\overline{B}_\epsilon$ perció $f$ é suriettiva tra $\overline{B}_\frac{\epsilon}{2}$ e $\overline{B}_\epsilon$. Ció non basta perché il teorema parla di insiemi aperti e non chiusi.\\

Siano $x$ e $y$ come sopra $||x||=||y+x-f(x)||=||y+H(x)||\leq||y||+||H(x)||\leq||y||+\frac{1}{2}||x||(**)$ $\Rightarrow\||x||\leq2||y||$. In particolare se $y\in B_\frac{\epsilon}{2}$ (aperto), ovvero $||y||<\frac{\epsilon}{2}$, $||x||<\epsilon$ perció l'aperto va nell'aperto quindi $f$ é suriettiva tra $B_\epsilon$ e $B_\frac{\epsilon}{2}$.\\

Infine, non è detto che $f(B_\epsilon)\subseteq B_{\frac{\epsilon}{2}}$ (purtroppo!) sia allora $U:=B_\epsilon\cap f^{-1}(B_{\frac{\epsilon}{2}})$. $U$ è aperto (intersezione di aperti), $0\in U$ ($f(0)=0$), f è iniettiva su U (perchè lo è su $B_\epsilon$) e suriettiva su $B_{\frac{\epsilon}{2}}$ (perchè sappiamo che $\forall y\in B_{\frac{\epsilon}{2}}$ esiste un $x\in B_\epsilon$ tale che $f(x)=y$). Quindi $f:U\to B_{\frac{\epsilon}{2}}$ è biunivoca.\\

Sia $g:B_{\frac{\epsilon}{2}}\to U$ l'inversa di f tra questi insiemi. Allora $(**)$ dice che in $B_{\frac{\epsilon}{2}}$ (posto $x:=g(y)$ e $w:=g(z)$): $$||g(y)-g(z)||\leq 2||y-z||\quad (*3*)$$ ovvero che $g$ è lipschitziana in $B_{\frac{\epsilon}{2}}$.\\

Mostro che $g$ è differenziabile e che $Jg=(Jf)^{-1}$. Siccome $f$ è differenziabile in $x\in\Omega$: $$f(w)-f(x)=(Jf)(x)\cdot(w-x)+R(w,x)\quad(*4*)$$ con $\frac{||R(w,x)||}{||x-w||}\to 0$ per $x\to w\quad(*5*)$. Da $(*4*)$ segue che $$g(z)-g(y)=(Jf(x))^{-1}\cdot (z-y)-(Jf(x))^{-1}\cdot R(w,x)\quad (*6*)$$ perchè $Jf(x)$ è invertibile quando $x\in B_\epsilon$. Supponiamo $z\to y$ allora $w=g(z)\to g(y)=x$ perchè $(*3*)$ dice che $g$ è continua. Inoltre: $$\frac{||R(w,x)||}{||z-y||}=\frac{||R(w,x)||}{||w-x||}\cdot\frac{||w-x||}{||z-y||}=\frac{||R(w,x)||}{||w-x||}\cdot\frac{||g(z)-g(y)||}{||z-w||}\leq 2\frac{||R(w,x)||}{||w-x||}\quad(*3*)\to 0\quad (*5*)$$ ma questo e $(*6*)$ mostrano che $g$ è differenziabile in $y$ con $(Jg)(y)=((Jf)(x))^{-1}$.\\

Gli elementi di $Jf$ sono continui e il suo determinante è diverso da $0$ quindi gli elementi della matrice $(Jf)^{-1}$ sono continui. Visto che $x=g(y)$ e $g$ è continua ne segue che $(Jg)(y)=(Jf)^{-1}(g(y))$ è continuo. $\Box$.\\

\textbf{Osservazione 2.18:} L'invertibilità locale in tutti i punti non implica l'invertibilità globale.\\

Si consideri $f(x,y)=\binom{x^2-y^2}{2xy}$ con $f:\mathbb{R}^2\textbackslash\{0\}\to\mathbb{R}^2\textbackslash\{0\}$. $f$ non è globalmente invertibile perchè $f(-x,-y)=f(x,y)$ eppure $Jf=\begin{vmatrix} 2x & -2y \\ 2y & 2x \end{vmatrix}$ ha determinante sempre diverso da $0$.\\

\textbf{Corollario 2.19:} Sia $f:\Omega\subseteq\mathbb{R}^n\to\mathbb{R}^n$, $\Omega$ aperto e $f\in\mathcal{C}^1(\Omega)$. Supponiamo $Jf(x)$ invertibile $\forall x\in\Omega$ allora $f$ è una mappa aperta ovvero $A$ aperto in $\Omega$ $\Rightarrow f(A)$ aperto.\\

\textbf{Dimostrazione:} Per il teorema $f$ è localmente invertibile in ogni punto e questo dà la tesi.\\

\section{Teorema di Dini (Multidimensionale)}

Ricordo che se $M:=\begin{vmatrix} I & 0 \\ A & B \end{vmatrix}$ è una matrice quadrata $\det M=\det I\cdot \det B$ ed $M$ è invertibile se e solo se B è invertibile e la sua inersa è $M^{-1}=\begin{vmatrix} I & 0 \\ -B^{-1}A & B^{-1} \end{vmatrix}$.\\

\textbf{Teorema di Dini (multidimensionale) 2.20:} Sia $f:\Omega\subseteq\mathbb{R}^m\times\mathbb{R}^n\to\mathbb{R}^n$, con $\Omega$ aperto e $f\in\mathcal{C}^1(\Omega)$:
\begin{enumerate}
\item Sia $(x_0,y_0)\in\Omega$ tale che $f(x_0,y_0)=0$ ($x_0\in\mathbb{R}^m$ e $y_0\in\mathbb{R}^n$)
\item $(J_yf)(x_0,y_0)$ è invertibile (la matrice quadrata di ordine $n\times n$ rispetto alle $y$)
\end{enumerate}

Allora esistono due intorni aperti $\mathcal{U}(x_0)$ e $\mathcal{V}(y_0)$ con $\overline{\mathcal{U}(x_0)\times\mathbb{V}(y_0)}\subseteq\Omega$ tali che l'insieme degli zeri di $f$ in $\mathcal{U}(x_0)\times\mathbb{V}(y_0)$ coincide con il grafico di una funzione $\phi:\mathcal{U}(x_0)\to\mathcal{V}(y_0)$, $\phi\in\mathcal{C}^1(\mathcal{U}(x_0))$ e $J\phi(x)=-(J_yf)^{-1}\cdot(J_xf)|_{x,\phi(x)}\quad\forall c\in\mathcal{U}(x_0)$.\\

\textbf{Dimostrazione:} Sia $F:\Omega\subseteq\mathbb{R}^m\times\mathbb{R}^n\to\mathbb{R}^m\times\mathbb{R}^n$ definita come $F(x,y):=\binom{x}{f(x,y)}$ ed $F\in\mathcal{C}^1(\Omega)$ con $(JF)(x,y)=\begin{vmatrix} I & 0 \\ J_xf & J_yf \end{vmatrix}$. In particolare $(Jf)(x_0,y_0)=\begin{vmatrix} I & 0 \\ J_xf(x_0,y_0) & J_yf(x_0,y_0 \end{vmatrix}$ è invertibile per ipotesi ed $F(x_0,y_0)=\binom{x_0}{0}$ quindi per il teorema di invertibilità locale $F$ è localmente invertibile in $(x_0,y_0)$ ovvero esistono due intorni aperti (in realtà sono delle bolle aperte) $\mathcal{W}(x_0,y_0)$ e $\mathcal{S}(x_0,0)$ tra i quali $F$ agisce come un diffeomorfismo. Siano\begin{itemize}
\item $\mathcal{U}:=$ proiezione sulle $x$ di $\mathcal{S}$ (aperto perchè la proiezione è una mappa aperta)
\item$\mathcal{V}:=$ proiezione sulle $y$ di $\mathcal{W}$ (aperto perchè la proiezione è una mappa aperta)\\
\end{itemize}

Siano poi $i:\mathbb{R}^m\to\mathbb{R}^m\times\mathbb{R}^n$, $i(x):=\binom{x}{0}$ (immersione) e $p_{r_y}:\mathbb{R}^m\times\mathbb{R}^n\to\mathbb{R}^m$, $p_{r_y}(x,y)=y$ (proiezione). Costruiamo $\phi:\mathcal{U}\to\mathcal{V}$ $\phi(x):=(p_{r_y}\circ F^{-1}\circ i)(x)$ di classe $\mathcal{C}^1$ (perchè $p_{r_y}$ e $y$ lo sono in quanto lineari e $F^{-1}$ lo è per Dini) e:$$J\phi=(Jp_{r_y})\cdot(JF^{-1})\cdot(Ji)=(0,I)\cdot(JF)^{-1}\cdot\binom{I}{0}=
(0,I)\cdot\begin{vmatrix} I & 0 \\ -(J_yf)^{-1}J_xf & (J_yf)^{-1} \end{vmatrix}=-(J_yf)^{-1}J_xf$$\\

Infine $f(x,\phi(x))=p_{r_y}(F(x,\phi(x))=(p_{r_y}\circ F)(p_{r_x}(F^{-1}(x,0)),(p_{r_y}(F^{-1}(x,0)))=(p_{r_y}\circ F)(F^{-1}(x,0))=(p_{r_y}\circ F\circ F^{-1})(x,0)=p_{r_y}(x,0)=0$
 ovvero il grafico di $\phi$ è contenuto negli zeri di $f$. Dal fatto che $F$ è biunivoca segue che vale anche il viceversa. $\Box$\\
 
\textbf{Osservazione 2.21:} Nella dimostrazione del teorema si decompone lo spazio di partenza in un $\mathbb{R}^m\times\mathbb{R}^n$, lo jacobiano di $f$ è una matrice rettangolare di ordine $n\times(n+m)$ e non è detto che la sua coda $n\times n$ sia invertibile. Chiamo $y$ tutto ciò che dà la sottomatrice $n\times n$ invertibile. Si può quindi riformulare le ipotesi del teorema chiedendo che $rk(Jf(x_0,y_0))=n$ che è equivalente alla richiesta di invertibilità.\\

\section{Estremi vincolati}

Data $f:\Omega\subseteq\mathbb{R}^n\to\mathbb{R}$ e $\Sigma\subseteq\Omega$ una regione. Sia $p\in\Sigma$ e supponiamo che $f(p)=\max\{f(x)\quad x\in\Sigma\cap\mathcal{U}(x)\}$ in tal caso diciamo che $p$ è un punto di massimo locale per $f$ vincolata a $\Sigma$ (analoga definizione per il minimo).\\

Se $\Sigma\cap\mathcal{U}(x)\}$ fosse un insieme aperto si sa come procedere, in particolare se $f\in\mathcal{C}^1$ cerchiamo gli estremanti tra gli zeri di $\nabla f$. La situazione è però nuova se 
$\Sigma\cap\mathcal{U}(x)$ non è un aperto di $\Omega$, in tal caso in genere $\nabla f(p)\ne 0$.\\

Se $\Sigma$ è il grafico di una funzione è chiaro come procedere:\\

\textbf{Teorema dei Moltiplicatori di Lagrange 2.22:} Sia $f\Omega\subseteq\mathbb{R}^m\to\mathbb{R}$, con $\Omega$ aperto e $f\in\mathcal{C}^1(\Omega)$, sia $\Sigma$ il luogo degli zeri di $g:\Omega\to\mathbb{R}^n$ ($m>n)$) e $g\in\mathcal{C}^1(\Omega)$, sia $p\in\Sigma$ tale che $f$ ha in $p$ un estremo locale vincolato a $\Sigma$ e che il rango di $(Jg(p))$ sia massimo ($=n$). Allora esistono $\lambda_1\ldots\lambda_n$ tali che: $$\begin{cases} \nabla f(p)=\lambda_1\nabla g_1(p)+\ldots+\lambda_n\nabla g_n(p) & \leftarrow\mbox{m equazioni scalari}\\ g_1(p)=0\\ \vdots  &  \leftarrow\mbox{n equazioni scalari} \\ g_n(p)=0
\end{cases}$$ Dove $g=(g_1,\ldots,g_n)$, il sistema ha $m+n$ equazioni in $m+n$ incognite $x$ e $\lambda$.\\

\textbf{Dimostrazione:} Per ipotesi $rk(Jg(p))=n$ quindi esiste in $Jg(p)$ una sottomatrice di ordine $n\times n$ invertibile. Senza perdita di generalità possiamo supporre che $\mathbb{R}^m=\mathbb{R}_x^{m-n}\times\mathbb{R}_y^n$ e che $J_yg(p)$ abbia rango massimo e quindi sia invertibile. Da Dini segue che in un intorno di $p$ $\Sigma$ è il grafico di una funzione quindi esistono due intorni aperti $\mathcal{U}(x_p)$ e $\mathcal{V}(y_p)$ e una mappa $\phi:\mathcal{U}(x_p)\to\mathcal{V}(y_p)$ tali che il grafico di $\phi$ coincide con $(\mathcal{U}(x_p)\times\mathcal{V}(y_p))\cap\Sigma$. \\

Ma allora $f|_\Sigma$ coincide con $x\mapsto (x,\phi(x))$. Prendo $h(x):=f(x,\phi(x))$ questa è definita su un aperto $\mathcal{U}(x_p)$, è di classe $\mathcal{C}^1$ ed ha un estremo in $x_p$ quindi: $$0=(\nabla h)(x_p)^T=(Jh)(x_p)=Jf\cdot\binom{I}{J\phi}=(Jf_x|Jf_y)\cdot\binom{I}{-(J_yg)^{-1}(J_xg}=J_xf-J_yf(J_yg)^{-1}J_xg$$ ovvero $$J_xf=J_yf(J_yg)^{-1}J_xg\quad(*)$$ Pongo $J_yf(J_yg)^{-1}$ matrice $1\times n$ $=(\lambda_1,\ldots,\lambda_n)$. Allora $(*)$ diventa: $$\nabla_xf=\lambda_1\nabla_xg_1+\ldots+\lambda_n\nabla_xg_n$$ Moltiplicando a destra $J_yf(J_yg)^{-1}$ per $J_yg$ si ottiene: $$\nabla_yf=\lambda_1\nabla_yg_1+\ldots+\lambda_n\nabla_yg_n$$ Queste du messe insieme danno il sistema della tesi. $\Box$\\

\textbf{Osservazione 2.23:} Se si introduce $\mathcal{L}:\Omega\times\mathbb{R}^n\to\mathbb{R}$, $\mathcal{L}(x,\lambda):=f(x)-\underline{\lambda}g(x)$ allora il sistema può essere riscritto come: $$\begin{cases} \nabla_x\mathcal{L}=0\\ \nabla_y\mathcal{L}=0 \end{cases}$$ ovvero $\nabla_{x,y}\mathcal{L}=0$. Quindi il problema originale (cercare estremi vincolati per $f$) è diventato cercare estremi liberi per $\mathcal{L}$. $\mathcal{L}$ è detta Lagrangiana.\\

\textbf{Osservazione 2.24:} Il teorema fornisce solo un criterio sufficiente, non è detto che i punti trovati risolvendo il sistema siano effettivamente estremanti. Per stabilirlo si potrebbe usare un'analisi al secondo ordine locale sulla restrizione o considerazioni sulla compattezza.\\

\textbf{Osservazione 2.25:} C'è un altro modo di interpretare il teorema dei moltiplicatori basato su nozioni di geometria differenziale:\\

Passo 1) In $\mathbb{R}^m$ sia $p\in\mathbb{R}^m$ e sia $E_p=\{\mbox{frecce uscenti da p}\}$ allora $E_p$ con la somma vettoriale e il prodotto per uno scalare è uno spazio vettoriale di dimensione $m$.\\

Passo 2) Sia $g:\Omega\subseteq\mathbb{R}^m\to\mathbb{R}^n$, $\Omega$ aperto, $m>n$, $g\in\mathcal{C}^1(\Omega)$ e sia $p\in\Omega$ con $g(p)=0$ e $Jg(p)$ di rango $n$ (massimo). Sia $T_p\Sigma:=\{v\in E_p \mbox{che sono derivate in } p \mbox{ di qualche cammino su } \Sigma \mbox{ ovvero tali che } \exists \psi:(-1,1)\to\Sigma\subseteq\mathbb{R}^m  \mbox{ di classe } \mathcal{C}^1 \mbox{ e } \psi(0)=p, \psi '(0)=v\}$, $T_p\Sigma$ è lo spazio tangente a $\Sigma$ in $p$ e per costruzione $T_p\Sigma$ è un sottospazio di $E_p$.\\

Passo 3) Dalle ipotesi fatte su $g$ e dal teorema di Dini segue che $\dim T_p\Sigma\leq m-n$. Infatti per Dini localmente $\Sigma$ è grafico di una funzione che a meno di riordinare le variabili possiamo immaginare dia le $x_{m-n+1},\ldots,x_n$ coodinate in funzione delle $x_1,\ldots,x_{m-n}$. Ma allora la curva $$\psi_1:t\to(x_{1,p}+t,x_{2,p},\ldots,x_{m-n,p},\phi(x_{1,p}+t,x_{2,p},\ldots,x_{m-n,p}))$$ è ben definita in $\mathcal{U}(0)$ dove $p=(x_{1,p},\ldots,x_{m-n,p})$ e $psi_1'(0)=1,0,\ldots,0,\frac{d\phi}{dx_1}())$ che quindi $\in T_p\Sigma$. Lo stesso risultato si può ottenere per $\psi_2,\ldots,\psi_{m-n}$. Questi sono $m-n$ vettori linearmente indipendenti.\\

Passo 4) Sia ora $\psi:(_1,1)$ una curva su $\Sigma$ di classe $\mathcal{C}^1$ e $\psi(0)=p$. Visto che è su $\Sigma$ allora $(g_1\circ\psi)(t)=0\quad\forall t$. In particolare $$0=\frac{d((g_1\circ\psi)(t))}{dt}|_{t=0}=\nabla g_1(p)\cdot\psi'(0)$$ ovvero $\nabla g_1(p)$ è ortogonale a $\psi ' (0)$. Visto che ogni vettore di $T_p\Sigma$ è di questo tipo ne segue che $\nabla g_1(p)\in (T_p\Sigma)^\perp$ ma questo può essere ripetuto per $g_2,\ldots,g_n$. La condizione "$Jg(p)$ ha rango $n$" garantisce che questi sono linearmente indipendenti quindi $\dim(T_p\Sigma)^\perp\geq n$.\\

Passo 5) Visto che $\dim(T_p\Sigma)+\dim(T_p\Sigma)^\perp=dim E_p=n$, da $\dim T_p\Sigma\geq m-n$ e $dim(T_p\Sigma)^\perp\geq n$, segue che $\dim T_p\Sigma= m-n$ e $dim(T_p\Sigma)^\perp = n$. Inoltre $(T_p\Sigma)^\perp$ è generato dai $\nabla g_j(p)\quad j=1,\ldots,n$

\chapter{Equazioni Differenziali}

Possiamo definire un'equazione differenziale come un'equazione le cui soluzioni sono funzioni $\varphi$ di cui si chiede un legame tra $x,\varphi(x),\varphi'(x),\ldots,\varphi^{(k)}(x)$ che deve essere soddisfatto $\forall x$ nel dominio. In modo meno vago è data $f:\Omega\subseteq\mathbb{R}\times\mathbb{R}^{k+1}\to\mathbb{R}$ e si cerca $\varphi:(\alpha,\beta)\to\mathbb{R}$ di classe $\mathcal{C}^k$ tale che: $$f(x,\varphi(x),\varphi'(x),\ldots,\varphi^{(k)})=0\quad\forall x \in (\alpha,\beta)$$ Una tale equazione è detta di ordine $k$.\\

\textbf{Osservazione 3.1:} Se $k=0$ l'equazione è di tipo "implicito" che è già stato tratta nel capitolo 2. Considereremo quindi sempre $k\geq 1$.\\

La teoria delle equazioni differenziali è abbastanza sviluppata nel caso l'equazione sia scrivibile in forma speciale: $$\varphi^{(k)}=f(x,\varphi(x),\varphi'(x),\ldots,\varphi^{(k-1)})$$ In cui la derivata di ordine massimo è esplicitata. Queste equazioni sono dette in forma normale.\\

Le equazioni in forma normale possono essere generalizzate a equazioni vettoriali, ovvero equazioni dove l'incognita cercata $\varphi:(\alpha ,\beta)\to\mathbb{R}^n$ di classe $\mathcal{C}^k((\alpha,\beta))$ è in realtà vettoriale: $$\underline{\varphi}^{(k)}(x)=\underline{f}(x,\underline{\varphi}(x),\underline{\varphi}'(x),\ldots,\underline{\varphi}^{(k-1)}(x)) \quad (*)$$
$\forall x\in(\alpha,\beta)$ dove $f:\Omega\subseteq\mathbb{R}\times (\mathbb{R}^n)^k\to\mathbb{R}^n$.\\

\textbf{Osservazione 3.2:} In genere un'equazione differenziale ha più di una soluzione. Si considerino:\begin{itemize}
\item $y'=f(x)$ ($f$ continua) ha $\varphi(x)=y_0+\int_{x_0}^xf(u)du$ come soluzione $\forall(x_0,y_0)$
\item $y'=y$ ha $\varphi(x)=y_0 e^x$ con $y_0\in\mathbb{R}$\\
\end{itemize}

La struttura dell'equazione normale $(*)$ suggerisce però che la soluzione sia unica qualora si aggiunga una condizione iniziale della forma: $ (\underline{\varphi}(x_0),\underline{\varphi}'(x_0),\ldots,\underline{\varphi}^{(k-1)}(x_0))=(\underline{\widetilde{y_0}},\underline{\widetilde{y_1}},\underline{\widetilde{y_2}},\ldots,\underline{\widetilde{y_{k-1}}})$ con $x_0$ e $\underline{\widetilde{y}}$ assegnati. Questo perchè la conoscenza delle condizioni iniziali e di $(*)$ danno: ${\varphi}^{(k)}(x_0)={f}(x_0,{\varphi}(x_0),{\varphi}'(x_0),\ldots,{\varphi}^{(k-1)}(x_0))$ che è noto e per derivazione (sempre se è possibile) di $(*)$ si ha: $$\varphi^{(k+1)}(x)=\frac{d}{dx}\varphi^{(k)}(x)=\frac{d}{dx}f(x,\varphi (x),\varphi'(x),\ldots,\varphi^{(k-1)}(x))$$ e chiamando $x,y_0,y_1,\ldots,y_{k-1}$ gli argomenti di $f$ questo è: $$=\left .\frac{d}{dx}\right|_{(x,\varphi(x),\ldots,\varphi^{(k-1)})}+\left .\frac{d}{dy_0}\right|_{(x,\varphi(x),\ldots,\varphi^{(k-1)})}\cdot\varphi'(x)+\ldots+\left .\frac{d}{dy_{k-1}}\right|_{(x,\varphi(x),\ldots,\varphi^{(k-1)})}\cdot\varphi^{(k)}(x)$$ e valutando in $x_0$ tutti i termini a destra risultano noti (si osservi che in $x_0$ $\varphi^{(k)}$ è stato calcolato al passo precedente), quindi $\varphi^{(k+1)}(x_0)$ è noto.\\

Il processo può formalmente proseguire ad ogni ordine (se $f$ è $\mathcal{C}^\infty$) e dà $\varphi^{(j)}(x_0)$ $\forall j$, quindi di $\varphi$ è determinato lo sviluppo di Taylor. Se questo converge e se converge a $\varphi$ ne segue che $\varphi$ è stata trovata univocamente. Tutto questo è formale ($f$ potrebbe non essere $\mathcal{C}^\infty$, la serie di Taylor potrebbe non convergere...) ma mostra che è ragionevole supporre che la coppia: $$\begin{cases}\mbox{  } y^{(k)}=f(x,y,y',\ldots,y^{(k-1)}) & \leftarrow \mbox{equazione normale}  \\ \left. \begin{array}{ll}
y(x_0)=y_0 \\ y'(x_0)=y_1\\ \vdots \\ y^{(k-1)}(x_0)=y_{k-1} 
\end{array}\right\} & \leftarrow\mbox{condizioni} \end{cases} \qquad (*)$$
abbia una sola soluzione. Buona parte della teoria che andremo a sviluppare servirà proprio ad individuare le condizioni sotto cui questo principio è valido.\\

\textbf{Definizione 3.3:}  Sia $f:\Omega\subseteq\mathbb{R}\times (\mathbb{R}^n)^k\to\mathbb{R}^n$, $\Omega$ aperto e $f\in\mathcal{C}(\Omega)$; sia inoltre $(x_0,y_0,y_1,\ldots,y_{k-1})\in\Omega$ allora chiamo $(*)$ problema di Cauchy. Chiamo soluzione di $(*)$ una funzione $\varphi:(\alpha,\beta)\to\mathbb{R}^n$ tale che:\begin{enumerate}
\item $\varphi\in\mathcal{C}^k((\alpha,\beta))$
\item $\alpha<x_0<\beta$
\item $\varphi$ (e le sue derivate) hanno valori tali per cui $(*)$ vale $\forall x\in(\alpha,\beta)$
\end{enumerate}
Si usa chiamare "soluzione globale" una soluzione per la quale $(\alpha,\beta)$ sia noto ed esplicitato, e invece "soluzione locale" una soluzione per la quale $(\alpha,\beta)$ sia noto esistere ma non venga determinato esplicitamente.\\

\textbf{Osservazione 3.4:} Il dominio di una soluzione è $(\alpha,\beta)$ in particolare è un intervallo aperto e $\varphi$ è di classe $\mathcal{C}^k$ se l'equazione ha ordine $k$.

\section{Equazioni di forma speciale}

Per certe funzioni $f$ esistono procedure capaci di dare una formula esplicita della soluzione del problema di Cauchy. Vediamone alcune:

\subsection{Equazioni lineari del primo ordine}


$$\begin{cases}
y'+p(x)y=q(x)\\
y(x_0)=y_0
\end{cases}$$

dove $p$ e $q$ $\in\mathcal{C}((\alpha,\beta))$, $x_0\in(\alpha,\beta)$ e $y_0\in\mathbb{R}$ qualunque.\\

Sia $H(x)$ una (qualunque) primitiva di $p(x)$ ad esempio: $H(x)=\int_{x_0}^xp(u)du$ (esiste perchè $p(x)$ è continua e $H'=p$ in $(\alpha,\beta)$). Moltiplicando per $e^{H(x)}$, ottengo: $$q(x)e^{H(x)}=y'(x)e^{H(x)}+p(x)e^{H(x)}y=\frac{d}{dx}(y(x)e^{H(x)}$$ ma allora $y(x)e^{H(x)}$ è una primitiva per $q(x)e^{H(x)}$ per cui: $$y(x)e^{H(x)}-y(x_0)e^{H(x_0)}=\int_{x_0}^xq(v)e^{H(v)}dv$$ Siccome $y(x_0)=y_0$ per la condizione iniziale e $e^{H(x_0)}=1$ ho che: $$\boxed{y(x)=e^{-H(x)}\left[y_0+\int_{x_0}^xq(v)e^{H(v)}dv\right]}$$ ed è soluzione su $(\alpha,\beta)$ e la procedura dimostra che è unica su tale intervallo.\\

\textbf{Esempio 3.5:} Si consideri il problema di Cauchy:


$$\begin{cases}
y'=2xy+e^{x^2-x}\\
y(0)=3
\end{cases}$$

Abbiamo che $H(x)=\int_{0}^x-2udu=-x^2$ mentre $\int_0^xe^{u^2-u}e^{-u^2}du=\int_0^xe^{-u}du=1-e^x$ perciò la soluzione (unica su $\mathbb{R}$) è: $$y(x)=e^{x^2}(4-e^{-x})$$

\textbf{Esempio 3.6:} Si consideri il problema di Cauchy:

$$\begin{cases}
y'=-\frac{y}{x}+\cos(x)\\
y(1)=2
\end{cases}$$

La procedura ci dà che l'unica soluzione su, $(0,+\infty)$, è $y(x)=\frac{1}{x}(2+x\sin(x)+\cos(x)-\sin 1-\cos1)$.\\

\subsection{Equazioni a variabile separabile}

$$\begin{cases}
y'=h(x)g(y)\\
y(x_0)=y_0
\end{cases}$$

Con $h\in\mathcal{C}((\alpha,\beta))$, $g\in\mathcal{C}((\gamma,\delta))$, $x_0\in(\alpha,\beta)$ e $y_0\in(\gamma,\delta)$. Allora $\exists !$ soluzione locale (di fatto esiste sul più ampio intervallo su cui la procedura seguente è corretta in ogni passo:\\

\textbf{Passo A:} Se $g(y_0)=0$, allora $y(x)=y_0$ (funzione costante) $\forall x\in(\alpha,\beta)$ è soluzione.\\

\textbf{Passo B:} Supponiamo $g(y_0)\ne 0$. Allora $\exists\mathcal{U}(y_0)$ in cui $g$ è diverso da $0$, in tal caso: $$\frac{y'(x)}{g(y(x))}=h(x)$$ e per integrazione: $$\boxed{\int_{y_0}^{y(x)}\frac{du}{g(u)}=\int_{x_0}^x h(v)dv} \qquad (**)$$\\

La $(**)$ è un'equazione implicita per $y(x)$, $F(x,y)=0$ con $$F(x,y)=\int_{y_0}^{y(x)}\frac{du}{g(u)} - \int_{x_0}^x h(v)dv$$

Visto che $\frac{dF}{dy}=\frac{1}{g(y)}\ne 0$ la $(**)$ definisce implicitamente $1!$ funzione e il ragionamento che ci ha portato alla $(**)$ mostra che la soluzione del problema di Cauchy e la soluzione di $F(x,y)=0$ coincidono fino a dove è lecito dividere per $g(y)$.\\

\textbf{Esempio 3.7:} Si consideri il problema di Cauchy:

$$\begin{cases}
y'=-8x^3\sqrt{y}\\
y(0)=64
\end{cases}$$

Siccome $64\ne 0$ la funzione $y(x)=64$ non è soluzione di questo problema di Cauchy. Allora $\frac{y'}{2\sqrt{y}}=-4x^3 \Leftrightarrow \int_{64}^y\frac{du}{2\sqrt{u}}=\int_0^x-4v^3dv$
perciò $\sqrt{y}-\sqrt{64}=-x^4 \Leftrightarrow y=(\sqrt{64}-x^4)^2$ ed è soluzione globale, di fatto lo è su tutto l'intervallo $(-64^{\frac{1}{8}},64^{\frac{1}{8}}$.\\

\subsection{Equazioni di Bernoulli}

$$\begin{cases}
y'+p(x)y=q(x)y^\gamma\\
y(x_0)=y_0
\end{cases}$$

dove $p$ e $q$ $\in\mathcal{C}((\alpha,\beta))$, $\gamma\in\mathbb{R}$, $x_0\in(\alpha,\beta)$ e $y_0$ tale che $y_0^\gamma$ è ben definita.\\

Se $\gamma=0$o $\gamma=1$ allora l'equazione è lineare e come trattarla è stato visto in precedenza, supponiamo $\gamma\ne 0,1$.\\

Se $\gamma >0$ tra le soluzioni dell'equazione c'è $y(x)\equiv0$ $\forall x$ (che soddisfa il problema di Cauchy nel caso $y_0=0$). Assumo $y_0\ne 0$ allora posso dividere per $y^\gamma$ ed ho: $$\frac{y'}{y^\gamma}+p(x)\frac{1}{y^{\gamma-1}}=q(x)$$

Pongo $z(x)=y^{1-\gamma}$ allora $z'(x)=(1-\gamma)\frac{y'}{y^\gamma}$ quindi $z$ soddisfa: $$\begin{cases} \frac{z'}{1-\gamma}+p(x)z=q(x)\\z(x_0)=y_0^{1-\gamma} \end{cases} \mbox{ ovvero } \begin{cases} z'+(1-\gamma)p(x)z=(1-\gamma)q(x)\\z(x_0)=y_0^{1-\gamma} \end{cases} $$

Quindi l'equazione per $z$ è lineare. Risolto il problem di Cauchy per $z$ si ottiene la $y$ invertendo la soluzione $z(x)=y(x)^{1-\gamma}$. (Attenzione a dove questa soluzione sia invertibile!)\\

\textbf{Osservazione 3.8:} La procedura è invertibile ad ogni punto se $y_0\ne0$ quindi quella così trovata è l'unica soluzione locale.\\

\section{Semplificazioni di equazioni differenziali generiche}

Proseguiamo ora nello studio delle equazioni differenziali generiche:

\subsection{Da equazione di ordine k a equazione del primo ordine}

Verifichiamo che da un certo punto di vista basta studiare le equazioni del primo ordine, purchè vettoriali, perché ogni equazione diordine $k$ può sempre essere scritta come equazione del primo ordine vettoriale. Infatti sia data: $$\begin{cases} \underline{y}^{(k)}=\underline{f}(x,\underline{y},\underline{y}',\ldots,\underline{y}^{(k-1)})\\ \underline{y}^{(j)}(x_0)=\widetilde{\underline{y}_j}\qquad j=0,\ldots,k-1 \end{cases} (*)$$ con $\underline{f}:\Omega\subseteq\mathbb{R}\times(\mathbb{R}^n)^k\to\mathbb{R}^k$, $\Omega$ aperto, $f\in\mathcal{C}(\Omega)$ e $(x_0,\widetilde{y_0},\ldots,\widetilde{y_{k-1}})\in\Omega$. Pongo: $$\underline{z}(x)=\begin{bmatrix} \underline{y}(x)\\ \underline{y}'(x) \\ \vdots \\ \underline{y}^{(k-1)}(x) \end{bmatrix} = = \begin{bmatrix}
\underline{z_1} \\ \underline{z_2} \\ \vdots \\ \underline{z_k} \end{bmatrix}$$

Provo a derivare le componenti del vettore (è possibile in quanto di classe $\mathcal{C}^k$) e ottengo: $$\underline{z}'(x)=\begin{bmatrix} \underline{y}'(x)\\
 \underline{y}''(x) \\ 
 \vdots \\ 
 \underline{y}^{(k)}(x) 
 \end{bmatrix} =
\begin{bmatrix} 
\underline{y}'(x)\\ 
\underline{y}''(x) \\ 
\vdots \\ 
\underline{f}(x,\underline{y},\underline{y}',\ldots,\underline{y}^{(k-1)}) \end{bmatrix}=  \begin{bmatrix}
\underline{z_2} \\ 
\vdots \\ 
 \underline{z_k} \\ 
 \underline{f}(x,\underline{y},\underline{y}',\ldots,\underline{y}^{(k-1)}) 
 \end{bmatrix} :=\underline{F}(x,\underline{z})$$
 ed è chiaro che $\underline{F}\in\mathcal{C}(\Omega)$. Inoltre: $$\underline{z}(x_0)=\begin{bmatrix}
 \widetilde{y_0}\\
 \vdots \\
 \widetilde{y_{k-1}}
 \end{bmatrix}$$
 
 Quindi se $\underline{y}$ soddisfa $(*)$ allora $\underline{z}$ soddisfa: $$\begin{cases}
 \underline{z}'=\underline{F}(x,\underline{z})\\
 \underline{z}(x_0)=\begin{bmatrix}
 \widetilde{y_0}\\
 \vdots \\
 \widetilde{y_{k-1}}
 \end{bmatrix}
 \end{cases} (**)$$
 
 che è del primo ordine vettoriale con $\underline{F}\in\mathcal{C}(\Omega)$ \\
 
 Viceversa se $\underline{z}$ soddisfa $(**)$ (quindi $\underline{z}\in\mathcal{C}^1(\mathcal{U}(x_0))$) allora la funzione $y(x):=z_1(x)$ è chiaramente $\mathcal{C}^1$ ma $(**)$ dice $y'(x)=z_2$ che è $\mathcal{C}^1$ allora $y(x)$ è in realtà $\mathcal{C}^2$. Iterando il procedimento si afferma che $y(x)$ è di classe $\mathcal{C}^k$ e $y^{(k)}=z'_k=f(x,z_1,z_2,\ldots,z_k)$ ovvero $\underline{y}$ soddisfa $(x)$ comprese le condizioni iniziali quindi $(*)$ equivale a $(**)$\\
 
 D'ora in poi ci limiteremo a problemi di Cauchy del primo ordine vettoriale.
 
 \subsection{Passaggio a integrale}

Sia dato il problema di Cauchy: 

$$\begin{cases}
\underline{y}'=f(x,\underline{y})\\
\underline{y}(x_0)=\underline{y}_0
\end{cases} (*3*) $$

con $f:\Omega\subseteq\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n$, $\Omega$ aperto, $f\in\mathcal{C}(\Omega)$ e $(x_0,\underline{y}_0)\in\Omega$.\\

Supponiamo che $y$ sia una soluzione del problema di Cauchy (quindi è di classe $\mathcal{C}^1$ su un intervallo $(\alpha,\beta)$). Integrando l'equazione tra $x_0$ e $x$ si ha allora:

$$y(x)=y_0+\int_{x_0}^x f(u,y(u))du\mbox{ in } (\alpha,\beta)$$ 

Introduco allora un "operatore", data una funzione "buona" $\varphi$ definiamo:

$$T:\varphi\to (T\varphi) \qquad (T\varphi)(x):=y_0+\int_{x_0}^x f(u,\varphi(u))du$$

Perciò se  $y$ è soluzione di $(*3*)$ su $(\alpha,\beta)$ ($y\in\mathcal{C}^1$) allora $Ty$ è ben definito e $Ty=y$ ovvero $y$ è punto fisso per $T$.\\

Viceversa, supponiamo che $\varphi$ sia $\mathcal{C}((\alpha,\beta))$ e sia punto fisso per $T$. Allora: 

$$\varphi(x)=y_0+\int_{x_0}^x f(u,\varphi(u))du\quad\forall x\in(\alpha,\beta)\quad (*4*)$$

ma $u\to f(u,\varphi(u))$ è continua (composizione di funzioni continue), quindi $x\to\int_{x_0}^x f(u,\varphi(u)) du$ è $\mathcal{C}^1$ (è integrale di una continua), ma allora $\varphi\in\mathcal{C}^1((\alpha,\beta))$ per la $(*4*)$. Valutando in $x_0$ la $(*4*)$ si ha $\varphi(x_0)=y_0$ e derivando la $(*4*)$ si ha $\varphi'(x)=f(x,\varphi(x))$. Quindi $\varphi$ soddisfa la $(*3*)$ ovvero se $y$ è $\mathcal{C}((\alpha,\beta))$ e $Ty=y$ in $(\alpha,\beta)$ allora $y$ è soluzione di $(*3*)$ su $(\alpha,\beta)$ e $y\in\mathcal{C}^1((\alpha,\beta))$\\

Quindi il problema di Cauchy equivale ad un problema di punto fisso (detto problema integrale di Volterra) che però per essere formulato ha bisogno solo di assumere che $y\in\mathcal{C}((\alpha,\beta))$ (e non $\mathcal{C}^1$). Questo semplifica notevolmente la teoria.

\section{Teoremi di esistenza e unicità}


Basandosi se questa equivalenza si può dimostrare il seguente teorema:\\

\textbf{Teorema di Peano 3.9:} Sia $f:\Omega\subseteq\mathbb{R}\times\mathbb{R}\to\mathbb{R}$, $\Omega$ aperto e $f\in\mathcal{C}(\Omega)$. Sia $(x_0,y_0)\in\Omega$ allora il problema di Cauchy: 

$$\begin{cases}
y'=f(x,y)\\
y(x_0)=y_0
\end{cases}$$

ha una soluzione locale (non dimostriamo questo risultato).\\

Osserviamo che il teorema afferma l'esistenza della soluzione ma non la sua unicità. In effetti sotto ipotesi così deboli (la sola continuità) non si ha unicità. Ad esempio:\\

\textbf{Esempio 3.10:} Sia dato il problema di Cauchy:

$$\begin{cases}
y'=3y^{\frac{2}{3}}\\
y(0)=0
\end{cases}$$

ha sia $y(x)\equiv 0$ che $y(x)=x^3$ come soluzioni.\\

La sola continuità di $f$ garantisce l'esistenza locale della soluzione del problema di Cauchy ma non la sua unicità. Introduciamo ora una proprietà aggiuntiva che si rivelerà adatta per l'unicità:\\

\textbf{Definizione 3.11:} Data $f:\Omega\subseteq\mathbb{R}\times\mathbb{R}^n \to\mathbb{R}^n$, $\Omega$ aperto e dato $(x_0,y_0)\in\Omega$; $f$ è detta localmente lipschitziana nelle $y$ uniformemente nelle $x$ nel punto $(x_0,y_0)$ quando $\exists d,\delta,L$ tali che: 

$$\left . \begin{array}{ll}
x\in[x_0-d,x_0+d]\\
y_1,y_2\in\overline{B_\delta(y_0)}
\end{array} \right\} \Rightarrow ||f(x,y_1)-f(x,y_2)||_2\leq L||y_1-y_2||_2$$\\

\textbf{Osservazione 3.12:} È una condizione di regolarità ma solo nelle $y$, non in tutte le variabili di $f$. Ad esempio $f(x,y):=[x]+y$ ($[x]$ parte intera di $x$) ha quella proprietà ma non è continua.\\

\textbf{Osservazione 3.13:} Se $f\in\mathcal{C}^1(\Omega)$ allora sicuramente ha quella proprietà in tutti i punti di $\Omega$ (perché le derivate parziali di $f$ sono continue quindi limitate sui compatti).\\

\textbf{Teorema esistenza e unicità locale 3.14:} Data $f:\Omega\subseteq\mathbb{R}\times\mathbb{R}^n \to\mathbb{R}^n$, $\Omega$ aperto, $f\in\mathcal{C}(\Omega)$ e sia $(x_0,y_0)\in\Omega$. Supponiamo che $f$ sia localmente lipschitziana nelle $y$ uniformemente nelle $x$ in $(x_0,y_0)$ allora il problema di Cauchy: 

$$\begin{cases}
y'=f(x,y)\\
y(x_0)=y_0
\end{cases}$$

ha una e una sola soluzione locale.\\

\textbf{Dimostrazione:} Siano $d,\delta,L$ come nella defizione 3.11. Sia $M:=\max||f(x,y)||_2\quad(x,y)\in[x_0-d,x_0+d]\times\overline{B_\delta(y_0)}$, osservo che $[x_0-d,x_0+d]\times\overline{B_\delta(y_0)}$ è compatto e $f$ è continua quindi esiste $M$. Sia $d'=\min(d,\frac{\delta}{M},\frac{1}{2L})$ e sia $\mathcal{X}:=\mathcal{C}([x_0-d',x_0+d'],\overline{B_\delta(y_0)})$ ovvero l'insieme delle funzioni continue da $[x_0-d',x_0+d']$ a $\overline{B_\delta(y_0)}$. $\mathcal{X}$ è uno spazio metrico completo rispetto alla metrica $d(\varphi,\psi):=||\varphi-\psi||_\infty$ (perchè il limite uniforme di funzioni continue è continuo e $\overline{B_\delta(y_0)}$ è chiuso).\\

Sia $T$ l'operatore integrale di Volterra: $$(T\varphi)(x):=y_0+\int_{x_0}^xf(u,\varphi(u))du$$

1) $T$ è una mappa $\mathcal{X}\to\mathcal{X}$ infatti è chiaro che se $\varphi\in\mathcal{X}$ allora $T\varphi$ è continua su $[x_0-d',x_0+d']$ (qui si usa la continuità di $f$) e inoltre:

$$||(T\varphi)(x)-y_0||_2=\left|\left|\int_{x_0}^x f(u,\varphi(u))du\right|\right|_2\leq\left|\int_{x_0}^x ||f(u,\varphi(u))||_2 du\right|\leq M(x-x_0)\leq Md'\leq\delta$$

2) $T$ è una contrazione. Infatti se $\varphi,\psi\in\mathcal{X}$ si ha: 

$$||(T\varphi)(x)-(T\psi)(x)||_2=\left|\left|\int_{x_0}^x (f(u,\varphi(u)-f(u,\psi(u))du\right|\right|_2\leq\left|\int_{x_0}^x ||f(u,\varphi(u)-f(u,\psi(u)||_2 du\right|$$

ma $f$ è localmente lipschitziana nelle $y$ uniformemente nelle $x$ quindi:

$$\leq\left| \int_{x_0}^x L||\varphi(u)-\psi(u)||_2du\right|\leq L||\varphi-\psi||_\infty\cdot(x-x_0)\leq d'L\cdot||\varphi-\psi||_\infty$$

Questo vale $\forall x\in[x_0-d',x_0+d']$ allora abbiamo verificato che: 

$$||T\varphi-T\psi||_\infty\leq d'L\cdot||\varphi-\psi||_\infty$$

ma $d'L\leq\frac{1}{2}$ quindi $T$ è una contrazione.\\

3) Quindi $T$ è una contrazione e $\mathcal{X}$ è completo, dal teorema di Banach - Cacciopoli sappiamo che esiste ed è unico il punto fisso per $T$ pvvero una mappa $\varphi$ tale che $T\varphi=\varphi$. Abbiamo già visto che l'esistenza e unicità della soluzione del problema di Volterra equivale all'esistenza e unicità del problema di Cauchy. $\Box$\\

\textbf{Esercizio 3.15:} Sia dato il problema di Cauchy:

$$\begin{cases}
y'=y \\
y(0)=1
\end{cases}$$

il teorema precedente garantisce l'esistenza e unicità della soluzione e lo fa tramite il teorema di Banach - Cacciopoli applicato a $(T\varphi)(x):=1+\int_0^x\varphi(u)du$. Sia $\varphi_0(x)\equiv 0$ e dato $\varphi_n$ costruisco $\varphi_{n+1}(x)=(T\varphi_n)(x)=1+\int_0^x\varphi_n(u)du$.\begin{enumerate}
\item Calcolare $\varphi_1,\varphi_2,\varphi_3,\ldots$ 
\item Determinare $\varphi_n$ esplicitamente $\forall n$ (per induzione)
\item Verificare che $\varphi_n(x)\to \exp^x$ $\forall x\in\mathbb{R}$ e verificare che effettivamente $\exp^x$ soddisfa il problema di Cauchy.\\
\end{enumerate}

Quindi dato un problema di Cauchy nella forma del teorema se $f$ è continua il problema di Cauchy ha almeno una soluzione locale (Peano), se poi $f$ è di classe $\mathcal{C}^1$ la soluzione è unica ovvero l'esistenza e unicità locale sono legate alla regolarità di $f$.\\

Diversa è invece la questione dell'esistenza e/o unicità globale.\\

\textbf{Esempio 3.16:} Si consideri:

$$\begin{cases}
y'=y^2 \\
y(0)=\alpha
\end{cases}$$

esso ammette come soluzione $y(x)=\frac{\alpha}{1-\alpha x}$ negli intervalli $(-\infty,frac{1}{\alpha}$ se $\alpha>0$, $(\frac{1}{\alpha},+\infty)$ se $\alpha<0$ e $\mathbb{R}$ se $\alpha=0$. Ciò nonostante $y'=f(x,y)$ con $f(x,y)\in\mathcal{C}^\infty(\mathbb{R}\times\mathbb{R})$. L'esistenza e unicità in grande è infatti sensibile alla crescita di $f$ in $y$.\\

\textbf{Teorema esistenza e unicità globale (striscia) 3.17:} Sia $f:S:=[a,b]\times\mathbb{R}^n\to\mathbb{R}^n$, $f\in\mathcal{C}(S)$. Supponiamo che f sia lipschitziana nelle $y$ uniformemente nelle $x$, ovvero $\exists L>0$ tale che: $\forall x\in [a,b]$, $\forall y_1,y_2\in\mathbb{R}^n$  $||f(x,y_1)-f(x,y_2)||_2\leq L||y_1-y_2||$ allora ogni problema di Cauchy della forma:

$$\begin{cases}
y'=f(x,y)\\
y(x_0)=y_0
\end{cases}$$

con $(x_0,y_0)\in S$ ha un'unica soluzione su $[a,b]$.\\

\textbf{Osservazione 3.18:} La condizione su $f$ dice (tra le altre cose) che $f$ cresce al più linearmente nelle $y$ visto che:

$$||f(x,y)||_2\leq ||f(x,0)||_2 +||f(x,y)-f(x,0)||_2\leq A+L||y||_2$$

dove $A:=\max ||f(x,0)||_2$ (con $x\in[a,b]$) che esiste per la continuità di $f$.\\

\textbf{Dimostrazione:} Dimostro la tesi prima in $[x_0,b]$ poi in $[a,x_0]$ dopo di che uso il teorema di esistenza e unicità locale per unire le due soluzioni in un'unica soluzione in $[a,b]$\\

Dimostro su $[x_0,b]$. Sia $\mathcal{X}:=\mathcal{C}([x_0,b],\mathbb{R}^n)$ e sia:

$$(T\varphi)(x)=y_0+\int_{x_0}^x f(u,\varphi(u))du$$

è chiaro che $T:\mathcal{X}\to\mathcal{X}$ (più semplice dell'altra volta perchè le funzioni in $\mathcal{X}$ non sono soggette a vincoli se non la continuità). Osservo che $\mathcal{X}$ è completo rispetto a $||$ $||_\infty$.\\

Introduco però una norma diversa fisso $\lambda>0$ e pongo:

$$||\varphi||_\lambda:=\sup\limits_{x_0\in[x_0,b]}||e^{-\lambda x}\varphi (x)||_2$$

Osservo che $e^{-\lambda b}\leq e^{-\lambda x}\leq e^{-\lambda x_0}$ e che quindi:

$$e^{-\lambda b}||\varphi||_\infty\leq||\varphi||_\lambda\leq e^{-\lambda x_0}||\varphi||_\infty$$

Questo dimostra che $||$ $||_\infty$ e $||$ $||_\lambda$ sono norme equivalenti per cui $\mathcal{X}$ è completo anche rispetto a $||$ $||_\lambda$. Ora:

$$||T\varphi(x)-T\psi(x)||_2\leq\int_{x_0}^x ||f(u,\varphi(u))-f(u,\psi(u)||_2 du$$

ma $f$ è lipschitziana quindi:

$$L\int_{x_0}^x ||\varphi(u)-\psi(u)||_2 du=L\int_{x_0}^x e^{\lambda u}\underbrace{e^{-\lambda u} ||\varphi(u)-\psi(u)||_2}_{\leq||\varphi-\psi||_\lambda} du$$ $$\leq L||\varphi-\psi||_\lambda\int_{x_0}^x e^{\lambda u}du=\frac{L}{\lambda}(e^{\lambda x}-e^{\lambda x_0})||\varphi-\psi||_\lambda$$

Ma allora $$e^{-\lambda x}||T\varphi(x)-T\psi(x)||_2\leq\frac{L}{\lambda}(1-e^{-\lambda(x-x_0)}||\varphi-\psi||_\lambda\leq\frac{L}{\lambda}||\varphi-\psi||_\lambda$$ ovvero:

$$||T\varphi-T\psi||_\lambda\leq\frac{L}{\lambda}||\varphi-\psi||_\lambda$$

Scegliendo $\lambda>L$ si trova una norma rispetto alla quale $T$ è una contrazione.\\

Su $[a,x_0]$ uso invece $$||\varphi||_\lambda:=\sup\limits_{x_0\in[a,x_0]}||e^{\lambda x}\varphi (x)||_2$$ (sempre $\lambda>0$). $\Box$\\

\section{Stabilità rispetto al modello e ai dati iniziali}

\textbf{Teorema 3.19:} Siano $f,g:[a,b]\times B\to\mathbb{R}^n$ dove $B$ è un aperto di $\mathbb{R}^n$. Supponiamo che:
\begin{itemize}
\item $\exists L$ con $||f(x,y)-f(x,z)||\leq L||y-z||$ $\forall x\in[a,b]$ $y,z\in B$
\item $\exists R:[a,b]\to\mathbb{R}$ continua con $||f(x,y)-g(x,y)||\leq R(x)$ $\forall x\in[a,b]$ $y\in B$
\end{itemize}
Siano $\alpha,\beta\in B$ e siano $\varphi$ e $\psi$ di classe $\mathcal{C}^1([\alpha,\beta])$ che soddisfano: 

$$\varphi:\begin{cases} \varphi'(x)=f(x,\varphi(x))\\ \varphi(a)=\alpha\end{cases} \psi:\begin{cases} \psi'(x)=g(x,\psi(x))\\ \psi(a)=\beta\end{cases}$$

Allora $||\varphi(x)-\psi(x)||\leq e^{L(x-a)}||\alpha-\beta||+e^{L(x-a)}\int_a^x e^{-L(s-a)}R(s)ds$.\\

\textbf{Dimostrazione:} Considero $||\varphi(x)-\psi(x)||$, questa è una mappa $[a,b]\to\mathbb{R}$ continua che in tutti i punti in cui $\varphi(x)\ne\psi(x)$ soddisfa:

$$\frac{d}{dx}||\varphi(x)-\psi(x)||=\frac{d}{dx}\left[ \sum_{i=1}^n(\varphi_i(x)-\psi_i(x))^2\right]^{\frac{1}{2}}=\frac{<\varphi(x)-\psi(x),\varphi'(x)-\psi'(x)>}{||\varphi(x)-\psi(x)||}$$

Così dalla disuguaglianza di Cauchy - Schwartz segue che:

$$\leq\frac{||\varphi(x)-\psi(x)||\cdot||\varphi'(x)-\psi'(x)||}{||\varphi(x)-\psi(x)||}=||\varphi'(x)-\psi'(x)||$$

Dalle ipotesi iniziali posso scrivere che questa quantità è uguale a:

$$=||f(x,\varphi(x))-g(x,\psi(x))||\leq ||f(x,\varphi(x))-f(x,\psi(x))||+||f(x,\psi(x))-g(x,\psi(x))||\mbox{ovvero}$$ $$\frac{d}{dx}||\varphi(x)-\psi(x)||\leq L||\varphi(x)-\psi(x)||+R(x)$$

Moltiplico la disuguaglianza per $e^{-Lx}$ ed ottengo:

$$\frac{d}{dx}(e^{-Lx}||\varphi(x)-\psi(x)||)\leq e^{-Lx}R(x)$$ che integrata su $[a,x]$ dà:

$$ e^{-Lx}||\varphi(x)-\psi(x)||-e^{-La}||\alpha-\beta||\leq\int_a^xe^{-Ls}R(s)ds$$ 

quindi:

$$||\varphi(x)-\psi(x)||\leq e^{L(x-a)}||\alpha-\beta||+e^{Lx}\int_a^x e^{-Ls}R(s)ds$$

che è la tesi ma è stata ottenuta supponendo $\varphi(x)\ne\psi(x)$ nell'intervallo di integrazione (o almeno nella sua parte aperta). La tesi in generale è ottenuta incollando questi risultati (cosa che è possibile perche $||\varphi(x)-\psi(x)||$ è continua su $[a,b]$). $\Box$\\

Questo risultato è noto col nome di Lemma di Gronwall. Dà una stima di quanto velocemente cambia la soluzione al variare della condizione iniziale ($\alpha$ o $\beta$) e della forma dell'equazione ($f$ o $g$). La stima in sè non è però molto buona per via del termine $e^{L(x-a)}$ (esponenziale). Questo risultato è il più semplice all'interno di una vasta famiglia di risultati di dipendenza continua dei parametri.\\

\section{Equazioni lineari}

Sono equazioni della forma:

$$y^{(n)}+a_1(x)y^{(n-1)}+\ldots+a_{n-1}(x)y'+a_n(x)y=f(x)$$

dove $a_1,\ldots,a_n$ e $f:[a,b]\to\mathbb{R}$ sono continue. Il teorema di base è una nostra vecchia conoscenza:\\

\textbf{Teorema 3.20:} Per ogni scelta di $y_0,\ldots,y_{n-1}\in\mathbb{R}$ e di $x_0\in[a,b]$ il problema di Cauchy:

$$\begin{cases}
y^{(n)}+a_1(x)y^{(n-1)}+\ldots+a_{n-1}(x)y'+a_n(x)y=f(x)\\
y^{(j)}(x_0)=y_j\quad j=0,\ldots,n-1
\end{cases}$$

ha una e una sola soluzione su $[a,b]$.\\

\textbf{Dimostrazione:} Formuliamo il problema di Cauchy come problema del primo ordine:

$$\begin{cases}
\underline{z}'=F(x,\underline{z})\\
y^{(j)}(x_0)=y_j\quad j=0,\ldots,n-1
\end{cases}\mbox{ dove } z:= \begin{bmatrix} y\\y'\\ \vdots \\ y^{(n-1)} \end{bmatrix}\mbox{ e } F(x,\underline{z})= \begin{bmatrix} z_2 \\ z_3 \\ \vdots \\ f(x)-\sum_{j=1}^n a_{n-j}(x)z_j \end{bmatrix}$$

ed osserviamo che $F:[a,b]\times\mathbb{R}^n\to\mathbb{R}^n$, è continua, e che:

$$||F(x,z)-F(x,w)||^2=(z_1-w_1)^2+\ldots+(z_n-w_n)^2+\left( \sum_{j=1}^n a_{1+n-j}(x)(z_j-w_j)\right)^2$$ $$ \leq ||z-w||^2+\left( \sum_{j=1}^n a_{1+n-j}^2(x)\right)||z-w||^2=\left( 1+\sum_{j=1}^n a_{1+n-j}^2(x)\right)||z-w||^2$$

e visto che le $a_j$ sono continue su $[a,b]$ la funzione $[1+\sum_{j=1}^n a_j^2(x)]^\frac{1}{2}$ ha un massimo su $[a,b]$ che chiamo $L$ e così:

$$||F(x,z)-F(x,w)||\leq L||z-w||$$

uniformemente in $[a,b]$. Quindi $F$ ha tutte le proprietà del teorema di esistenza e unicità globale. $\Box$\\

Date $a_1,\ldots,a_n$ e $f$ come sopra (quindi continue) pongo:

$$\mathcal{L}:\mathcal{C}^n([a,b])\to\mathcal{C}([a,b])$$
$$\varphi\mapsto(\mathcal{L}\varphi)(x):=\varphi^{(n)}+\sum_{j=1}^n a_j(x)\varphi^{(n-j)}(x)$$

Si osservi che effettivamente $\mathcal{L}\varphi\in\mathcal{C}([a,b])$.\\

$\mathcal{L}$ è un operatore differenziale ed usando $\mathcal{L}$ l'equazione è scritta come:

$$\mathcal{L}y=f$$

Risolvere l'equazione quindi equivale a trovare la contro immagine di f tramite $\mathcal{L}$. L'osservazione chiave è che $\mathcal{L}$ è lineare infatti $\mathcal{L}(\lambda\varphi+\psi)=\lambda\mathcal{L}\varpi+\mathcal{L}\psi$. Ne segue che:\\

\textbf{Teorema 3.21:} L'insieme delle soluzioni $\mathcal{L}y=f$ è uno spazio affine ovvero può essere individuato così:\begin{itemize}
\item Sia $V:=\ker\mathcal{L}=\{y:\mathcal{L}y=0\}$ allora $V$ è spazio vettoriale
\item Sia $\varphi_0$ una funzione tale che $\mathcal{L}\varphi_0=f$ allora:
\end{itemize}
$$\{\varphi:\mathcal{L}\varphi=f\}=V+\varphi_0$$

\textbf{Dimostrazione: }$\supseteq)$ Se $\tilde{\varphi}\in V$ allora $\mathcal{L}(\tilde{\varphi}+\varphi_0)=\mathcal{L}\tilde{\varphi}+\mathcal{L}\varphi_0=0+f=f$. 

$\subseteq)$ Se $\psi$ è tale che $\mathcal{L}\psi=f$ allora $\psi=(\psi-\varphi_0)+\varphi_0$ e $\mathcal{L}(\psi-\varphi_0)=\mathcal{L}(\psi)-\mathcal{L}\varphi_0=f-f=0\Rightarrow\psi-\varphi_0\in V$. $\Box$\\


\subsection{Costruzione di una base}

La seguente proposizione costituisce il cuore di questa teoria:\\

\textbf{Teorema 3.22:} Sia $\mathcal{L}$ come sopra e $V:=\ker\mathcal{L}=\{\varphi\in\mathcal{C}^n([a,b]): \mathcal{L}\varphi=0\}$) allora $\dim V=n$.\\

\textbf{Dimostrazione: }Considero gli $n$ problemi di Cauchy seguenti ($x_0\in[a,b]$ fissato):

$$\begin{cases}
\mathcal{L}y=0\\
y(x_0)=1\\
y'(x_0)=0\\
y''(x_0)=0\\
\vdots \\
y^{(n-1)}(x_0)=0
\end{cases}
\begin{cases}
\mathcal{L}y=0\\
y(x_0)=0\\
y'(x_0)=1\\
y''(x_0)=0\\
\vdots \\
y^{(n-1)}(x_0)=0
\end{cases}
\ldots\quad
\begin{cases}
\mathcal{L}y=0\\
y(x_0)=0\\
y'(x_0)=0\\
y''(x_0)=0\\
\vdots \\
y^{(n-1)}(x_0)=1
\end{cases}$$
che hanno tutti una e una sola soluzione (dal teorema precedente) e le chiamo rispettivamente $\varphi_1,\varphi_2,\ldots,\varphi_n$.\\

($\dim V\geq n$) Dimostro che le $\varphi_j$ sono linearmente indipendenti su $\mathbb{R}$. Infatti supponiamo che $\alpha_1,\alpha_2,\ldots,\alpha_n\in\mathbb{R}$ siano tali che $\sum_{J=1}^n\alpha_j\varphi_j=:\psi$ sia la funzione identicamente nulla. Allora $\psi^{(k)}(x_0)=0\quad\forall k$ ma dalle condizioni iniziali segue che:
$$\psi^{(k)}(x)=\sum_{j=1}^n\alpha_j\varphi_j^{(k)}(x)\Rightarrow 0=\psi^{(k)}(x_0)=\sum_{J=1}^n\alpha_j\varphi_j^{(k)}(x_0)=\sum_{j=1}^n\alpha_j\delta_{j,k+1}=\alpha_{k+1}$$
quindi ogni $\alpha_k=0$.\\

($\dim V\leq n$) Sia $h\in V$ allora $h\in\mathcal{C}^n([a,b])$ e in particolare i numeri $h^{(j)}(x_0)$ con $j=0,\ldots,n-1$ sono ben definiti. Sia $\psi^{(k)}=\sum_{j=1}^n h^{(j-1)}(x_0)\varphi_j$. Osservo che $\psi\in\mathcal{C}^n([a,b])$ e $\mathcal{L}\psi=0$ (perchè $\mathcal{L}\varphi_j=0\quad\forall j$ e $\mathcal{L}$ è lineare). Inoltre con lo stesso conto di prima mostriamo che $\psi^{(k)}(x_0)=\sum_{j=1}^n h^{(j-1)}(x_0)\varphi_j^{(k)}(x_0)=\sum_{J=1}^n h^{(j-1)}(x_0)\delta_{j,k+1}=h^{(k)}(x_0)$. Quindi sia $h$ che $\psi$ sono soluzioni del problema di Cauchy:
$$\begin{cases}
\mathcal{L}y=0\\
y^{(j)}(x_0)=h^{(j)}(x_0)\quad\forall j=0,\ldots,n-1
\end{cases}$$
Ma sappiamo che il problema di Cauchy ha un'unica soluzione quindi $h=\psi$ ovvero $h(x)=\sum_{j=1}^n h^{(j-1)}(x_0)\varphi_j^(x)$ quindi $h$ è combinazione lineare delle $\varphi_j$. $\Box$\\

\textbf{Osservazione 3.23:} La dimostrazione del teorema mostra che le $\varphi_j$ costituiscono una base per $V$. Possiamo quindi produrre una base costruendo le soluzioni degli $n$ problemi di Cauchy (anche se non è un metodo pratico per farlo).\\

\subsection{Matrice Wronskiana}

\textbf{Definizione 3.24: }Siano $\varphi_1,\ldots,\varphi_n$ $n$ soluzioni di $\mathcal{L}y=0$ (non necessariamente quelle del teorema) chiamo matrice Wronskiana la matrice $n\times n$:

$$W:=\begin{bmatrix}
\varphi_1 & \varphi_2 & \ldots & \varphi_n \\
\varphi'_1 & \varphi'_2 & \ldots & \varphi'_n \\
\vdots & \vdots & \ddots & \vdots\\
\varphi_1^{(n-1)} & \varphi_2^{(n-1)} & \ldots & \varphi^{(n-1)}_n \\
\end{bmatrix}$$
e Wronskiano $\det(W(x))$.\\

\textbf{Proposizione 3.25: }Sia $\mathcal{L}$ come sempre allora $(\det W)'(x)=-a_1(x)(\det W)(x)$ $\forall x\in [a,b]$ e quindi $(\det W)(x)=(\det W)(x_0)\cdot e^{-\int_{x_0}^x a_1(u)du}$. In particolare $(\det W)(x)=0$ per qualche $x$ se e solo se $(det W)(x)=0\quad\forall x$.\\

\textbf{Dimostrazione: }É un calcolo (un po' noioso per la verità): ricordiamo che il $\det W$ è somma (con segno) dei prodotti degli elementi allora $(\det W)'$ è la somma (con segno) delle derivate dei prodotti ma ogni derivata del prodotto è somma di $n$ termini in cui la derivata è eseguita sui termini successivi. Da cui segue che $(\det W)'(x)=$
$$\det\begin{bmatrix}
\varphi_1' & \ldots & \varphi_n'  \\
\varphi_1' & \ldots & \varphi_n'\\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-1)} & \ldots & \varphi_n^{(n-1)}
\end{bmatrix} + \det\begin{bmatrix}
\varphi_1 & \ldots & \varphi_n  \\
\varphi_1'' & \ldots & \varphi_n''\\
\varphi_1'' & \ldots & \varphi_n''\\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-1)} & \ldots & \varphi_n^{(n-1)}
\end{bmatrix}+\ldots+\det\begin{bmatrix}
\varphi_1 & \ldots & \varphi_n  \\
\varphi_1' & \ldots & \varphi_n'\\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)} & \ldots & \varphi_n^{(n-2)}\\
\varphi_1^{(n)} & \ldots & \varphi_n^{(n)}\end{bmatrix}$$I primi $n-1$ termini della sommatoria sono in realtà nulli poichè hanno due righe uguali. Abbiamo quindi:
$$(\det W)'(x)=\det\begin{bmatrix}
\varphi_1 & \ldots & \varphi_n  \\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)} & \ldots & \varphi_n^{(n-2)}\\
\varphi_1^{(n)} & \ldots & \varphi_n^{(n)}\end{bmatrix}$$ Ma $\mathcal{L}\varphi_j=0$ ovvero $\varphi^{(n)}_j=-\sum^n_{k=1}a_k(x)\varphi_j^{(n-k)}\quad\forall j$ quindi sostituendo:
$$\det\begin{bmatrix}
\varphi_1 & \ldots & \varphi_n  \\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)} & \ldots & \varphi_n^{(n-2)}\\
-\sum^n_{k=1}a_k(x)\varphi_1^{(n-k)} & \ldots & -\sum^n_{k=1}a_k(x)\varphi_n^{(n-k)}\end{bmatrix}=-\sum^n_{k=1}a_k(x)\det\begin{bmatrix}
\varphi_1 & \ldots & \varphi_n  \\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)} & \ldots & \varphi_n^{(n-2)}\\
\varphi_1^{(n-k)} & \ldots & \varphi_n^{(n-k)}\end{bmatrix}$$ ma i termini con $k\geq2$ hanno due righe uguali quindi resta solo $k=1$ che dà:
$$=-a_1\det\begin{bmatrix}
\varphi_1 &   \ldots & \varphi_n \\
\vdots & \ddots & \vdots\\
\varphi_1^{(n-1)} & \ldots & \varphi^{(n-1)}_n\end{bmatrix}=-a_1(\det W)(x)$$ che è l'equazione data. Le altre formule sono delle immediate conseguenze. $\Box$ \\

\subsection{Costruzione di una soluzione}

Cerchiamo ora di costruire una soluzione di $\mathcal{L }y=f$ ($f\in\mathcal{C}([a,b])$) nota una base di $\mathcal{L }y=0$.\\

Siano $\varphi_1,\ldots,\varphi_n$ una base del nucleo, cerco una soluzione di $\mathcal{L }y=f$ che sia della forma:
$$\psi(x)=\sum_{j=1}^n A_j(x)\varphi_j(x)$$
con $A_1,\ldots,A_n$ funzioni incognite da determinare. Osservo che $\psi(x)=\left\langle A(x),\varphi(x)\right\rangle$ dove $\underline{A}=(A_1,\ldots,A_n)$ e $\underline{\varphi}=(\varphi_1,\ldots,\varphi_n)$. Allora $\psi'(x)=\left\langle A'(x),\varphi(x)\right\rangle+\left\langle A(x),\varphi'(x)\right\rangle$ ma se impongo la condizione $\left\langle A'(x),\varphi(x)\right\rangle=0$ si ha:
$$\psi'(x)=\left\langle A(x),\varphi'(x)\right\rangle$$
Ripeto il procedimento: $\psi''(x)=\left\langle A'(x),\varphi'(x)\right\rangle+\left\langle A(x),\varphi''(x)\right\rangle$, impongo $\left\langle A'(x),\varphi'(x)\right\rangle=0$ e rimane $\psi''(x)=\left\langle A(x),\varphi''(x)\right\rangle$. Itero fino a ottenere:
$$\left.\begin{array}{ll} \psi'= \left\langle A(x),\varphi'(x)\right\rangle,\qquad  \left\langle A'(x),\varphi(x)\right\rangle=0 \\
\psi''= \left\langle A(x),\varphi''(x)\right\rangle,\qquad  \left\langle A'(x),\varphi'(x)\right\rangle=0\\
\vdots\\
\psi^{(n-1)}= \left\langle A(x),\varphi^{(n-1)}(x)\right\rangle,\qquad  \left\langle A'(x),\varphi^{(n-1)}(x)\right\rangle=0\end{array}\right\}(*)$$
ed infine $\psi^{(n)}= \left\langle A'(x),\varphi^{(n-1)}(x)\right\rangle+\left\langle A(x),\varphi^{(n)}(x)\right\rangle$. Ma allora:
$$\mathbf{L}\underline{\varphi}=\psi^{(n)}+\sum_{j=0}^{n-1}a_{n-j}\psi^{(j)}=\left\langle A'(x),\varphi^{(n-1)}(x)\right\rangle+\left\langle A(x),\varphi^{(n)}(x)\right\rangle+\sum_{j=0}^{n-1}a_{n-j}\left\langle A(x),\varphi^{(j)}(x)\right\rangle=$$
$$\left\langle A'(x),\varphi^{(n-1)}(x)\right\rangle+\left\langle A(x),\mathcal{L}\varphi\right\rangle$$
ma $\varphi_1,\ldots,\varphi_n$ sono una base quindi $\mathcal{L}\varphi=(\mathcal{L}\varphi_1,\ldots,\mathcal{L}\varphi_n)=0$ quindi:
$$\mathcal{L}\psi=\left\langle A'(x),\varphi^{(n-1)}(x)\right\rangle\qquad (**)$$

Da $(*)$ e $(**)$ segue che affinché $\psi$ soddisfi l'equazione $\mathcal{L}\psi=f$ basta che le $A_1,\ldots,A_n$ soddisfino:

$$\begin{cases}
\left\langle A'(x),\varphi(x)\right\rangle=0\\
\left\langle A'(x),\varphi'(x)\right\rangle=0\\
\vdots\\
\left\langle A'(x),\varphi(x)^{(n-2)}\right\rangle=0\\
\left\langle A'(x),\varphi(x)^{(n-1)}\right\rangle=f
\end{cases}$$
ovvero che:
$$W\begin{bmatrix}
A_1'\\
\vdots\\
A_{n-1}'\\
A_n'
\end{bmatrix} = \begin{bmatrix}
0\\ \vdots \\ 0 \\ f
\end{bmatrix}$$
dove $W$ è la matrice Wronskiana.\\

Sappiamo che $\det W\ne 0$ $\forall x$ quindi il sistema nelle $A_1',\ldots,A_n'$ può sempre essere risolto e una volta fatto si trovano le $A_j$ integrando le $A'_j$ trovate. In una formula:

$$\begin{bmatrix}
A_1'\\
\vdots\\
A_{n-1}'\\
A_n'
\end{bmatrix}=W^{-1}\begin{bmatrix}
0\\ \vdots \\ 0 \\ f
\end{bmatrix} \mbox{ quindi }\begin{bmatrix}
A_1(x)\\
\vdots\\
A_{n-1}(x)\\
A_n(x)
\end{bmatrix}=\int_{x_0}^xW^{-1}(u)\begin{bmatrix}
0\\ \vdots \\ 0 \\ f(u)
\end{bmatrix} du$$
e così:
$$\psi(x)=\sum_{j=1}^n A_j(x)\varphi_j(x)=[\varphi_1(x),\ldots,\varphi_n(x)]\cdot\begin{bmatrix}
A_1(x)\\
\vdots\\
A_n(x)
\end{bmatrix}=[\varphi_1(x),\ldots,\varphi_n(x)]\int_{x_0}^xW^{-1}(u)\begin{bmatrix}
0\\ \vdots \\ 0 \\ f(u)
\end{bmatrix} du$$
Tenendo conto della formula per l'inversa di una matrice questa può essere scritta anche così:
$$\psi(x)=[\varphi_1(x),\ldots,\varphi_n(x)]\int_{x_0}^xW^{-1}(u)\begin{bmatrix}
0\\ \vdots \\ 0 \\ f(u)
\end{bmatrix} du=\int_{x_0}^x\frac{\det\begin{bmatrix}
\varphi_1(u) & \ldots & \varphi_n(u)  \\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)}(u) & \ldots & \varphi_n^{(n-2)}(u)\\
\varphi_1(x) & \ldots & \varphi_n(x)\end{bmatrix}}{\det\begin{bmatrix}
\varphi_1(u) & \ldots & \varphi_n(u)  \\
\vdots & \ddots & \vdots \\
\varphi_1^{(n-2)}(u) & \ldots & \varphi_n^{(n-2)}(u)\\
\varphi_1^{(n-1)}(u) & \ldots & \varphi_n^{(n-1)}(u)\end{bmatrix}}f(u)du$$

Si noti che nell'ultima riga del numeratore la variabile è $x$ e il denominatore è sempre diverso da 0 perchè è il Wronskiano.\\

La formula precedente presuppone la conoscenza di una base per $\ker\mathcal{L}$ e la dimostrazione del teorema che afferma che $\dim\ker\mathcal{L}=n$ contiene una "ricetta" per costruire una base: risolvere $n$ problemi di Cauchy con condizioni indipendenti. Purtroppo questo è difficile da fare concretamente. Per le equazioni lineari a coefficienti costanti questo può essere fatto. Ecco come, sia:
$$\mathcal{L}y=y^{(n)}+a_1y^{(n-1)}+\ldots+a_{n-1}y'+a_ny^{(0)}$$
con $a_j\in\mathbb{R}$ (costanti). Sia $P\in\mathbb{R}[z]$ il polinomio di grado $n$ costruito così a partire da $\mathcal{L}$:

$$P(z):=z^n+a_1z^{n-1}+\ldots+a_{n-1}z+a_n$$
detto polinomio caratteristico. Osserviamo che se $y(x)=e^{\lambda x}$, con $\lambda$ costante, allora $y'(x)=\lambda e^{\lambda x}$, $y''(x)=\lambda^2 e^{\lambda x}$ ed in generale $y^{(j)}(x)=\lambda^j e^{\lambda x}$. Ne segue che:

$$\mathcal{L}(e^{\lambda x})=P(\lambda)e^{\lambda x}$$

Questa relazione evidenzia che se $\lambda$ è una radice di $P$, quindi se $P(\lambda)=0$, allora:

$$\mathcal{L}(e^{\lambda x})=P(\lambda)e^{\lambda x}=0$$

e quindi $e^{\lambda x}\in\ker\mathcal{L}$. Possiamo perciò costruire funzioni del nucleo ponendo $e^{\lambda x}$ e facendo variare $\lambda$ tra le radici di $P$.\\

\textbf{Esercizio 3.26:} Sia data l'equazione: $y''-3y'+2y=0$, $\mathcal{L}(y)=y''-3y'+2y$.

Il polinomio caratteristico è: $P(z)=z^2-3z+2$ ha radici 1 e 2 allora $e^x$ e $e^{2x}$ sono nel nucleo. È chiaro che queste funzioni sono linearmente indipendenti quindi $\ker\mathcal{L}=span_\mathbb{R}( e^x,e^{2x})$.\\

Vi sono però due problemi:\\

\textbf{Esempio 3.27: } (radici multiple) Si consideri: $y''-2y'+y=0$ allora il polinomio caratteristico $P(z)=z^2-2z+1=(z-1)^2$ ha una sola radice doppia 1; la procedura costruisce una sola funzione $e^x$ ma il nucleo ha dimensione 2 quindi manca una funzione.\\

\textbf{Esempio 3.28: } (radici non reali) Si consideri: $y''+2y'+5y=0$ e il polinomio caratteristico ha radici $-1+2i$ e $-1-2i$ non reali. Le funzioni $e^{-1+2i}$ e $e^{-1-2i}$ non sono a valori reali e quindi non sono nel $\ker\mathcal{L}$ reale.\\

Entrambi i problemi posso però essere risolti. Ricordiamo che se $\lambda\in\mathbb{C}$ è una radice di $P$ allora la molteplicità $\mu$ di $\lambda$ è il massimo intero positivo tale che $(z-\lambda)^\mu$ divide $P(z)$. Quindi se $\lambda_1,\ldots,\lambda_k$ sono le radici distinte in $\mathbb{C}$ di $P$ e se $\mu_1,\ldots,\mu_n$ le loro molteplicità, dal teorema fondamentale dell' algebra segue che: 
$$P(z)=(z-\lambda_1)^{\mu_1}\cdot\ldots\cdot(z-\lambda_k)^{\mu_k}$$
Inoltre se $\lambda\in\mathbb{C}$ è una radice di $P(z)$ allora anche $\overline{\lambda}$ è radice di $P(z)$ (perchè $P\in\mathbb{R}[z]$, per cui $P(\overline{\lambda})=\overline{P(\lambda)}=\overline{0}=0$) e le molteplicità di $\lambda$ e $\overline{\lambda}$ dono uguali. Si ha così:\\

\textbf{Teorema 3.29: }Dato $\mathcal{L}$ a coefficienti costanti e $P$ il suo polinomio caratteristico, $\forall\lambda$ radice di $P$ si prenda:\begin{itemize}
\item Se $\lambda\in\mathbb{R}$: $e^{\lambda x}$, $xe^{\lambda x}$, $x^2e^{\lambda x}$,$\ldots$, $x^{\mu-1}e^{\lambda x}$ ($\mu$ funzioni)
\item Se $\lambda\in\mathbb{C}\textbackslash\mathbb{R}$ alla coppia $(\lambda,\overline{\lambda})$ associo (scrivo $\lambda=u+iv$): $e^{ux}\cos(vx)$, $xe^{ux}\cos(vx)$,$\ldots$, $x^{\mu-1}e^{ux}\cos(vx)$ e $e^{ux}\sin(vx)$, $xe^{ux}\sin(vx)$,$\ldots$, $x^{\mu-1}e^{ux}\sin(vx)$ ($\mu+\mu$ funzioni)
\end{itemize} In questo modo si costruiscono $n$ funzioni che sono nel $\ker\mathcal{L}$ e sono linearmente indipendenti (sono una base perchè $\dim\ker\mathcal{L}=n$). \\

\textbf{Dimostrazione:} Sia $\lambda$ radice di molteplicità $\mu$ allora $p(z)=q(z)(z-\lambda)^\mu$ con $q(z)\in\mathbb{R}[z]$. Osservo che $\mathcal{L}=p(\frac{d}{dx})$, ovvero $\mathcal{L}$ è l'operatore differenziale associato al polinomio $p$ (con $z^j\mapsto\underbrace{\frac{d}{dx}\circ\cdot\circ{\frac{d}{dx}}}_{j-volte}=\frac{d^j}{dx^j}$)\\

L'operatore $\frac{d}{dx}$ e l'operatore $\lambda\cdot$ (quello che moltiplica per lo scalare $\lambda$) commutano ovvero $\frac{d}{dx}(\lambda(f(x))=\lambda(\frac{d}{dx}(f(x)))$ e questo è vero perchè $\lambda$ è costante. Ne segue che:
$$\mathcal{L}=p\left(\frac{d}{dx}\right)=q\left(\frac{d}{dx}\right)\left(\frac{d}{dx}-\lambda\right)^\mu\qquad(*)$$
Osserviamo che $\left(\frac{d}{dx}-\lambda\right)(e^{\lambda x})=\lambda e^{\lambda x}-\lambda e^{\lambda x}=0$ e che $\left(\frac{d}{dx}-\lambda\right)(x^l e^{\lambda x})=lx^{l-1}e^{\lambda x}+\lambda x^le^{\lambda x}-\lambda x^l e^{\lambda x}=lx^{l-1}e^{\lambda x}$ quindi interando si ha che:
$$\left(\frac{d}{dx}-\lambda\right)^\mu(x^l e^{\lambda x})=l(l-1)(l-2)\ldots(l-\mu+1)x^{l-\mu}e^{\lambda x}$$
e questo è $=0$ quando $l<\mu$ (perchè $l(l-1)\ldots(l-\mu+1)=0$ in tal caso). Questo dimostra che:

$$\mathcal{L}(x^le^{\lambda x})=p\left(\frac{d}{dx}\right)(x^le^{\lambda x})=q\left(\frac{d}{dx}\right)\left(\frac{d}{dx}-\lambda\right)^\mu(x^le^{\lambda x})=0$$
se $l<\mu$. Ovvero $e^{\lambda x},xe^{\lambda x},\ldots,x^{\mu-1}e^{\lambda x}\in\ker\mathcal{L}$. Si osservi che il calcolo vale anche se $\lambda\in\mathbb{C}$.\\

Ora mostriamo che sono linearmente indipendenti (si ammetta $\lambda\in\mathbb{C}$), basta farlo per la famiglia:
$$e^{\lambda x},xe^{\lambda x},\ldots,x^{\mu-1}e^{\lambda x}\quad(**)$$
perchè la famiglia $e^{ux}\cos(vx),e^{ux}\sin(vx)\ldots$ è combinazione lineare di quella esponenziale.\\

Siano $\lambda_1,\ldots,\lambda_k$ le radici in $\mathbb{C}$ distinte e siano $\mu_1,\ldots,\mu_k$ le loro molteplicità. La generica combinazione lineare delle $(**)$ corrisponde ad una somma:
$$\sum_{j=1}^kp_j(x)e^{\lambda_j x}$$
dove ogni $p_j$ è un polinomio a coefficienti complessi di grado $\leq\mu_j$. Dimostro che se tale combinazione è nulla sono nulli tutti i $p_j$ (e questo dà la tesi). Supponiamo che esistono dei $p_j\ne 0$ con:
$$\sum_{j=1}^kp_j(x)e^{\lambda_j x}=0\quad\forall x\quad(*3*)$$
Posso eliminare dalla somma quei $p_j$ che sono $=0$ quindi assumo che in $(*3*)$ ogni $p_j\ne0$. Per derivazione da $(*3*)$ segue:
$$\sum_{j=1}^k(p_j'(x)+p_j(x)\lambda_j)e^{\lambda_j x}=0$$
Itero il processo, ottengo che:
$$\sum_{j=1}^k(p_j''(x)+2\lambda_jp_j'(x)+\lambda_j^2p_j(x))e^{\lambda_j x}=0$$
Ovvero:
$$\begin{bmatrix}
p_1 & p_2 & \ldots & p_k\\
p_1'+\lambda_1p_1 & p_2'+\lambda_2p_2 & \ldots & p_k'+\lambda_kp_k\\
p_1''+2\lambda_1p_1'+\lambda_1^2p_1 & p_2''+2\lambda_2p_2'+\lambda_2^2p_2 & \ldots & p_k''+2\lambda_kp_k'+\lambda_k^2p_k\\
\vdots & \vdots & \ddots & \vdots\end{bmatrix}
\begin{bmatrix}
e^{\lambda_1 x}\\
e^{\lambda_2 x}\\
\vdots\\
e^{\lambda_k x}\\
\end{bmatrix}=0$$
Visto che il vettore $[e^{\lambda_1 x},\ldots,e^{\lambda_k x}]\ne 0$ deve essere che $\det M=0$ dove $M$ è la matrice scritta sopra.\\

Osserviamo che il grado di $p_1'$ è sempre minore di quello di $p_j$ quindi se $p_1(x)=a_1x^{\partial p}+$ termini di grado minore, allora:
$$\sum_{j=0}^l\binom{l}{j}\lambda_1^jp_1^{(l-j)}=\lambda_1^la_1x^{\partial p}+\mbox{ termini di grado minore}$$
e quindi:
$$0=\det M=\det\begin{bmatrix}
1 & 1 & \ldots & 1 \\
\lambda_1 & \lambda_2 &\ldots & \lambda_k\\
\vdots & \vdots & \ddots & \vdots \\
\lambda_1^k & \lambda_2^k &\ldots & \lambda_k^k\\
\end{bmatrix}$$
ma questo è impossibile perchè $a_1,\ldots,a_k$ sono $\ne 0$ e $\det M=\prod\limits_{1\leq i\leq j\leq k}(\lambda_i-\lambda_j)$ è $\ne 0$ (determinante di Van der Monde).\\

Siamo dunque arrivati a un assurdo che nasce dall'ipotesi che i $p_j$ siano $\ne 0$. $\Box$

\section{Alcuni esempi interessanti}

Sia $y''+4y=0$, le radici del polinomio caratteristico sono $\pm 2i$ allora lo spazio delle soluzioni è $span_\mathbb{R}(\cos(2x),\sin(2x))$. Se impongo:
$$\begin{cases}
y''+4y=0\\
y(0)=a\\
y'(0)=b \end{cases}$$
la soluzione è $a\cos(2x)+\frac{b}{2}\sin(2x)$.\\

Sia $E_x$ la mappa che manda $\binom{y(0)}{y'(0)}\mapsto\binom{y(x)}{y'(x)}$ (evoluzione al punto $x$). Allora:

$$E_x\binom{a}{b}=\binom{a\cos(2x)+\frac{b}{2}\sin(2x)}{-2a\sin(2x)+b\cos(2x)}=\begin{bmatrix} \cos(2x) & \frac{1}{2}\sin(2x) \\ -2\sin(2x) &  \cos(2x) \end{bmatrix}\begin{bmatrix}
a \\ b
\end{bmatrix}$$
quindi $E_x$ è una mappa lineare e la possiamo scrivere come:
$$\begin{bmatrix}
\sqrt{2} & 0\\
0 & \frac{1}{\sqrt{2}}
\end{bmatrix}
\begin{bmatrix}
\cos(2x) & \sin(2x) \\
-\sin(2x) & \cos(2x)
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{2}} & 0\\
0 & \sqrt{2}
\end{bmatrix}$$
Si noti che la matrice centrale rappresenta una rotazione.\\

\textbf{Osservazione 3.30}: Come trasforma una regione del piano $E_x$? $E_x$ trasforma le figure preservandone l'area. Questo deriva dal fatto che $\det E_x=1$ e questo a sua volta accade perchè la matrice $E_x$ è la Wronskiana del sistema (con base $\cos(2x),\frac{1}{2}\sin(2x)$) e sappiamo che $(det W)'=-a_1\det W$ dove $a_1$ è il coefficienti dell'equazione $y''+a_1y'+a_2y=0$ e in tal caso $a_1=0$ quindi $\det W$ è costante.\\

\textbf{Osservazione 3.31:} $\forall x,w$, si ha $E_{x+w}=E_x\cdot E_w$ infatti:
$$E_{x+w}=
\begin{bmatrix}
\sqrt{2} & 0\\
0 & \frac{1}{\sqrt{2}}
\end{bmatrix}
R_{x+w}\begin{bmatrix}
\frac{1}{\sqrt{2}} & 0\\
0 & \sqrt{2}
\end{bmatrix}=
\begin{bmatrix}
\sqrt{2} & 0\\
0 & \frac{1}{\sqrt{2}}
\end{bmatrix}
R_x R_w\begin{bmatrix}
\frac{1}{\sqrt{2}} & 0\\
0 & \sqrt{2}
\end{bmatrix}=$$
$$\begin{bmatrix}
\sqrt{2} & 0\\
0 & \frac{1}{\sqrt{2}}
\end{bmatrix}
R_x 
\begin{bmatrix}
\frac{1}{\sqrt{2}} & 0\\
0 & \sqrt{2}
\end{bmatrix}
\begin{bmatrix}
\sqrt{2} & 0\\
0 & \frac{1}{\sqrt{2}}
\end{bmatrix}
R_w
\begin{bmatrix}
\frac{1}{\sqrt{2}} & 0\\
0 & \sqrt{2}
\end{bmatrix}=E_x\cdot E_w$$

Quindi l'insieme degli $\{E_x:x\in\mathbb{R}\}$ è un gruppo abeliano di trasformazioni del piano (di fatto un sottogruppo di $SL(\mathbb{R}^2)$). Anche questo fenomeno ha una spiegazione generale ma illustrarla ci porterebbe troppo lontano...

\section{Soluzione particolare per le lineari a coefficienti costanti}

$$y^{(n)}+a_1y^{(n-1)}+\ldotsèa_ny=f(x),\quad f(x)\in\mathcal{C}([a,b])$$

Visto che conosciamo una base esplicita per il nucleo, possiamo trovare una soluzione particolare usando la formula generale. Questo è in effetti tutto quello che si può dire per una $f(x)$ generica. Tuttavia per certe $f(x)$ si può trovare una soluzione seguendo una strada puramente algebrica.\\

\textbf{Teorema 3.32:} Sia $f(x)$ della forma: $f(x)=q(x)e^{\lambda x}$ con $q\in\mathbb{R}[x]$ e $\lambda\in\mathbb{R}$. Sia $\mu$ la molteplicità ddi $\lambda$ come radice del polinomio caratteristico dell'equazione (con $\mu=0$ se $\lambda$ non è radice). Allora $\exists r(x)\in\mathbb{R}[x]$ con grado $r$= grado $q$ tale che:

$$\mathcal{L}(x^\mu r(x) e^{\lambda x})=q(x)e^{\lambda x}$$

Ovvero l'equazione $\mathcal{L}y=q(x)e^{\lambda x}$ ha $x^\mu r(x) e^{\lambda x}$ come soluzione particolare. Questo consente di trovare $r$ per via algebrica.\\

\textbf{Dimostrazione: }Per ogni scelta di interi $n$ e $d$ e $\forall \lambda\in\mathbb{R}$ considero l'insieme di funzioni $V_d^{(n,\lambda)}:=\{\varphi(x):\varphi(x)=r(x)x^ne^{\lambda x}\mbox{ con }r\in\mathbb{R}[x]\mbox{ e grado di }r\leq d\}$\\

Osservo che $V_d^{(n,\lambda)}$ è uno spazio $\mathbb{R}-$vettoriale di dimensione $d+1$ perchè $n$ e $\lambda$ sono fissati quindi gli stessi per tutti gli elementi. La dimostrazione consiste nel:\begin{enumerate}

\item Verificare che:
$$\mathcal{L}:V_d^{(n,\lambda)}\to V_d^{(0,\lambda)}$$
ovvero $\mathcal{L}$ manda elementi della forma $r(x)x^\mu e^{\lambda x}$ dove $\mu$ è la molteplicità di $\lambda$ e grado di $r\leq d$, in elementi della forma  $q(x)e^{\lambda x}$ con grado $q\leq d$)\\

\item Verificare che $\mathcal{L}$ è iniettiva

\end{enumerate}

Fatto ciò, visto che $\dim V_d^{(n,\lambda)}=d+1=\dim V_d^{(0,\lambda)}$  ne segue che $\mathcal{L}$ è anche suriettiva, ovvero la tesi.

\textbf{Osservazione 3.33:} Il teorema è applicabile a ogni $\lambda\in\mathbb{R}$ quindi anche quando $\lambda=0$ ovvero quando $f(x)=q(x)$ (è un polinomio).\\

\textbf{Osservazione 3.34:} Il teorema è applicabile anche se la radice è complessa. L'unico problema è che se $\lambda$ è complesso $e^{\lambda x}$ è a valori complessi e non reali.\\

\textbf{Esempio 3.35:} Sia $y''-4y=(x+2)e^x$. Il polinomio caratteristico è $z^2-4$ e ha radici $\pm 2$ con molteplicità 1.\\

$f(x)=(x+2)e^x$, è della forma prevista con $\lambda=1$ di molteplicità 0 e $q(x)=x+2$. Allora esiste una soluzione della forma $x^\mu r(x)e^{\lambda x}$ $\Rightarrow x^0(ax+b)e^x=(ax+b)e^x$ con $a$ e $b$ opportuni. Per trovarli derivo, se $y=(ax+b)e^x$ allora:

$$y'=(ax+a+b)e^x\qquad y''=(ax+2a+b)e^x$$
così:

$$(x+2)e^x=y''-4y=(ax+2a+b-4ax-4b)e^x=(-3ax+2a-3b)e^x$$

Ma allora $x+2=-3ax+2a-3b$ da cui si ricava $a=-\frac{1}{3}$ e $b=-\frac{8}{9}$ da cui la soluzione generale è $\alpha e^{2x}+\beta e^{-2x}-(\frac{1}{3}x+\frac{8}{9})e^x$\\

\textbf{Esempio 3.36:} Sia $y''-4y=(x+2)e^{2x}$. Come prima il polinomio caratteristico è $z^2-4$ e ha radici $\pm 2$ con molteplicità 1.\\

$f(x)=(x+2)e^{2x}$, è della forma prevista con $\lambda=2$ di molteplicità 1 e $q(x)=x+2$. Allora esiste una soluzione della forma $x^\mu r(x)e^{\lambda x}$ $\Rightarrow x^1(ax+b)e^{2x}=(ax^2+xb)e^{2x}$ con $a$ e $b$ opportuni. Per trovarli derivo come prima, se $y=(ax+b)e^{2x}$ allora:

$$y'=(2ax^2+(2a+2b)x+b)e{2x}\qquad y''=(4ax^2+(8a+4b)x+2a+4b)e^{2x}$$
così:

$$(x+2)e^x=y''-4y=(8ax+2a+4b)e^{2x}$$

Ma allora $x+2=-8ax+2a+4b$ da cui si ricava $a=\frac{1}{8}$ e $b=\frac{7}{16}$ da cui la soluzione generale è $\alpha e^{2x}+\beta e^{-2x}+(\frac{1}{8}x+\frac{7}{16})e^{2x}$.\\

Usando le formule trigonometriche di base $\cos(x)=\frac{e^{ix}+e^{-iv}}{2}$ e $\sin(x)=\frac{e^{ix}-e^{-iv}}{2}$ (e le inverse $e^{ix}=\cos(x)+i\sin(x)$, $e^{-ix}=\cos(x)-i\sin(x)$) si dimostra la seguente variante, se $f(x)$ ha la forma:

$$f(x)=q(x)e^{ux}\cos(vx)$$
Sia $\mu$ la molteplicità di $u+iv$ come radice del polinomio caratteristico. Allora esistono due polinomi $r_1$ e $r_2$ $\in\mathbb{R}[x]$ con grado entrambi $\leq$ grado $q$ tali che:

$$\mathcal{L}(x^\mu(r_1(x)\cos(vx)+r_2\sin(vx))e^{ux})=q(x)e^{ux}\cos(vx)$$
quindi $x^\mu(r_1(x)\cos(vx)+r_2\sin(vx))e^{ux}$ è una soluzione.\\

\textbf{Osservazione 3.37:} In generale $r_1$ e $r_2$ sono entrambi $\ne 0$, ovvero la soluzione contiene sia $\cos(vx)$, sia $\sin(vx)$ anche se $f(x)$ ha solo il $\cos(vx)$\\

\textbf{Osservazione 3.38:} Visto che il grado di $r_1$ e $r_2$ è minore o uguale di quello di $q$ $r_1$ e $r_2$ possono essere trovati per via algebrica. \\

\textbf{Osservazione 3.39:} La stessa conclusione vale se $f(x)=q(x)e^{ux}\sin(vx)$

\chapter{Curve, Campi vettoriali, Forme differenziali}

\section{Curve}

\textbf{Definzione 4.1: }Sia $\Omega$ aperto in $\mathbb{R}^n$. Chiamiamo curva in $\Omega$ qualunque mappa $\underline{\varphi}:[a,b]\subseteq \mathbb{R}\to\Omega$, che sia continua, l'immagine $\gamma$ di $\underline{\varphi}$ è detto sostegno della curva e $\varphi$ è detta parametrizzazione di $\gamma$.\\

\textbf{Osservazione 4.2: }Il dato geometrico è codificato in $\gamma$, quindi capiterà spesso di chiamare "curva" quello che è (solo) il sostegno di una data curva.\\

\textbf{Definizione 4.3:} La curva è detta:\begin{itemize}
\item Chiusa quando $\varphi(a)=\varphi(b)$
\item Semplice quando $\varphi$ è iniettiva in $[a,b)$ e $(a,b]$
\item Regolare quando $\varphi$ è $\mathcal{C}^1$ e $\varphi'(t)\ne 0$ $\forall t\in[a,b]$
\item Regolare a tratti quando $\exists a=t_0<t_1<\ldots<t_n=b$ tale che $\varphi$ è regolare in ogni tratto $(t_j,t_{j+1})$.
\end{itemize}

\textbf{Osservazione 4.4: }La definizione di regolarità è probabilmente "strana". Il fatto è che si vuole che l'oggetto non solo sia regolare dal punto di vista "analitico" (ovvero $\varphi\in\mathcal{C}^1$) ma che lo sia anche dal punto di vista "geometrico" (ovvero $\varphi'(t)\ne 0$ $\forall t$). La condizione $\varphi'(t)\ne 0$ consente infatti di definire un versore tangente $\tau(t):=\frac{\varphi'(t)}{||\varphi'(t)||
}$ in ogni punto. Si osservi che $||\tau(t)||=1$ $\forall t$.

\subsection{Classe di equivalenza e orientazione}

Sia $\underline{\varphi}:[a,b]\to\mathbb{R}^n$ una curva, sia poi $f:[a',b']\to[a,b]$ una mappa biunivoca e bidifferenziale (quindi $f$ e $f^{-1}$ entrambi $\mathcal{C}^1$, ovvero $f$ è un diffeomorfismo). Allora:

$$\underline{\tilde{\varphi}}:=\underline{\varphi}\circ f:[a',b']\to\mathbb{R}^n$$

è una nuova curva con lo stesso sostegno di $\varphi$. Inoltre $\tilde{\varphi}$ sarà chiusa/ regolare/ regolare a tratti/ semplice se e solo se $\varphi$ ha le medesime proprietà.\\

Sotto molti punti di vista quindi $\varphi$ e $\tilde{\varphi}$ sono la stessa cosa; ciò suggerisce di introdurre una relazione di equivalenza $\sim$= $\varphi$ e $\tilde{\varphi}$ sono equivalenti quando $\exists f$ diffeomorfismo tale che $\underline{\tilde{\varphi}}:\underline{\varphi}\circ f$ $(\varphi\sim\tilde{\varphi})$. Si tratta di una effettiva relazione di equivalenza: le informazioni geometriche sono legate non tanto alla curva (cioè a $\varphi$) quanto alla classe di equivalenza $[\varphi]_{/_\sim}$.\\

Per dimostrare che qualche concetto è quindi "geometrico" nonsotante sia definito usando una specifica $\varphi$ si dovrà verificare che esso dipende solo dalla classe di equivalenza $[\varphi]_{/_\sim}$. Ad esempio:\\

\textbf{Definizione 4.5: }Data $\varphi:[a,b]\to\mathbb{R}^n$ regolare, chiamo lunghezza (del sostegno $\gamma$) della curva $\varphi$ l'integrale:

$$l(\gamma):=\int_a^b||\varphi'(t)||dt$$

In effetti l'integrale esiste ed il suo valore non cambia quando si utilizza una diversa $\tilde{\varphi}$ che è equivalente a $\varphi$.\\

\textbf{Dimostrazione: } Se $\tilde{\varphi}=\varphi\circ f$ con $f:[a',b']\to[a,b]$ diffeomorfismo allora $f'\ne 0$. La derivata di $f$ è quindi sempre o negativa o positiva, supponiamo $f'>0$ ($\Rightarrow a=f(a')$ e $b=f(b')$) e si ha:

$$\int_a^b||\varphi'(t)||dt=\int_{a'}^{b'} ||\varphi'(f(u))||f'(u)du=\int_{a'}^{b'} ||\varphi'(f(u))f'(u)||du=\int_{a'}^{b'}||\tilde{\varphi}'(u)||du$$

Se invece si suppone $f'<0$ ($\Rightarrow a=f(b')$ e $b=f(a')$) si ha:

$$\int_a^b||\varphi'(t)||dt=\int_{b'}^{a'} ||\varphi'(f(u))||f'(u)du=\int_{a'}^{b'} ||\varphi'(f(u))f'(u)||du=\int_{a'}^{b'}||\tilde{\varphi}'(u)||du\quad \Box$$\\

Che l'integrale costituisca una misura della lunghezza della curva è una definizione motivata dal fatto che l'integrale assume come valore l'usuale lunghezza nel caso di curve che sono delle spezzate.\\

Ci sono anche altri motivi per chiamare lunghezza quell'integrale. Ad esempio è ragionevole pensare alla lunghezza di una curva come al $\sup$ delle lunghezze delle spezzate interpolanti e  $l(\gamma):=\sup\{ \mbox{lunghezza delle spezzate con vertici in }\gamma \}$. Si dimostra che se $\gamma$ è regolare allora questo $\sup$ esiste e coincide con quell'integrale. Non dimostreremo questo risultato.\\

Inoltre sia $\varphi$ una curva semplice allora $\varphi'(t)\ne 0$, sia $\underline{P}:=\varphi(t_0)$, l'insieme dei vettori $\langle \lambda \varphi'(t_0)\rangle_{\lambda\in\mathbb{R}}$ è un sottospazio d $E_p$ di dimensione 1. Se si cambia parametrizzazione $\tilde{\varphi}=\varphi\circ f$ il vettore cambia in $\tilde{\varphi}(u_0)=\varphi'(f(u_0))f'(u_0)$ (posto $t_0:=f(u_0)$) ma lo span di $\tilde{\varphi}'(u_0)$ è lo stesso du $\varphi'(t_0)$: lo spazio vettoriale $\langle \lambda \varphi'(t_0)\rangle_{\lambda\in\mathbb{R}}$ è quindi definito usando $\varphi$ ma è invariante sotto relazione di equivalenza $\sim$: è quindi un oggetto geometrico infatti è lo spazio tangente a $\gamma$ in $\underline{P}$.\\

Il versore $\underline\tau$ invece non è invariante: se $\tilde{\varphi}=\varphi\circ f$ con $t_0=f(u_0)$, si ha:

$$\tilde{\tau}(u_0)=\frac{\tilde{\varphi}'(u_0)}{||\tilde{\varphi}'(u_0)||}=\frac{\varphi'(t_0)f'(t_0)}{||\varphi'(t_0)f'(t_0)||}=\frac{\varphi'(t_0}{||\varphi'(t_0)||}\cdot\frac{f'(t_0)}{|f'(t_0)|}=\tau{(t_0)}\cdot sgn(f'(t_0))$$

Il nuovo versore $\tilde{\tau}$ coincide con il vecchio versore se e solo $f'>0$ (ovvero $f$ è  crescente), altrimenti è il suo opposto.\\

Questo suggerisce di introdurre una nuova (e più fine) relazione di equivalenza: $\tilde{\varphi}$ $\utilde\cdot$ $\varphi$ quando $\exists f$ biunivoca, bidifferenziabile e crescente tale che $\tilde{\varphi}=\varphi\circ f$. Anche $\utilde{\cdot}$ è una relazione di equivalenza e ogni classe di equivalenza è chiamata "curva orientata".\\

Per le curve orientate ha senso parlare di un "prima" e un "dopo" almeno se sono semplici e non chiuse.\\

Data una curva $\varphi:[a,b]\to\mathbb{R}^n$ con sostegno $\gamma$ si indica con $-\gamma$ la curva $\overline{\varphi}:[a,b]\to\mathbb{R}^n$ con $\overline{\varphi}(t):=\varphi(a+b-t)$. Allora $\varphi$ e $\overline{\varphi}$ non sono $\utilde{\cdot}$ equivalenti anche se sono $\sim$ equivalenti; ogni $\tilde{\varphi}$ che sia $\sim\varphi$ è di fatto $\utilde{\cdot}$ a $\varphi$ o a $\overline{\varphi}$. Questo corrisponde a dire che:\begin{itemize}
\item  Se $\tilde{\varphi}$ $\utilde{\cdot}$ $\varphi\Rightarrow\tilde{\varphi}\sim\varphi$
\item La classe di equivalenza $[\varphi]_{/_\sim}$ si spezza in due classi di $\utilde{\cdot}$ equivalenza.
\end{itemize}
Da ciò segue che $\tau$ non è ben definito in $[\varphi]_{/_\sim}$ ma lo è in $[\varphi]_{/_{\utilde\cdot}}$

\subsection{Concatenazione}

Se $\gamma_1$ è sostegno da $\underline{P}$ a $\underline{Q}$ e $\gamma_2$ da $\underline{Q}$ a $\underline{S}$ allora è possibile concatenare $\gamma_1$ con $\gamma_2$ in ciò che denotiamo con $\gamma_1+\gamma_2$. Anche $\gamma_1+\gamma_2$ è il sostegno di una curva: se $\varphi_1:[a_1,b_1]$ ha sostegno $\gamma_1$ e $\varphi_2:[a_2,b_2]$ ha sostegno $\gamma_2$ allora $\varphi:[0,1]\to\mathbb{R}^n$ con:

$$\begin{cases}
\varphi_1(a_1+(b_1-a_1)2t)\quad t\in[0,\frac{1}{2}]\\
\varphi_2 (a_2+(b_2-a_2)(2t-1))\quad t\in[\frac{1}{2},1]
\end{cases}$$
parametrizza $\gamma_1+\gamma_2$. Si osservi che $\varphi$ è ben definita perchè $\varphi_1(b_1)=Q=\varphi_2(a_2)$ e che $\varphi$ è regolare a tratti se $\varphi_1$ e $\varphi_2$ lo sono ma in generale in $Q$ le derivate possono non coincidere.\\

\textbf{Definizione 4.6: }Dato $\Omega\subseteq\mathbb{R}^n$ aperto, $\Omega$ è detto connesso per archi (semplici/ regolari/...) quando per ogni coppia di punti in $\Omega$ esiste una curva (di quel tipo) che li connette.\\

\textbf{Teorema 4.7: }Sia $\Omega\subseteq\mathbb{R}^n$ un aperto connesso allora è connesso per archi (che si possono prendere semplici, regolari ed orientati).\\

\textbf{Dimostrazione: }Fisso $\underline p$ in $\Omega$. Sia $A:=\{\underline q\in\Omega$, per i quali esiste una curva che connette $\underline p$ con $\underline q\}$:\begin{itemize}
\item $A$ non è vuoto. Infatti esiste una bolla di centro $\underline p$ e tutta in $\Omega$ (perché $\Omega$ è aperto) e ogni punto della bolla è connesso a $\underline p$ dal raggio quindi la bolla è contenuta in $A$.
\item $A$ è aperto. Infatti se $\underline q\in A$ allora esiste una bolla centrata in $\underline q$ tutta in $\Omega$. Ogni elemento $\underline r$ della bolla è connesso dal raggio a $\underline q$ e $\underline q$ lo è a $\underline p$ (perché $\underline q\in A$), quindi $\underline r$ è connesso a $\underline p$ (dalla concatenazione) perciò $\underline r\in A$
\item $A$ è chiuso in $\Omega$ ovvero $A^c \cap \Omega$ è aperto. Infatti sia $\underline q \in A^c$ se non esiste una bolla di centro $\underline q$ tutta in $A^c$ allora la bolla di centro $\underline q$ e tutta in $\Omega$ (che esiste perchè $\Omega$ è aperto) avrebbe almeno un punto $\underline r$ in $A$ ovvero connesso a $\underline p$. Ma allora $\underline r$ è connesso a $\underline q$ dal raggio e quindi $\underline p$ sarebbe connesso a $\underline q$ contro l'ipotesi. L'assurdo dimostra che $A^c$ è aperto.
\end{itemize}
Ma allora $A\ne\emptyset$ e sia $A$ che $A^c\cap\Omega$ sono aperti. Visto che $\Omega$ è connesso, $A^c\cap\Omega=\emptyset$ ovvero la tesi. $\Box$\\

\textbf{Osservazione 4.8:} Vale anche il viceversa. Non dimostreremo questo risultato.\\

\textbf{Osservazione 4.9:} I convessi sono connessi per archi (si scelga il segmento che unisce due punti).\\

\textbf{Definizione 4.10:} $\Omega$ è detto stellato quando $\exists \underline P\in\Omega$ tale che il segmento $\overline{PQ}\in\Omega$ $\forall\underline{Q}\in\Omega$. $\underline P$ è detto centro. Allora $\{Convessi\}\subset\{Stellati\}\subset\{Connessi$ $per$ $archi\}$\\

\textbf{Esempio 4.11: }Esistono $\Omega$ che sono aperti, stellati ed hanno un unico centro. Ad esempio $\mathbb{R}^2$/$\left[\{(x,0)\in\mathbb{R}^2, |x|>1\}\cup\{(0,y)\in\mathbb{R}^2, |y|>1\}\right]$ ha come unico centro il punto $(0,0)$.\\

Curiosità: Se $\Omega$ è stellato l'insieme dei suoi centri è convesso.

\section{Campi vettoriali e Forme differenziali}

Sia $\Omega\subseteq\mathbb{R}^n$ un aperto. Ad ogni $\underline{p}\in\Omega$ è associato lo spazio $E_p$ (delle frecce uscenti da $\underline p$) e lo spazio $E_p^*$: il duale vettoriale di $E_p$ (ovvero l'insieme dei funzionali lineari si $E_p$).\\

\textbf{Definizione 4.12:} Chiamo campo vettoriale su $\Omega$ ogni mappa che associa ad ogni $x\in\Omega$ un elemento di $E_x$, chiamo forma differenziale su $\Omega$ ogni mappa che associa ad $x\in\Omega$ un elemento di $E_x^*$\\

\textbf{Esempio 4.13:} C'è un modo "canonico" per produrre un campo vettoriale e una forma differenziale: sia $f:\Omega\to\mathbb{R}$ di classe $\mathcal{C}^1$.\begin{itemize}
\item La mappa del $\nabla f$ manda $x\mapsto (\nabla f)(x)\in E_x$ è campo vettoriale.
\item La mappa $df$ che manda $x\mapsto (df)(x)\in E_x^*$ (la derivata di $f$ in $x$) che manda $v\mapsto(D_vf)(x)$ (derviata di $f$ in $x$ lungo la direzione $v$) è una forma differenziale.
\end{itemize}
Questi sono tipi di campi e di forme estremamente importanti e hanno un nome proprio\\

\textbf{Definizione 4.14: }Un campo vettoriale $\underline F$ è detto conservativo su $\Omega$ se esiste $f:\Omega\to\mathbb{R}$ di classe $\mathcal{C}^1$ con $\underline{F}=\nabla f$. Una forma differenziale $\omega$ è detta esatta su $\Omega$ se esiste $f:\Omega\to\mathbb{R}$ di classe $\mathcal{C}^1$ con $\omega=df$. In entrambi i casi $f$ è detto potenziale per $F$ (o per $\omega$)\\.

\textbf{Osservazione 4.15: }Il potenziale se esiste non è mai unico: questo perchè qualunque sia il valore di una costante $c\in\mathbb{R}$ si ha $\nabla (f+c)=\nabla f$ (e $d(f+c)=df$).\\

D'altra parte è evidente che se $f$ e $\tilde{f}$ sono due potenziali per il medesimo campo allora $\nabla (f-\tilde{f})=\underline{0}$ quindi i potenziali sono unici a meno di funzioni $\mathcal{C}^1(\Omega)$ a gradienti $\equiv 0$. Si osservi che:

$$V_0(\Omega):=\{g\in\mathcal{C}^1(\Omega)\mbox{, }\nabla g=0\}$$
è uno spazio vettoriale. Ogni funzione di tale spazio è costante sulle componenti connessi di $\Omega$, d'altra parte per ogni componente connessa $C_j$ si può scegliere arbitrariamente un nuovo $c_j\in\mathbb{R}$ e ponendo:

$$g(x)=\{c_j\mbox{ quando }x\in C_j,\forall j\}$$
si definisce un elemento di $V_0(\Omega)$. Questo dimostra che:
$$\dim V_0(\Omega)=\# \mbox{Componenti connesse in }\Omega$$

\textbf{Osservazione 4.16:} $E_p$ è uno spazio euclideo su cui cioè esiste un prodotto scalare $\langle$ , $\rangle$. Tramite questo prodotto è possibile identificare un elemento $E_p*$ con un vettore di $E_p$ (isomorfismo di Riesz). Ciò significa che dato un campo vettoriale si può sempre costruire una forma differenziale:

$$\underline{F}\to\omega:=\langle\underline F,\cdot\rangle$$
e viceversa, data una forma differenziale $\omega$ esiste un campo $\underline F$ tale che:

$$\omega\to\exists \underline F\mbox{ con }\omega=\langle\underline F,\cdot\rangle$$
Usare campi o forme è quindi; a questo livello, del tutto equivalente.

\subsection{Notazione}

Per $\Omega\subseteq\mathbb{R}^n$ indichiamo con: \begin{itemize}
\item  $\partial x_i$ (o $\partial_i$) il campo vettoriale definito da $\partial x_i$:= versore $e_i$ di $E_x$ che è parallelo ad $e_i$ di $\mathbb{R}^n$
\item $dx_i$ la forma differenziale duale ovvero tale che:

$$(dx_i)(\partial_j)=\begin{cases} 1 &\mbox{se } j=i\\0 &\mbox{se } j\ne i\end{cases}$$
\end{itemize}
Il generico campo vettoriale sarà quindi scritto:

$$\underline F=a_1(x)\partial_1+a_2(x)\partial_2+\ldots+a_n(x)\partial_n$$
e la generica forma differenziale sarà:

$$\omega=a_1(x)dx_1+a_2(x)dx_2+\ldots+a_n(x)dx_n$$
ed $F$ (o $\omega$) sono di classe $\mathcal{C}^k$ quando $a_1,\ldots,a_n$ sono $\mathcal{C}^k(\Omega)$\\

\textbf{Esempio 4.17: } Se $F(x)=(3x-y)\partial_x+(7x^2-9y)\partial_y$ in $\mathbb{R}^2$ e $\omega(x)=(4-2x)dx+(5-xy)dy$ allora $\omega(x)(F(x))=(4-2x)(3x-y)+(5-xy)(7x^2-9y)$\\

Inoltre $F$ è conservativa (in $\Omega$) se e solo se $\exists f \in \mathcal{C}^1(\Omega)$ tale che: 

$$\begin{cases}
\frac{\partial f}{\partial x_1}(x)=a_1(x)\\
\vdots\\
\frac{\partial f}{\partial x_n}(x)=a_n(x)
\end{cases}$$
Analogamente $\omega$ è esatta se e solo se $\exists f \in \mathcal{C}^1(\Omega)$ tale che: 

$$\begin{cases}
\frac{\partial f}{\partial x_1}(x)=a_1(x)\\
\vdots\\
\frac{\partial f}{\partial x_n}(x)=a_n(x)
\end{cases}$$

\subsection{Integrazione di forme/campi lungo curve}

Sia $\omega$ una forma differenziale continua su $\Omega$, sia $\varphi$ una curva in $\Omega$, orientata con sostegno $\gamma$. Si pone:

$$\int_\gamma\omega:=\int_\gamma \sum_{j=1}^na_j(x)dx_j:=\int_a^b\sum_{j=1}^n a_j(\varphi(t))\cdot\varphi'(t)dt$$
Se $F$ è un campo vettoriale analogamente si pone:

$$\int_\gamma F\cdot ds:=\int_\gamma \langle F,ds\rangle=\int_a^b\sum_{j=1}^n a_j(\varphi(t))\cdot\varphi'(t)dt$$

\textbf{Osservazione 4.18: }Le due definizioni sono coerenti, nel senso che se $\omega$ è la forma associata ad $F$ (o viceversa) allora $\int_\gamma \omega=\int_\gamma \underline{F}\cdot\underline{ds}$\\

\textbf{Osservazione 4.19: }L'integrale appena definito è $\utilde\cdot$ invariante: se $\tilde{\varphi}$ è una diversa parametrizzazione di $\gamma$ (ma con la stessa orientazione), l'integrale calcolato con $\tilde{\varphi}$ o con $\varphi$ danno lo stesso valore. Tuttavia non è $\sim$ invariante:

$$\int_{-\gamma}\omega=-\int_\gamma\omega$$
ovvero cambiando l'orientazione l'integrale inverte il segno.\\

\textbf{Osservazione 4.20: }L'integrale è "$\gamma$-additivo" ovvero:

$$\int_{\gamma_1+\gamma_2}\omega=\int_{\gamma_1}\omega+\int_{\gamma_2}\omega$$
purchè $\gamma_1+\gamma_2$ sia definito, orientato e $\mathcal{C}^1$ a tratti.

La seguente proposizione spiega perchè la nozione di campo conservativo / forma esatta sia interessante e quale sia il legame con l'integrale definito.\\

\textbf{Proposizione 4.21: }Sia $\Omega$ aperto connesso e $\omega$ una forma differenziale continua si $\Omega$. Allora le seguenti proprietà sono equivalenti:\begin{enumerate}
\item $\omega$ è esatta
\item Siano $\underline{p}$ e $\underline{q}$ punti qualunque in $\Omega$, allora l'integrale $\int_{\underline{p}\gamma\underline{q}}\omega$, dove $\gamma$ è una qualunque curva (sostegno) regolare a tratti e orientata da $\underline{p}$ a $\underline{q}$, non dipende dalla scelta di $\gamma$: $\int_{\underline{p}\gamma_1\underline{q}}\omega=\int_{\underline{p}\gamma_2\underline{q}}\omega$
\item Per ogni curva $\gamma$ in $\Omega$ che sia regolare a tratti e chiusa si ha $\int_\gamma\omega=0$
\end{enumerate}

\textbf{Osservazione 4.22: }L'ipotesi di $\Omega$ connesso garantisce $\Omega$ connesso per archi: dati $\underline{p}$ e $\underline{q}$ punti qualunque esiste di sicuro la curva $\underline{p}\gamma_1\underline{q}$\\

\textbf{Osservazione 4.23:} La proposizione mostra che l'esattezza di $\omega$ equivale alla possibilità di calcolare $\int_{\underline{p}\gamma\underline{q}}\omega$ a $\underline{p}$ e $\underline{q}$ fissati cambiando $\gamma$ come ci pare.\\

\textbf{Osservazione 4.24:} L'equivalente affermazione per i campi vettoriali dice che $F$ è conservativo se e solo se il lavoro di $F$ da $\underline{p}$ e $\underline{q}$ lungo $\gamma$ non dipende da $\gamma$.\\

\textbf{Dimostrazione:} $1\Rightarrow 2)$ Supponiamo $\omega$ esatta, quindi $\omega=df$ con $f\in\mathcal{C}^1(\Omega)$. Allora se $\varphi:[a,b]\to\Omega$ è la curva ($\mathcal{C}^1$ a tratti, orientata) da $\underline{p}$ a $\underline{q}$ si ha:

$$\int_{\underline{p}\gamma\underline{q}}\omega=\int_a^b\sum_{i=1}^n a_i(\varphi(t))\varphi_i'(t)dt=\int_a^b\sum_{i=1}^n \frac{\partial f}{\partial x_i}(\varphi(t))\varphi_i'(t)dt=\int_a^b\frac{d}{dt}(f(\varphi(t)))=$$

$$f(\varphi(b))-f(\varphi(a))=f(q)-f(p)$$
che quindi non dipende da $\gamma$.\\

$2\Rightarrow 3)$ Se $\gamma$ è chiusa: prendo $p$ e  $q$ su $\gamma$ e spezzo $\gamma$ come $\gamma_1+\gamma_2$ dove $gamma_1$ va da $p$ a $q$ e $\gamma_2$ da $q$ a $p$. Allora:

$$\int_\gamma\omega=\int_{\gamma_1+\gamma_2}\omega=\int_{\gamma_1}\omega+\int_{\gamma_2}\omega=\int_{\gamma_1}-\int_{-\gamma_2}\omega=0$$
perchè sia $\gamma-1$ sia $-\gamma_2$ vanno da $p$ a $q$ e stiamo assumendo $2$.\\

$3\Rightarrow 2)$ Se $\gamma_1$ a $\gamma_2$ vanno da $p$ a $q$ allora $\gamma_1+(-\gamma_2)$ è chiusa, quindi:

$$\int_{\gamma_1}\omega-\int_{\gamma_2}\omega=\int_{\gamma_1}\omega+\int_{-\gamma_2}\omega=\int_{\gamma_1+(-\gamma_2)}\omega=0$$\\

$2\Rightarrow 1)$ Sia $p\in\Omega$ fissato (a caso ma fissato). Sia $f(x):=\int_{p\gamma x}\omega$ dove $\gamma$ è una qualunque curva (regolare a tratti, orientata) che collega $p$ ad $x$ in $\Omega$ ($\gamma$ esiste perchè $\Omega$ è connesso per archi).. Allora $f$ è ben definito in $\Omega$, perchè $2$ mostra che $f(x)$ dipende solo da $x$ e non dalla scelta di $\gamma$. Mostriamo che $f$ è un potenziale per $\omega$: questo implica che $\omega$ è esatta.\\

Sia $h\ne o$ e supponiamo piccolo perchè $\underline{x}+h\underline{e_1}\in\Omega$ (possibile perchè $\Omega$ è aperto). Allora $f(\underline{x}+h\underline{e_1}\in\Omega)$ è per definizione l'integrale lungo qualunque cammino che colleghi $\underline P$ con $\underline{x}+h\underline{e_1}\in\Omega$, come cammino scelgo $\gamma+$ segmento $[\underline{x},\underline{x}+h\underline{e_1}\in\Omega]$ così:

$$\frac{1}{h}(f(\underline{x}+h\underline{e_1})-f(\underline{x})=\frac{1}{h}\left(\int_{p\gamma x}\omega+\int_{[x,x+he_1]}\omega-\int_{p\gamma x}\omega\right)=\frac{1}{h}\int_{[x,x+he_1]}\omega$$
Come parametrizzazione prendo:

$$\varphi:[0,h]\to\mathbb{R}^n\qquad\varphi(t)=\underline{x}+t\underline{e_1}$$
Allora $\varphi'(t)=e_1$ quindi:

$$\frac{1}{h}\int_{[x,x+he_1]}\sum_{i=1}^n a_i(w)dw_i=\frac{1}{h}\int_0^h\sum_{i=1}^n a_i(x+te_1)\cdot(e_1)_idt=\frac{1}{h}\int_0^ha_1(x+te_1)dt$$
ma $a_1$ è continua quindi usiamo il teorema del valor medio integrale:

$$=a_1(x+\xi e_1)$$

con $\xi\in[0,h]$ e se $h\to 0$ allora $\xi\to 0$ ed $a_1(x+\xi e_1)\to a_1(x)$ perchè $a_1$ è continua quindi:

$$\lim_{h\to 0} \frac{1}{h}(f(x+te_1)-f(x))\mbox{ esiste e vale }a_1(x)$$
Lo stesso vale per le altre direzioni quindi $df=\omega$ e visto che le $a_i$ sono continue ne segue che $f\in\mathcal{C}^1(\Omega)$. $\Box$\\

Il teorema appena visto illustra l'interesse per la proprietà di conservatività del campo vettoriale (o esattezza della forma), ma è poco pratico come strumento per dimostrare queste proprietà. (Banalmente; come fare a verificare che $\int_\gamma\omega=0$ per ogni curva chiusa). La seguente proprietà mostra una conseguenza della esattezza e quindi è un utile criterio necessario.\\

\textbf{Definizione 4.25: }Dato il campo vettoriale $F$ su $\Omega$, $\Omega$ aperto in $\mathbb{R}^n$, di classe $\mathcal{C}^1$. Sia $F(x)=(a_1(x),\ldots,a_n(x))$, $F$ è detto irrotazionale quando:

$$\frac{\partial a_j(x)}{\partial x_i}=\frac{\partial a_i(x)}{\partial x_j}\quad\forall i,j=1,\ldots,n$$
Analogamente, data una forma differenziale $\omega$ su $|Omega$ di classe $\mathcal{C}^1$, $\omega=a_1(x)dx_1+\ldots+a_ndx_n$, $\omega$ è detta chiusa quando:

$$\frac{\partial a_j(x)}{\partial x_i}=\frac{\partial a_i(x)}{\partial x_j}\quad\forall i,j=1,\ldots,n$$

\textbf{Teorema 4.26: }Sia $\Omega$ aperto e $F$ campo vettoriale su $\Omega$ di classe $\mathcal{C}^1$. Se $F$ è conservativo allora $F$ è irrotazionale. Analogamente sia $\omega$ una forma differenziale di classe $\mathcal{C}^1$ su $\Omega$; se $\omega$ è esatta allora è chiusa.\\

\textbf{Dimostrazione:} La diamo per i campi, per le forme è uguale. $F=(a_1(x),\ldots,a_n(x))$ con $a_j\in\mathcal{C}^1(\Omega)$ per ipotesi. $F$ è esatta quindi esiste $f:\Omega\to\mathbb{R}$, $f\in\mathcal{C}^1(\Omega)$ tale $F=\nabla f$ quindi:

$$a_j(x)=\frac{\partial f}{\partial x_j}(x)\quad\forall j$$
Visto che $a_j\in\mathcal{C}^1$ allora $f\in\mathcal{C}^2(\Omega)$ e così:

$$\frac{\partial a_j(x)}{\partial x_i}=\frac{\partial}{\partial x_i}\frac{\partial f}{\partial x_j}=\frac{\partial}{\partial x_j}\frac{\partial f}{\partial x_i}=\frac{\partial a_i(x)}{\partial x_j}\qquad\Box$$

\textbf{Esempio 4.27: } (vortice) Sia $F(x,y)=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$ (forma equivalente: $\omega(x,y)=\frac{1}{x^2+y^2}(-ydx+xdy)$). $F$ è irrotazionale perchè:

$$\frac{\partial}{\partial y}\left(\frac{-y}{x^2+y^2}\right)=\frac{y^2-x^2}{x^2+y^2}=\frac{\partial}{\partial_x}\left(\frac{x}{x^2+y^2}\right)$$
$F$ non è però conservativo: se $\gamma$ è la circonferenza di raggio 1:

$$\varphi(t)=(\cos t,\sin t) \quad t:0\to 2\pi$$ 

$$\int_\gamma\omega=\int_\gamma \frac{-ydx+xdy}{x^2+y^2}=\int_0^{2\pi}-\sin t\mbox{ }d(\cos t)+\cos t\mbox{ }d(\sin t)=\int_0^{2\pi}1dt=2\pi\ne 0$$
Se fosse conservativo l'integrale dovrebbe essere 0 visto che $\gamma$ è curva regolare è chiusa.\\

La irrotazionalità per campi vettoriali o la chiusura per le forme differenziali non sono quindi sufficienti per l'essere conservativi o esatte.\\

\textbf{Osservazione 4.28: }Dato $\Omega$ aperto siano:
$$V_1(\Omega)=\{\omega\mbox{ forme }\mathcal{C}^1(\Omega),\mbox{ chiuse}\}$$
$$V_1^e(\Omega)=\{\omega\mbox{ forme }\mathcal{C}^1(\Omega),\mbox{ esatte}\}$$

$V_1(\Omega)$ è spazio vettoriale, perchè se $\omega$ e $\tilde\omega$ sono forme chiuse anche $\omega+\lambda\tilde\omega$ lo è visto che:

$$\frac{\partial}{\partial x_j}(a_1+\lambda\tilde{a}_i)=\frac{\partial a_i}{\partial x_j}+\lambda\frac{\tilde{a}_itilde}{\partial x_j}$$

Anche $V_i^e(\Omega)$ è uno spazio vettoriale perchè se $\omega=df$ e $\tilde{\omega}=d\tilde{f}$ allora $\omega+\lambda\tilde{\omega}=d(f+\lambda\tilde{f})$.

Lo spazio quoziente:

$$\frac{V_1(\Omega)}{{V_i^e(\Omega)}}$$
quindi è una misura di quanto l'insieme delle forme chiuse "disti" da quello delle esatte poichè $V_1(\Omega)\equiv V_1^e(\Omega)$ se e solo se il quoziente è banale.\\

 L'esempio del vortice mostra che per $\Omega=\mathbb{R}^2\textbackslash\{(0,0)\}$ si ha $V_1(\Omega)\neq V_1^e(\Omega)$ ovvero il quoziente non è banale (quindi ha dimensione $\geq1$). Ovviamente le stesse condizioni possono essere fatte per l'insieme dei campi irrotazionali e conservativi. Il seguente teorema 4.30 è fondamentale:\\
 
\textbf{Lemma (derivazione sotto segno di integrale) 4.29:} Sia $f:\Omega\times [a,b]\to\mathbb{R}$, con $\Omega$ aperto di $\mathbb{R}$. Sia $x_0\in\Omega$ e supponiamo che $\exists\mathcal{U}(x_0)$ aperto tale che:\begin{itemize}
\item $f\in\mathcal{C}(\mathcal{U}(x_0)\times[a,b])$
\item $\frac{\partial f}{\partial x}$ esiste in $\mathcal{U}(x_0)\times[a,b]$ e $\in\mathcal{C}(\mathcal{U}(x_0)\times[a,b])$
\end{itemize}
Allora la funzione:

$$F(x):=\int_a^b f(x,t)dt$$
è derivabile in $x_0$ e $\frac{dF}{dx}(x_0)=\int_a^b\frac{\partial f}{\partial x}(x_0,t)dt$.\\

\textbf{Teorema (Lemma di Poincarè) 4.30: } Sia $\Omega\subseteq\mathbb{R}^n$ aperto e stellato. Sia $\omega=\sum_{j=1}^n a_j(x)dx_j$ una forma differenziale $\mathcal{C}^1(\Omega)$, se $\omega$ è chiusa allora è anche esatta.\\

\textbf{Osservazione 4.31: }Ovviamente esiste il teorema gemello per i campi vettoriali.\\

\textbf{Osservazione 4.32: } Il teorema mostra che una informazione sulla topologia di $\Omega$ rende la chiusura una condizione sufficiente per l'esattezza.\\

\textbf{Osservazione 4.33: } Ogni bolla aperta è convessa quindi stellata. Ne segue che le forme chiuse sono sempre localmente esatte (il problema è che l'esattezza locale non implica quella globale).\\

\textbf{Osservazione 4.34: }Dal teorema segue che se $\Omega$ è stellato allora $V_1(\Omega)=V_1^e(\Omega)$ ovvero il loro quoziente è banale.

\textbf{Dimostrazione: }$\Omega$ è stellato per ipotesi, sia quindi $\underline{P}$ uno dei suoi centri (quale non importa). Sia $X\in\Omega$, il segmento $\overline{PX}$ è quindi
\end{document}


   

